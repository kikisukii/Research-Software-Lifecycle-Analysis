id,slug,brand_name,description,repo_urls,github_owner_repo
008a37be-b7d3-4b77-9251-b1e524fd8d8c,experiment-launcher,Experiment Launcher,"* For software developers wanting to talk to a Swagger based webservice to generate and launch a Jupyter notebook based on parameters and a template * When the experiment launcher is combined with a frontend application like, the TerriaJS the end user is able click somewhere in the frontend and end up in a Jupyter notebook environment * Can be extended to have end points for different notebook types",https://github.com/eWaterCycle/experiment-launcher,eWaterCycle/experiment-launcher
01168f4c-0fb6-4c49-a876-db1042cbe728,fhir-to-capacity,FHIR to CAPACITY,"* FHIR-to-CAPACITY provides an automated way for hospitals to upload electronic health records on from COVID-19 patients to the centralized CAPACITY registry * FHIR-to-CAPACITY queries patient data from a specified FHIR endpoint and converts it into records compliant with the CAPACITY codebook * Much of the data for CAPACITY is still entered by hand. FHIR-to-CAPACITY provides a piece of the puzzle for hospitals to automate the process  The FAIR data for CAPACITY project aims to automate the process of uploading data on COVID-19 patients to the central [CAPACITY registry](https://capacity-covid.eu/). However, the data models for electronic health records vary a lot per hospital.  In the FAIR data for CAPACITY project we aim to generalize the process as much as possible by requiring that the hospital data should be converted to the FHIR format first. After this conversion, which has to be customized per hospital, the final conversion can be picked up by FHIR to CAPACITY. This software will convert the FHIR data into records that are compliant with the [CAPACITY codebook](http://capacity-covid.eu/wp-content/uploads/CAPACITY-REDCap-2.pdf) and will upload it to the REDCap registry that has been provided by the user.",https://github.com/FAIR-data-for-CAPACITY/FHIR-to-CAPACITY,FAIR-data-for-CAPACITY/FHIR-to-CAPACITY
01422160-1c7f-4bc8-a8b6-31cb179c2bf2,rwikipathways,rWikiPathways,"R Client library for the WikiPathways API (https://webservice.wikipathways.org/) (license: MIT).  WikiPathays is described in the following papers: * 2016 NAR paper by [Kutmon et al.](https://doi.org/10.1093/nar/gkv1024) * 2018 NAR paper by [Slenter et al.](https://doi.or/10.1093/nar/gkx1064) * 2021 NAR paper by [Martens et al.](https://doi.org/10.1093/nar/gkaa1024) * 2024 NAR paper by [Agrawal et al.](https://doi.org/10.1093/nar/gkad960)  If you like this package, or want to make it easier to work with Xrefs, then you may also like these R packages:  * [BridgeDbR](https://github.com/BiGCAT-UM/bridgedb-r) * [PathVisioRPC](http://projects.bigcat.unimaas.nl/pathvisiorpc/) * [RCy3](https://github.com/cytoscape/RCy3)  ## Getting Started * [Documentation site](https://r.wikipathways.org) * [Overview vignette](articles/Overview.html)  ## How to install **_Official bioconductor releases_ (recommended)** ``` if (!requireNamespace(""BiocManager"", quietly=TRUE))     install.packages(""BiocManager"") BiocManager::install(""rWikiPathways"") ``` *Note: Be sure to use the [latest Bioconductor](https://www.bioconductor.org/install/) and recommended R version*   **_Unstable development code from this repo_ (at your own risk)** ``` install.packages(""devtools"") library(devtools) install_github('wikipathways/rWikiPathways', build_vignettes=TRUE) library(rWikiPathways) ```",https://codeberg.org/egonw/rWikiPathways,
018c7b14-2802-4acb-9426-454b0ddef06a,spex-x-ray-spectral-fitting-package,SPEX X-ray spectral fitting package,"SPEX is a software package for fitting astrophysical X-ray spectra. It has been developed since the 1970s at SRON Netherlands Institute for Space Research. SPEX is an interactive command-line program for the computation of emergent spectra of optically thin plasmas such as stellar coronal loop structures, supernova remnants (also including transient ionization effects), photo-ionized plasmas, and optically thick plasmas. These model spectra can be fitted to measured X-ray spectra from various X-ray observatories, like XMM-Newton and Chandra. SPEX has been optimized for high-resolution X-ray spectroscopy, which makes it especially suitable for analyzing grating and micro-calorimeter spectra.",,
01a3e457-1d77-432f-a8e5-55fb7985b78f,4tu-dataset-nondimensionalisationqmom,Dataset-NonDimensionalisationQMOM,"Wet granulation is a multiphase process utilised to produce aggregate particles with defined properties from very fine powders. Simulating this process on the microscale is challenging because of the large number of particles involved, which differ widely in both size and material properties. Macroscale methods, which track only the particle bulk properties, are efficient but do not resolve disperse particle properties such as the particle size distribution (PSD), which is key information for downstream processing. These deficiencies are addressed by mesoscale methods like population balance (PB) models, which track distributed properties such as the particle size by adding them as internal variables to the macroscale (CFD) model. However, most mesoscale methods are either inaccurate (method of moments when cutting off moments) or computationally expensive (Monte Carlo, class methods). Recently a new closure for the method of moments, the quadrature method of moments (QMOM), was introduced to allow accurate moment tracking of a PSD with low computational effort. One disadvantage of this method is that it can suffer from instabilities. These, however, can be overcome by non-dimensionalization. In the study we show our insights gained by non-dimensionalizing the QMOM equations for wet granulation processes, which model the PSD via growth, aggregation and breakage kernels. Also relevant numerical and theoretical issues as well as limitations are discussed.  The software in this dataset can be used to reproduce all figures and verify the integrity of the title paper. The software consists of a fully working MATLAB (Version R2019b) script for a non-dimensional and dimensional quadrature method of moments and all the additional methods that are needed to run the quadrature method of moments (reading in data, adaptive Wheeler algorithm, etc.). Further, a Python script (Version 3.7) for the Buckingham theorem using BuckinghamPy is available.",https://data.4tu.nl/v3/datasets/de2d879d-af3f-47cc-a6ac-49543ad10746.git,
01a7b4ab-9740-4714-acd3-1da9b8b2b2cc,bridgedb-webservice,BridgeDb Webservice,"The BridgeDb Webservice is a REST API for identifier mapping, based on the [BridgeDb Java](https://research-software-directory.org/software/bridgedb-java) library.  For example, the following call can be made to get identifier mappings for NCBI Gene 1234:  ```shell curl https://webservice.bridgedb.org/Human/xrefs/L/1234 ```  To get a list of supported databases, use:  ```shell curl https://webservice.bridgedb.org/Human/sourceDataSources ```  More information on [this OpenAPI documentation page](https://webservice.bridgedb.org/).",https://github.com/bridgedb/BridgeDbWebservice,bridgedb/BridgeDbWebservice
01ad58da-d725-4d31-9d28-e1c0365052ad,yoda,Yoda,"Yoda is a research data management solution developed by Utrecht University and used by multiple institutes around the world. It enables researchers and their partners to securely deposit, share, publish and preserve large amounts of research data during all stages of a research project.",https://github.com/UtrechtUniversity/yoda,UtrechtUniversity/yoda
021b22ed-d51e-4d27-852a-5b06635e8cfc,dtscalibration,dtscalibration,"'dtscalibration' is a Python package to load Distributed Temperature Sensing files, perform a calibration, and plot the result. A detailed description of the calibration procedure can be found at https://doi.org/10.3390/s20082235   ### Package features  * Advanced calibration routine    * Both single- and double-ended setups    * Confidence intervals of calibrated temperature    * Time integration of calibration parameters    * Weighted least-squares calibration    * Fixing parameters to a previously determined value]    * (Asymmetric) step loss correction    * Matching temperature sections * Dynamic reference section definition * Tools for merging and aligning double-ended setups * Data formats of most manufacturers are supported  ### Devices supported * Silixa Ltd.: **Ultima** & **XT-DTS** .xml files *(up to version 8.1)* * Sensornet Ltd.: **Oryx**, **Halo** & **Sentinel** .ddf files * AP Sensing: **CP320** .xml files *(single ended only)* * SensorTran: **SensorTran 5100** .dat binary files *(single ended only)*",https://github.com/dtscalibration/python-dts-calibration,dtscalibration/python-dts-calibration
02523914-8a05-4f28-b2e6-d967f367a553,4tu-diverse-projection-ensembles,diverse-projection-ensembles,"This is the official code repository of projection-ensemble DQN, accompanying the paper ""Diverse projection ensembles for distributional reinforcement learning"" (ICLR 2024).",https://data.4tu.nl/v3/datasets/26e6f861-05e8-4446-b58b-4c045bf8b60c.git,
0287179f-6362-4a66-8c9a-812882fa3b0b,4tu-matlab-scripts-and-python-notebook-to-analyze-the-experiment-yeast-population-growth-in-different-galactose-concentrations,Matlab scripts and python notebook to analyze the experiment: Yeast population growth in different galactose concentrations,These dataset provides some mat files and the scripts used to analyze the experiment under doi: https://doi.org/10.4121/12961922.v1. These files are used to generate Fig3A from the paper: https://doi.org/10.1101/2020.09.09.290510,,
02962dbf-4c06-4b15-8b7e-241e3331e252,rcoins,rcoins,"<!-- badges: start --> [![R-CMD-check](https://github.com/CityRiverSpaces/rcoins/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/CityRiverSpaces/rcoins/actions/workflows/R-CMD-check.yaml) <!-- badges: end -->  Identify continuous lines in a network using an approach based on the Continuity in Steet Network (COINS) method[^1].  [^1]: [Tripathy, P., Rao, P., Balakrishnan, K., & Malladi, T. (2021). An open-source tool to extract natural continuity and hierarchy of urban street networks. Environment and Planning B: Urban Analytics and City Science, 48(8), 2188-2205.][coins]  [coins]: http://dx.doi.org/10.1177/2399808320967680  ## Example  Given the street network of the city of Bucharest (data source: [OpenStreetMap][osm])  ``` r library(rcoins) streets <- bucharest$streets ```  ![](https://raw.githubusercontent.com/CityRiverSpaces/rcoins/main/man/figures/streets.png)  Determine continuous lines in the spatial network as:  ``` r continuous_streets <- stroke(streets) ```  ![](https://raw.githubusercontent.com/CityRiverSpaces/rcoins/main/man/figures/continuous-streets.png)  [osm]: https://www.openstreetmap.org/",https://github.com/CityRiverSpaces/rcoins,CityRiverSpaces/rcoins
033da964-1a8d-4550-bdda-c2f12d5e2b65,lxcat,LXCat,"The *Plasma Data Exchange Project* is a community-based project which was initiated as a result of a public discussion held at the 2010 Gaseous Electronics Conference (GEC), a leading international meeting for the Low-Temperature Plasma community. This project aims to address, at least in part, the well-recognized needs for the community to organize the means of collecting, evaluating and sharing data both for modeling and for interpretation of experiments.  At the heart of the Plasma Data Exchange Project is LXCat (pronounced ""elecscat""), an open-access website for collecting, displaying, and downloading electron and ion scattering cross sections, swarm parameters (mobility, diffusion coefficient, etc.), reaction rates, energy distribution functions, etc. and other data required for modeling low temperature plasmas. The available data bases have been contributed by members of the community and are indicated by the contributor's chosen title.",https://github.com/LXCat-project/LXCat,LXCat-project/LXCat
0352bda7-9fbc-493f-a66c-053dcd8f54d1,ree-hdsc,REE-HDSC,Assistance in hand-written text recognition of Dutch civil registry records with the Transkribus software package including entity recognition and evaluation of the results.,https://github.com/ree-hdsc/ree-hdsc,ree-hdsc/ree-hdsc
03551f8f-c995-4aeb-9ce5-e8d4c4b9d85e,scifeon-lims-eln-and-workflow-automation,"Scifeon Lims, ELN and Workflow Automation","Scifeon is a cloud-based LIMS and ELN platform designed for research labs in biotech, pharma, and CDMO environments. It helps scientific teams digitize their processes, reduce manual documentation, and manage lab data in a structured, compliant, and scalable way.  Built with researchers in mind, Scifeon supports everything from sample tracking and experiment documentation to QA workflows and digital approvals. Scientists can build and execute workflows step-by-step using a visual interface, ensuring repeatability, traceability, and easy onboarding for new team members.  Scifeon enables labs to move away from paper, Excel, and scattered documentation platforms like Confluence. The system supports structured and unstructured data, integrates with lab instruments and ERP systems, and provides full compliance with GMP, GxP, and FDA 21 CFR Part 11 standards. Audit trails, electronic signatures, and permission-based access control are built-in.  One of Scifeonâ€™s key advantages is its modularity: labs can start with a single workflow or department and expand over time. The platform supports rapid implementation using SAPPA (Structured Approach for Process-specific Pilots Adoption), allowing teams to go live with meaningful digitalization in just 30 to 45 days.  Used by industry leaders like AGC Biologics, Scifeon delivers measurable resultsâ€”reducing coordination effort, improving data integrity, and supporting innovation through better lab oversight. Whether youâ€™re in upstream research, downstream QC, or tech transfer, Scifeon helps your lab work smarter, faster, and with greater confidence.  Scifeon is more than just a digital lab tool: it's a flexible, user-driven informatics platform that adapts to how real research teams work. Whether youâ€™re running fermentation experiments, managing analytical workflows, or documenting assay development, Scifeon provides a structured digital environment that makes data easier to capture, review, and reuse.  The platform supports full experiment lifecycle management: from planning and protocol execution to result validation and reporting. Through its ELN module, scientists can create reusable templates, collaborate in real-time, and link results directly to samples and metadata. The LIMS component ensures that materials, samples, and test results are fully traceable across projects, batches, and teams.  Scifeon also supports digital inventory management, compliance documentation, instrument integrations, and custom dashboards for real-time data insights. The system is hosted in secure EU cloud environments and can be validated for regulated use.  Itâ€™s designed to serve both R&D and QA/QC use cases with equal efficiency, helping teams standardize processes without sacrificing flexibility. Whether your lab is scaling up biologics production or conducting early-phase discovery, Scifeon is built to grow with you, empowering science through smarter software.",,
0357d4cc-9e1f-4e8f-b12c-bd1192ef48f1,4tu-source-documentation-tests-mace-automated-assessment-of-stereochemistry-of-transition-metal-complexes-and-its-applications-in-computational-catalysis,"Source, documentation, tests: MACE: Automated Assessment of Stereochemistry of Transition Metal Complexes and its Applications in Computational Catalysis","MACE is an open source toolkit for the automated screening and discovery of metal complexes.&nbsp;The main features of the MACE package are to discover all possible configurations for square-planar and octahedral metal complexes, and generate atomic 3D coordinates suitable for quantum-chemical computations. This dataset contains source code, documentation, CLI examples, and test cases for MACE workflow reported in&nbsp; ""MACE: Automated Assessment of Stereochemistry of Transition Metal Complexes and its Applications in Computational Catalysis"", J. Chem. Theory Comput.",https://data.4tu.nl/v3/datasets/b4e720b9-03ec-4227-8a31-88883ab80d74.git,
03595f7d-d649-49c4-8c7d-e36c65904e11,4tu-framework-for-the-publication-mirrorlabs-creating-similar-learning-environments-for-students-all-over-europe-for-human-robot-coproduction,Framework for the publication: MirrorLabs â€“ creating similar learning environments for students all over Europe for human-robot coproduction,"In order to educate students for â€œpeople and robots for sustainable workâ€, students will need to access real hardware. One of the emerging Human-Robot-Interaction techniques is the Augmented and Virtual Reality (AR/VR). Combining AR/VR with existing robotics infrastructure is quite challenging. Furthermore, it is even more complicated to get students familiar with all of the required background. The aim of this proposal is the development of an common, easy-to-use ICT infrastructure for the existing equipment in the labs of the participating partners. The developed software is planned to be released open source, so that it can be used by other partners within and outside EIT Manufacturing community. The project will develop a software platform and tutorials on how to set up the platform and how to get started to work with it. More information can be found under www.mirrorlabs.eu .",,
03ae5e6f-8844-439a-b340-39ca1990d7f6,trap-the-lofar-transients-pipeline,TraP: The LOFAR Transients Pipeline ,"The LOFAR Transients Pipeline (â€œTraPâ€) provides a means of searching a stream of N-dimensional (two spatial, frequency, polarization) image â€œcubesâ€ for transient astronomical sources. The pipeline was developed specifically to address data produced by the LOFAR Transients Key Science Project, but is also applicable to a range of other instruments.  The TraP codebase provides the pipeline definition itself, as well as a number of supporting routines for source finding, measurement, characterization, and so on. Some of these routines are also available as stand-alone tools.  ### Key features  * Customisable â€˜quality-controlâ€™ steps to weed out bad images before processing. * Built-in sourcefinder optimized for radio-synthesis images. * Optional source-fitting constraints (only fit point-sources, avoid fitting sources near to edge of image, etc). * Source-association incorporates knowledge of positional errors (using the DeRuiter radius algorithm) - this means much less trial-and-error tweaking of source-association parameters when working with a new dataset. * â€˜Skyregionâ€™ tracking - this keeps a record of which parts of sky have been previously surveyed, and to what faint limit, allowing for better separation of real transients and marginal steady-source detections. * Variability metrics and cataloguing for every source - no need to choose transient-detection thresholds ahead of time, simply sort through the data after processing and judge for yourself. * Position monitoring and null-detection tracking. â€˜Forcedâ€™ source-fits are attempted at positions where a source has been previously detected, or where a monitoring location has been manually specified, allowing for better detection of sources near to the faint limit. * All source measurements are stored in a standard SQL database; users can write their own custom data-extraction and analysis tools if desired. * Ready-made web-based data-exploration interface. TraP is accompanied by Banana, a web-based tool which allows astronomers to sort and search source-catalogues without requiring any local installation or programming. Provides interactive plots, links to external catalogue searches, and more. * Support for multiple data formats and telescopes. TraP can process both FITS and CASA MeasurementSet formats, and it is usually quite straightforward to add support for a new telescope.   See https://tkp.readthedocs.io/en/latest/introduction.html",https://github.com/transientskp/tkp,transientskp/tkp
042b4edb-4aa3-4297-b1ed-2c8090a34335,parcels,Parcels,"Parcels (which is a â€˜backronymâ€™ for â€œProbably A Really Computationally Efficient Lagrangian Simulatorâ€) is a set of Python classes and methods to create customisable particle tracking simulations using output from Ocean Circulation models. Parcels can be used to track passive water parcels in what is often referred to as Lagrangian Ocean Analysis ([Van Sebille _et al_ 2018](https://doi.org/10.1016/j.ocemod.2017.11.008)), as well as active particulates such as plastic and fish.  Parcels has been used in more than 80 peer-reviewed articles. It has been used in applications as diverse as global oceanic transport of microplastic particles (e.g., [Wichmann _et al_ 2019](https://doi.org/10.1088/2515-7620/ab4e77)), planning of routes of Underwater Autonomous Vehicles ([Boulares and Barnawi 2021](https://doi.org/10.3389/frobt.2022.958930)), mapping of the genetic connectivity and of fish ([Sefc _et al_ 2020](https://onlinelibrary.wiley.com/doi/10.1111/jbi.13946), [Schilling _et al_ 2020](https://doi.org/10.1111/fog.12473), [Lindoâ€Atichati _et al_ 2020](https://doi.org/10.1029/2019JC015348)) and assessment of the role of swimming in turtle hatchling survival ([Le Gouvello _et al_ 2020](https://doi.org/10.1016/j.ecolmodel.2020.109130)).   Parcels supports a multitude of Ocean General Circulation Model outputs, including from NEMO, MITgcm, HYCOM, POP, MOM and ROMS/CROCO. In general, it supports any vertical grid and (curvi)linear grid with the only limitation that it does not yet support unstructured meshes.  The unique feature of Parcels, compared to other community codes such as Ariane ([Blanke and Raynaud 1997](https://doi.org/10.1175/1520-0485(1997)027%3C1038:KOTPEU%3E2.0.CO;2)), TRACMASS ([DÃ¶Ã¶s _et al_ 2017](https://doi.org/10.5194/gmd-10-1733-2017)), OpenDrift ([Dagestad _et al_ 2018](https://doi.org/10.5194/gmd-11-1405-2018)), CMS ([Paris _et al_ 2013](http://dx.doi.org/10.1016/j.envsoft.2012.12.006)), LADIM ([Ã…dlandsvik and Sundby 1994](https://www.ices.dk/sites/pub/Publication%20Reports/Marine%20Science%20Symposia/ICES%20Marine%20Science%20Symposia%20-%20Volume%20198%20-%201994%20-%20Part%2033%20of%2063.pdf)) or TrackMPD ([JalÃ³n-Rojas _et al_ 2019](https://doi.org/10.1016/j.marpolbul.2019.02.052)), is that in Parcels it is extremely simple to encode the â€˜behaviorâ€™ of particles.   Using what are called â€˜kernelsâ€™ in Parcels, users can write simple functions that act on the virtual particles and make them e.g., sink, beach, fragment or die. These actions can depend on the local conditions â€“ such as sea water temperature, current speed or any other field that comes out of an Ocean General Circulation Model â€“ as well as on properties of the particles themselves such as their age, density, or size. These kernels can then be combined with each other and with pre-loaded kernels such as for advection and diffusion. This kernel-driven computation makes Parcels highly versatile and adaptable to different types of virtual particles, including plastic and plankton as well as turtles and tuna ([Scutt Phillips _et al_ 2016](https://doi.org/10.1016/j.pocean.2018.04.007)).",https://github.com/OceanParcels/parcels,OceanParcels/parcels
04560da4-e575-41fd-b0eb-b90d4a6e581d,4tu-models-and-optimization-tools-for-a-novel-3d-printed-capacitive-shear-stress-sensor,Models and Optimization Tools for a Novel 3D Printed Capacitive Shear Stress Sensor,"These files contain the models and optimization tools for study and development of a 3D-printed capacitive shear stress sensor. This research is described in our research paper:  Oprel, J., Wolterink, G., Schilder, J., Krijnen, G. (2023). Novel 3D Printed  Capacitive Shear Stress Sensor. Journal of Additive Manufacturing.  doi: 10.1016/j.addma.2023.103674  The models are used to study the electrical and the mechanical behavior of printed sensing structures. The main models are made using Matlab. Additionally, Finite Element (FE) models are used to verify these results. This is done using Comsol Multiphysics, which is controlled from a Matlab environment using 'Comsol livelink for Matlab'. The models are integrated in an optimization tool, in the form of a Matlab App.",,
046adf5f-3f88-4d2a-9b41-78853eb6e513,python-pcl,python-pcl,* Point clouds are accessible as Numpy arrays * Loading and saving of (possibly colored) point clouds * Registration and boundary detection  The library was written for use in the Via Appia project; it is based on the work [here](https://github.com/strawlab/python-pcl). Example usage and support for the common LAS format can be found in the [PattyAnalytics](https://github.com/NLeSC/PattyAnalytics) repository.,https://github.com/NLeSC/python-pcl,NLeSC/python-pcl
046b76ea-995c-49cb-ae48-9d80a407aa07,4tu-data-underlying-the-publication-fabrication-and-characterization-of-pneumatic-unit-cell-actuators,Data underlying the publication: Fabrication and Characterization of Pneumatic Unit Cell Actuators,"Pneumatic Unit Cells (PUC) are soft, hollow cylinder actuators fabricated using silicone. This repository contains the data presented in the MPDI Actuators journal titled 'Fabrication and Characterization of Pneumatic Unit Cell Actuators'.     The dataset contains:  Top layer thickness measurements of 0.6 mm, 0.9 mm, and 1.2 mm PUC designs (Figure 3).Free deflection measurements under static pressure condition of 0.6 mm, 0.9 mm, and 1.2 mm PUC designs, with three samples per design (Figure 7).Blocking force measurements under static pressure condition of 0.9 mm and 1.2 mm PUC designs, with three samples per design (Figure 8).Free deflection measurements under frequency condition of 0.9 mm and 1.2 mm PUC designs, with three samples per design (Figure 10).",https://data.4tu.nl/v3/datasets/1fef2678-5b29-4460-8c1c-0423fab9a85e.git,
04a5f063-e417-4750-8084-eddca218707d,annular,Annular,,https://gitlab.tudelft.nl/demoses/annular,
04ca26ae-c087-44e4-8012-abfaa2df2d1d,4tu-matlab-script-and-comsol-models-of-the-article-equivalent-continuum-for-viscoelastic-metamaterials, MATLAB script and COMSOL models of the article 'Equivalent continuum for viscoelastic metamaterials',Research data from the article 'Equivalent continuum for viscoelastic metamaterials' to generate the results that validate research findings. It includes MATLAB scripts and COMSOL models. A computational homogenization framework is used for solving wave propagation in the linear regime in composites that may exhibit local resonances due to microstructural heterogeneities. The homogenized material properties for a given metamaterial can be computed with the present MATLAB script. COMSOL models are used to generate the reference material behavior. Additional COMSOL models are used to solve finite metamaterial problems using the homogenized continuum approach with significantly reduced computational cost.,https://data.4tu.nl/v3/datasets/7187d549-0347-44f6-8355-a9a059ae11c3.git,
04e5b6e7-a454-48c5-bbb7-375059f99bb0,4tu-code-for-the-implementation-of-state-dependent-dynamic-tube-model-predictive-control,Code for the Implementation of State-Dependent dynamic tube model predictive control            ,"The code of this repository is used to implement and test a new algorithm called SDD-TMPC. It is a control algorithm that sacrifices a bit of optimality (much less than tube MPC) but returns robust solutions. In this repository, MPC, TMPC, and SDD-TMPC were implemented.     SDD-TMPC needs to have a model of boundaries of future disturbance. I used a fuzzy logic-based model trained with a genetic algorithm.     4 scenarios were created to compare all methods:  1. Following a path in an empty trajectory  2. Moving closely to the wall  3. Moving through a narrow corridor.  4. Avoiding an obstacle",https://data.4tu.nl/v3/datasets/9aefaae8-7eee-49e9-8545-88c67fac56fb.git,
04e8f730-ea9f-4b24-885c-f3cf824ff3e6,4tu-heast,HEAST,"The Hybrid Electric Aircraft Sizing Tool (HEAST) was developed at Delft University of Technology and is intended for the conceptual design of aircraft with (hybrid-) electric propulsion systems. These drivetrains open the space to new propulsor layouts commonly know as ""distributed propulsion"", where the aerodynamic interaction between propulsors and airframe can be significant. More specifically, the tool focuses on the preliminary sizing part of the conceptual design phase, where the designer translates the top-level aircraft requirements and design choices into a first layout and estimate of weights, wing area, and installed power. The tool is therefore a sizing tool and *not* an analysis tool: it cannot assess the performance of a predefined geometry/weight/etc.     The tool is implemented in MATLAB and is versatile because it allows the user to size aircraft with a wide array of powertrain types (conventional, serial, parallel, electric, turboelectric,...), propulsor layouts (leading-edge propellers, over-the-wing propellers,...), and can easily be adapted to account for different performance requirements. However, it requires a significant amount of input parameters and contains numerous (sub-) functions with many if-checks and iteration loops. It is therefore only recommended for users who have a detailed understanding of aircraft conceptual design *and* are versatile in the MATLAB language.",https://data.4tu.nl/v3/datasets/61cbe615-b7a8-4fa5-893b-9871f73e163f.git,
0526e4cf-5ca0-42f1-821c-9ed5b017e40a,salient-detector-python,SalientDetector-python,"* Provides implementation of a competitive Data-driven Morphological Salient Region (DMSR) image detector for computer vision, biodiversity and other researchers who need to analyzable large amount of images for discovering or identifying objects or scenes * Highly repetitive detection  and visualization of salient regions in multiple structured images of the same scene or object of interest  * Open-source and free code  * Successfully applied to various tasks: automated determination weather images are from the same scene, marine mammals photo-identification, wood species classification from microscopy images, etc.  * DMSR shows invariance to affine geometric image transformations as well as to photo-metric transformations such as blur and lighting  * DMSR gives better results than the popular MSER region detector  E. Ranguelova, ""A Salient Region Detector for structured images,""  2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA), Agadir, 2016, pp. 1-8. doi: 10.1109/AICCSA.2016.7945643",https://github.com/NLeSC/SalientDetector-python,NLeSC/SalientDetector-python
05d760b7-3ae0-4863-a603-9332edfe2a53,4tu-sda-thermo-elastic-python-scripts-underlying-the-master-thesis-thermo-elastic-analysis-of-a-spiral-dish-antenna-reflector-in-space,"SDA Thermo-Elastic Python Scripts, underlying the master thesis: Thermo-Elastic Analysis of a Spiral Dish Antenna Reflector In Space.","These scripts were used for Nathan van der Wielen's Master thesis for the thermo-elastic study of a novel spiral dish antenna (SDA) reflector. The following scripts are used with ESATAN-TMS, Abaqus, and a Python IDE.   Base Reflector Thermo-Elastic Analysis.py -Â reads .csv data of heat flux, surface area and time from 7 different ESATAN output files for all LPM nodes, builds a normal reflector model in Abaqus, assignes the heat fluxes to the correct surfaces, performs a thermo-elastic analysis, and outputs a thermal and deflection csv file.   Spiral Dish Antenna Reflector Thermo-Elastic Analysis.py - same as the previous python script but for an SDA reflector.   SDA and Reflector Thermo-Elastic Analysis Result Study.py - Reads the output files from the different scripts for the different cases, calculates, and output results/plots comparing different cases.",,
05db2170-5a1d-4d27-a698-f4d0d54d64be,canwin,CanWIN,"**The Canadian Watershed Information Network (CANWIN)** is a web based open access data and information network created by Environment Canada as part of the Lake Winnipeg Basin Initiative under Canadaâ€™s Action Plan on clean water. It was created in order to help address key water quality issues within the lake and its contributing watersheds. In 2012 management of the network transferred to the University of Manitoba under CEOS.  The Nelson River Watershed covers over 1 million square km. It covers 4 (four) Canadian provinces and four (4) U.S. States. The CANWIN provides a central open access data hub where researchers, decision makers, government agencies, organizations and the community can visualize and view the WHO, WHAT WHEN and WHERE of the basin. (Who is working in the basin, and WHAT, WHEN and WHERE are they doing it?); Searchable maps and tables of research allow users to gain a better understanding of the projects and activities occurring within the basin.  The CANWIN aims to aids research, education and decision making in the basin through three key strategies: Aid Transparency, Build Understanding, Create Awareness.  These key strategies allow the CANWIN to facilitate the management of natural and anthropogenic resources in the basin by integrating multiple information and data sources and expertise into a central open access resource.",https://cwincloud.cc.umanitoba.ca/canwin_public,
0607d5cc-f7a5-4982-a41a-adb179b3cb8e,4tu-matlab-scripts-created-during-the-work-on-multi-t-approach-for-peak-locking-error-correction-and-uncertainty-quantification-in-piv,"MATLAB scripts created during the work on ""Multi-Î”t approach for peak-locking error correction and uncertainty quantification in PIV""",This dataset contains MATLAB scripts.,,
064e58c4-f98f-482c-9b27-eae6e3b3da25,4tu-simplicial-convolutions,simplicial_convolutions,"This implements the simplicial convolutional filters in Matlab, together with their design and applications. In this work, we have developed filters of processing data defined on the edge space of a simplicial 2-complex, and Fourier analysis of such signals. To achieve certain filtering, we proposed different ways of designing the required filters.",https://data.4tu.nl/v3/datasets/1c8621e9-11e7-45ff-a223-ce975871e304.git,
06740b4b-1a81-4ded-88d6-3b989b3f7122,4tu-data-underlying-the-bsc-project-an-analysis-of-java-release-practices-on-github,"Data underlying the BSc project:  ""An analysis of Java release practices on GitHub""","This dataset contains the following inside a tar.zst file:  A list of all Java repositories on GitHub in a CSV formatThe POM.xml file from those repositories if there was one at the root of the repoA sample of 500 000 repositories thatHave been searched recursively for POM.xml filesOf those that have a POM.xml file an 'effective' POM.xml has been createdOf those that have distribution repositories configured, GitHub workflow files if they exista report.json file that contains aggregate information of the sample   The scraper written to retrieve this data is also included.     This dataset was created for a Computer Science Bachelor Research Project titled ""An analysis of Java release practices on GitHub"" by Vivian Roest.",https://data.4tu.nl/v3/datasets/f38d8fdd-fe95-4bd4-968e-71db238de45e.git,
068e1476-c254-47bb-a47a-2b64bb2fed4e,sfb-annotator,sfb-annotator,"- web application that enables semantic annotation of historical collection using controlled vocabularies and ontologies - a user draws  bounding boxes over image scans and describes their contents (e.g., an organism, with associated measurements or facts, that was found at some location and/or point in time) - the annotations of transcribed text are stored in a knowledge base (RDF store) and are accessible via a SPARQL endpoint and Web API  The Semantic Field Book Annotator is a web application developed for domain experts to harvest structured annotations from field books, drawings and specimen labels of natural history collections. Users can draw bounding boxes over (zoomable) image scans of historical field notes, to which annotations can be attached. All metadata regarding an annotation event, annotation provenance, transcription and semantic interpretation of the text are stored in a knowledge base, which is accessible via a SPARQL endpoint and Web API.",https://github.com/LINNAE-project/SFB-Annotator,LINNAE-project/SFB-Annotator
06c151ed-a8d1-4b0e-86f6-5f815e78e211,phenotips,PhenoTips,"PhenoTips is based in Toronto, North Americaâ€™s fastest-growing tech hub, just steps from the MaRS Discovery District and leading healthcare institutions. Surrounded by innovation, we are inspired to challenge the status quo of medical care in genetics, working towards a future in which patient data is unified and usable, rather than siloed in EHR systems and hospital filing cabinets.",https://github.com/phenotips/phenotips,phenotips/phenotips
06e9187e-c76b-40a4-bb2e-2529d0a7547a,drug-named-entity-recognition,Drug named entity recognition,"# ðŸ’Š Drug named entity recognition  Developed by Fast Data Science, https://fastdatascience.com  Source code at https://github.com/fastdatascience/drug_named_entity_recognition  Tutorial at https://fastdatascience.com/drug-named-entity-recognition-python-library/  This is a lightweight Python library for finding drug names in a string, otherwise known as [named entity recognition (NER)](https://fastdatascience.com/named-entity-recognition/) and named entity linking.  Please note this library finds only high confidence drugs and doesn't support misspellings at present.  It also only finds the English names of these drugs. Names in other languages are not supported.  It also doesn't find short code names of drugs, such as abbreviations commonly used in medicine, such as ""Ceph"" for ""Cephradin"" - as these are highly ambiguous.  # ðŸ’»Installing drug named entity recognition Python package  You can install from [PyPI](https://pypi.org/project/drug-named-entity-recognition).  ``` pip install drug-named-entity-recognition ```  If you get an error installing, try making a new Python environment in Conda (`conda create -n test-env; conda activate test-env`) or Venv (`python -m testenv; source testenv/bin/activate` / `testenv\Scripts\activate`) and then installing the library.  The library already contains the drug names so if you don't need to update the dictionary, then you should not have to run any of the download scripts.  If you have problems installing, try our [Google Colab](https://colab.research.google.com/github/fastdatascience/drug_named_entity_recognition/blob/main/drug_named_entity_recognition_example_walkthrough.ipynb) walkthrough.  # ðŸ’¡Usage examples  You must first tokenise your input text using a tokeniser of your choice (NLTK, spaCy, etc).  You pass a list of strings to the `find_drugs` function.  Example 1  ``` from drug_named_entity_recognition import find_drugs  find_drugs(""i bought some Prednisone"".split("" "")) ```  outputs a list of tuples.  ``` [({'name': 'Prednisone', 'synonyms': {'Sone', 'Sterapred', 'Deltasone', 'Panafcort', 'Prednidib', 'Cortan', 'Rectodelt', 'Prednisone', 'Cutason', 'Meticorten', 'Panasol', 'Enkortolon', 'Ultracorten', 'Decortin', 'Orasone', 'Winpred', 'Dehydrocortisone', 'Dacortin', 'Cortancyl', 'Encorton', 'Encortone', 'Decortisyl', 'Kortancyl', 'Pronisone', 'Prednisona', 'Predniment', 'Prednisonum', 'Rayos'}, 'medline_plus_id': 'a601102', 'mesh_id': 'D018931', 'drugbank_id': 'DB00635'}, 3, 3)] ```  You can ignore case with:  ``` find_drugs(""i bought some prednisone"".split("" ""), is_ignore_case=True) ```",https://github.com/fastdatascience/drug_named_entity_recognition,fastdatascience/drug_named_entity_recognition
071018e8-d425-457c-a425-bbb4d77eea24,4tu-visible-vapor-plumes-in-a-tropical-wet-forest-r-script,Visible vapor plumes in a tropical wet forest: R script,"This script was used to characterize the micro-meteorological conditions when visible vapor plumes form in tropical ecosystems. The script runs in the open-source software R, and the require packages are mentioned on the script.",,
07574c96-d90d-4a6f-b40d-10be73c847b3,parareal,parareal,"This package implements the Parareal algorithm in Python. Parareal is an algorithm for parallel-in-time integration, meaning that it attempts to parallelize the otherwise completely sequential time integration of an ordinary differential equation (ODE). The algorithm works by using two methods to solve the ODE: a cheap and coarse method and a more computationally intensive fine method. This will only work if the coarse method is good enough so that we can initiate multiple fine integrations from a coarsely estimated intermediate condition.  This Python module implements Parareal as a so-called black-box method. This means that the algorithm doesn't need to know about the details of the simulation. The user writes a script containing a fine and a coarse integrator. The parareal module then manages the computation using dask.",https://github.com/parallelwindfarms/parareal,parallelwindfarms/parareal
075cdd96-1cfe-427b-802d-68928414c9d4,candig-chord,CanDIG CHORD,"CHORD is a set of microservices that allow the sharing, management, and discovery of genomic and health data across a federated network. CHORD heavily benefits from and utilizes GA4GHâ€™s efforts to create standard APIs and formats for genomic and health data transfer and storage.",https://github.com/c3g/chord_singularity,c3g/chord_singularity
076cbe33-9d80-48ae-b734-29dbc8b5d2e0,4tu-ml-bias-prediction-mw-radiances,ML-bias-prediction-MW-Radiances,"HARMONIE-AROME is a regional Numerical Weather Prediction (NWP) model used for short- range weather forecasting in several operational centres across Europe. Microwave radiance observations from satellites are one of the main contributors to its forecast skill, but these observations contain biases that need to be corrected. Currently, in HARMONIE-AROME, bias correction is performed using a Variational Bias Correction method (VarBC), which consists of linear models including different bias predictors alongside their bias coefficients. However, this method is computationally intensive. This study investigates the potential of machine learning models to emulate the VarBC method by predicting the bias coefficients of its linear equations, offering a more computationally efficient alternative. This research uses data corresponding to the microwave radiance observations from the instruments AMSU-A, MHS and MWHS2. The machine learning models were trained on data from the DINI domain and subsequently tested on a new domain. The findings reveal that the machine learning models successfully emulate VarBC on a new, unseen domain and that they can be trained within seconds/minutes. Moreover, the predictions generated by these models are immediately usable without requiring a spin-up phase. The study further reveals that including bias predictors additional to those preselected by HARMONIE-AROME does not enhance the prediction accuracy. Furthermore, the research suggests that data from some instruments are helpful in predicting bias coefficients from other instruments.&nbsp;",https://data.4tu.nl/v3/datasets/30fe17c5-c43d-4833-a464-33029b779ccc.git,
079cc55b-5296-4c6f-a28d-8921cffa25e8,stac2dcache,STAC2dCache,STAC2dCache is a Python tool to create and manipulate catalogs implemented using the SpatioTemporal Asset Catalog (STAC) specifications on a dCache storage system such as the infrastructure available at SURF.  It is based on PySTAC and it offers the following additional functionalities:  - download remote assets to the local filesystem or to a dCache storage; - load assets using a set of predefined drivers (e.g. for raster data and text files),https://github.com/NLeSC-GO-common-infrastructure/stac2dcache,NLeSC-GO-common-infrastructure/stac2dcache
07d2e615-1e5b-484e-9725-8868c08cab86,i-vresse-workflow-builder,i-VRESSE workflow-builder,* The builder allows you to create a complex TOML formatted config file based on a set of JSON schemas. * Build from a React component library for flexible customization,https://github.com/i-VRESSE/workflow-builder,i-VRESSE/workflow-builder
07f4620c-2eae-4545-a731-268c1bce0246,overture-arranger,Overture Arranger,"# What is Arranger? Arranger is a collection of reusable components for creating centric search portals with Elasticsearch. Arranger consists of the following components:  * Arranger Search API provides a layer that sits above your Elasticsearch cluster to expose a data-model aware GraphQL API, generated from your own Elasticsearch index mapping. * Arranger Components provides a rich set of UI components that are configured to speak to the search API. * Arranger Admin provides the API and UI for configuring the search API and content management for the search portal.  Arranger is one of many products provided by Overture and is completely open-source and free for everyone to use.  # Features * GraphQL API for query flexibility. * SQON query filter notation balances between human-interpretability and machine-readability to simply search. * Admin UI for API configuration and content management. * Configuration import and export for easy migration.",https://github.com/overture-stack/arranger,overture-stack/arranger
086efc80-8149-41c0-a581-d9bc99e36fa4,fairseco,FAIRSECO,"The UU has developed a software search engine for the worldwide software ecosystem(SearchSECO). They have done so by mining software repositories on the method level, instead of the project or source-code level. Abstract representations of the methods are hashed and stored in a scalable distributed database that stores millions of methods from the most common programming languages. The database can be used to analyze software reuse at the method level, follow method evolution over time, and perform vulnerability detection across multiple projects.  The research software(ReSoft)ecosystem can benefit from this technology, as it enables research software engineers to rapidly find information about research software, that can improve FAIRness. Using a Github action, we try to collect data about a research software project and compile this into a clear and concise overview. In this way we make research software more Findable, Accessible, Interoperable, and Reusable (FAIR).  The intended beneficiaries of the project are Research Software Engineers (RSEs), who benefit from the FAIRSECO dashboard, which enables them to monitor the impact and FAIRness of their software. Organizations that produce ReSoft can use the tool to report on the impact their software has made. Finally, empirical software engineering researchers can use the  content in the SearchSECO DB to analyze the ReSoft ecosystem at a scale that was impossible before.",https://github.com/QDUNI/FairSECO,QDUNI/FairSECO
08a83f46-6799-4dd5-bda3-39f82e1ca6a1,4tu-software-tool-associated-to-the-master-thesis-multi-objective-optimisation-in-grid-shell-design,Software tool associated to the master thesis: Multi-Objective optimisation in grid-shell design,"This file is the main tool file developed in Grasshopper.     The Script is operational and working.  The computation time is roughly 3sec (estimation done in a computer with parts from 2020-2021).     Some clusters have passwords, please contact me through my LinkedIn in order to have access to them.",,
08e8766b-28c7-47ce-a704-8405d77c6307,distance-explainer,Distance explainer,,https://github.com/dianna-ai/distance_explainer,dianna-ai/distance_explainer
09449848-a99e-4588-a5bb-b384dd0337e7,emma,Emma,"* It is designed for users deploying Spark and DockerSwarm clusters in a cloud infra-structure. * It helps the user to prepare cloud virtual machines * The provision of machines is done with Ansible, an automation tool for IT infra-structure.  * It provides command line access to the users to install the required libraries and systems, configure them, start/stop services, add new modules for Jupyter notebooks, and even update the firewall  Emma is an open-source project to create a platform for development of applications for Spark and DockerSwarm clusters. The platform runs on an infra-structure composed by virtual machines that must be reachable by SSH. The machines are either cloud virtual machines or Vagrant machines. The latter tool allows the platform to be simulated on a local machine, i.e. in a local development environment.  Once the machines are prepared, the servers are provisioned using Ansible, an automation tool for IT infra-structure. Ansible playbooks are used to create a storage layer, processing layer, and JupyterHub services. The storage layer offers two flavors of storage, file-base by GlusterFS and Hadoop Distributed File System (HDFS), and object-based using Minio. The processing layer has a Apache Spark cluster and a Docker Swarm sharing the storage instances.   With Ansible we are able to deploy a platform with the same features at different locations, such as local cluster, national infra-structure, or even a commercial cloud provider. Such a feature allows us to have tool-provenance for easily repeatability of experiments between scientists.",https://github.com/nlesc-sherlock/emma,nlesc-sherlock/emma
0945ccca-53af-4312-8ef8-5120491110cf,ahn2webviewer,AHN2 pointcloud viewer,"* For all users interested in geo-data that need an intuitive interface to browse this data and / or extract parts of it for other uses, this webGL Pointcloud viewer is the best of the bunch, because it is the only one out there capable of displaying 640 billion points. Geo-scientists, archeologists, spatial planners, biologists or environmentalists performing exploratory analysis of massive LiDAR data sets ('point clouds') composed of billions of points using Potree-powered web-based viewer.  * Tools to enable interactive web-based visual analytics on massive point clouds.  * These are the only tools that enable interactive web-based visual analytics on massive point clouds. No other web-based tool can display a point cloud this massive. 640 billion points.",https://github.com/NLeSC/ahn-pointcloud-viewer,NLeSC/ahn-pointcloud-viewer
094638fd-7cef-4772-8b85-298e458c7b34,pyelsepa,PyELSEPA,- a programmable interface to ELSEPA,https://github.com/eScatter/pyelsepa,eScatter/pyelsepa
09d9e2b1-d0df-477d-91cf-7c141676c23c,4tu-mlat,MLAT,"In this software, 6 different algorithms were implemented to localize aircraft based on MLAT measurements. Moreover, 2 different algorithms were implemented to track over time the location of noisy computed locations. The objective of this software was to compare these algorithms. All input data were generated in simulations and the output data were processed by the implemented algorithms.",https://data.4tu.nl/v3/datasets/8401e595-be07-407f-9b41-34517b444117.git,
09e21c8e-487b-4664-84e9-feeb81e7a50b,mibiremo,mibiremo,,https://github.com/MiBiPret/mibiremo,MiBiPret/mibiremo
0a0156b1-3c2d-4df1-9fdb-6365ceaa39fd,4tu-matlab-scripts-belonging-to-the-paper-predicting-metabolic-adaptation-under-dynamic-substrate-conditions-using-a-resource-dependent-kinetic-model-a-case-study-using-saccharomyces-cerevisiae,"MATLAB scripts belonging to the paper ""Predicting metabolic adaptation under dynamic substrate conditions using a resource dependent kinetic model: A case study using Saccharomyces cerevisiae""","MATLAB scripts belonging to the paper ""Predicting metabolic adaptation under dynamic substrate conditions using a resource dependent kinetic model: A case study using Saccharomyces cerevisiae""Contains scripts for:- Converting proteome datasets to the model proteome allocation format.- Generating optimized proteomes for both steady-state and dynamic (feast/famine) conditions under user-specified conditions.- Determining the overcapacity present in proteome provided as input under user-specified conditions.",,
0a27dcf1-3f13-4a9e-8014-c84ad4f953f7,salient-descriptor-matlab,SalientDescriptor-matlab,"* Provides implementation of a competitive Shape and Moment Invariant (SMI) descriptor  for computer vision, biodiversity and other researchers who need to match images for identifying objects or scenes * Simple and scalable description and matching of salient regions in multiple structured images of the same scene or object of interest  * Open-source code for researchers who are used to MATLAB * Successfully applied to automated determination weather images are from the same scene for the Oxford (VGG) and OxFrei benchmarks * SMI gives better results than the popular SURF descriptor on MSER regions with less computations  Ranguelova, E., Local Shape and Moment Invariant Descriptor for Structured Images,  Proceedings of the 19th Irish Machine Vision and Image Processing (IMVIP) conference,  Maynooth, Ireland, 2017, pp. 245-248",https://github.com/NLeSC/SalientDescriptor-matlab,NLeSC/SalientDescriptor-matlab
0a80741a-8607-4683-94da-c3200a64bd40,4tu-cross-domain-classification-of-moral-values-code,Cross-Domain Classification of Moral Values - code,"Code for the paper ""Cross-Domain Classification of Moral Values"", published at NAACL '22. This code implements a cross-domain evaluation of BERT, fastText, and LSTM across the seven datasets of MFTC. After being trained, the models are evaluated for generalizability, transferability, and catastrophic forgetting.",https://data.4tu.nl/v3/datasets/ae8d98d8-629f-4c61-bf05-8c15b5fb2ab0.git,
0ab7a043-33a4-4735-bd53-6956fbc32dd2,introduction-to-deep-learning-lesson,Introduction to deep learning lesson,,https://github.com/carpentries-incubator/deep-learning-intro,carpentries-incubator/deep-learning-intro
0ad135ec-69ff-4386-bf51-7c465a20622e,eecology-sms-reciever,eEcology-SMS-reciever,"* Provides a way for eEcologists to read the SMS messags sent by the UvA-BiTS trackers * It receives messages from SMSSync Android app and stores them in the UvA-BiTS database * When a bird is far away from a base station and an ecologist wants a low frequency way of tracking the bird, this software will capture the SMS message sent by the tracker.",https://github.com/NLeSC/eEcology-SMS-reciever,NLeSC/eEcology-SMS-reciever
0b086277-d115-42b8-a3b6-a1613a8195a5,4tu-fast-rotor-loss-calculations-in-fractional-slot-permanent-magnet-machines,Fast Rotor Loss Calculations in Fractional-Slot Permanent Magnet Machines,"The COMSOL model enables the user to calculate the eddy current losses in the rotor of the permanent magnet machines. The detailed explanation of the model can be found in the linked reference paper. Main purpose is to show how eddy current losses of different frequencies in the rotor can be evaluated with a single frequency domain calculation, provided they arise from a single time harmonic frequency in the stator. This method speeds up the rotor loss calculation significantly, albeit at the cost of reduced accuracy.For comparison purposes, time domain model and a frequency domain model are uploaded.",,
0b374be5-d2a5-41e8-99c5-65573604a9d8,4tu-paramount-parallel-modal-analysis-of-large-datasets,PARAMOUNT: parallel modal analysis of large datasets,"PARAMOUNT: parallel modal analysis of large datasets    PARAMOUNT is a python package developed at University of Twente to perform modal analysis of large numerical and experimental datasets. Brief video introduction into the theory and methodology is presented Â here.    Features  Â    - Distributed processing of data on local machines or clusters using Dask Distributed  - Reading CSV files in glob format from specified folders  - Extracting relevant columns from CSV files and writing Parquet database for each specified variable  - Distributed computation of Proper Orthogonal Decomposition (POD)  - Writing U, S and V matrices into Parquet database for further analysis  - Visualizing POD modes and coefficients using pyplot     Using Â PARAMOUNT    Make sure to install the dependencies by running `pip install -r requirements.txt`    Â    Refer to csv_example to see how to use PARAMOUNT to read CSV files, write the variables of interest into Parquet datasets and inspect the final datasets.    Refer to svd_example to see how to read Parquet datasets, compute the Singular Value Decomposition, and store the results in Parquet format.    To visualize the results you can simply read the U, S and V parquet files and your plotting tool of choice. Examples are provided in viz_example.    Author and Acknowledgements    This package is developed by Alireza Ghasemi (alireza.ghasemi@utwente.nl) at University of Twente under the MAGISTER (https://www.magister-itn.eu/) project. This project has received funding from the European Unionâ€™s Horizon 2020 research and innovation program under the Marie SkÅ‚odowska-Curie grant agreement No. 766264.",,
0b3c0bf3-a3d5-464f-97ed-8e207398365a,4tu-research-compendium-for-fair-sharing-is-caring-poster,Research compendium for FAIR Sharing is Caring poster,# FAIR Sharing is Caring     Poster presented at ESHE 2023.     Contents:     - Analysis  &nbsp;+ data retrieval: `data-raw/DATASET.R`  &nbsp;+ data cleaning: `scripts/data-cleaning.R`  &nbsp;+ processed survey data: `data/paleoanth-clean.csv`  &nbsp;+ survey questions: `data/survey-questions.csv`        - Report  &nbsp;+ rendered: `poster-analysis.html` or https://bbartholdy.github.io/eshe2023-data-sharing/poster-analysis.html  &nbsp;+ source code: `poster-analysis.qmd`  &nbsp;+ references cited: `references.bib`,https://data.4tu.nl/v3/datasets/1907198f-7f25-46e1-bf45-dd8f18b9a7fa.git,
0b826d5c-5995-4314-8dc3-5e4634d99474,4tu-artifact-of-knor-reactive-synthesis-using-oink,Artifact of Knor: reactive synthesis using Oink,"This is the artifact belonging to the paper Knor: reactive synthesis using Oink.     The artifact requires the Virtual Machine of TACAS23 obtainable via https://zenodo.org/doi/10.5281/zenodo.7113222     For information on using the artifact, consult the README files.",https://data.4tu.nl/v3/datasets/56144303-c2b3-49f8-8380-d094c6dd5473.git,
0b940308-a80d-4b29-86b3-ef34c9c59366,dive,Dive,"* Provides interactive web-visualization of data in 3D/2D for data scientists and scientists. * One can explore millions of data points interactively * Has been used in bioinformatics and forensics projects   When a data scientist has a problem like clustering data, a usual starting point is to ""embed"" and ""see"" the data in 3D or 2D. Ideally she would like to explore data interactively, by being able to search or color the data based on provided metadata. As scientific datasets tend to become larger, current visualization tools are facing scalability issues. Dive is made to provide interactive exploration of (embedded) data in 3D or 2D for million of points.",https://github.com/NLeSC/DiVE,NLeSC/DiVE
0b993064-49b7-4d8d-9ac0-14d5f9d465d6,4tu-code-repository-of-advancing-data-quality-assurance-with-machine-learning-a-case-study-on-wind-vane-stalling-detection,"Code Repository of ""Advancing Data Quality Assurance with Machine Learning: A Case Study on Wind Vane Stalling Detection""","The code repository to replicate the work (e.g., figures and results) from the publication: ""Advancing Data Quality Assurance with Machine Learning: A Case Study on Wind Vane Stalling Detection"". Repository includes dedicated Python files and a README document.",,
0b9f4075-d72f-46d5-a017-9dcb23b83231,wrfpy,WRFpy,"* Provides a simple user-editable JSON configuration file * Integrates with the Cylc workflow engine to access distributed computing and storage resources as well as monitoring * Supports data assimilation through WRFDA and postprocessing through NCEP Unified Post Processing System (UPP)  WRFpy is a python application that provides an easy way to set up, run, and monitor (long) Weather Research and Forecasting (WRF) simulations. It provides a simple user-editable JSON configuration file and integrates with Cylc to access distributed computing and storage resources as well as monitoring. Optionally, WRFpy allows for data assimilation using WRF data assimilation system (WRFDA) and postprocessing of wrfinput files using the NCEP Unified Post Processing System (UPP).",https://github.com/era-urban/wrfpy,era-urban/wrfpy
0bb5c042-1272-447b-9496-02c4fad2abf1,code-auditor,Code Auditor,,https://github.com/SS-NES/code-auditor,SS-NES/code-auditor
0bdbe625-656c-4fe2-88d3-53decda423f7,cgc,Clustering Geo-data Cubes,"* Tool to perform co- and tri-cluster analysis  * Targets geospatial data sets * Includes functionalities to refine the clustering using k-means  * Multiple implementations to work with local or distributed systems  Clustering Geo-data Cubes (CGC) includes various implementations of co- and tri-clustering algorithms to efficiently carry out cluster analysis on both small and large data sets, using either local or distributed resources. Tutorials illustrate how to use this Python tool with real-world geospatial raster data.",https://github.com/phenology/cgc,phenology/cgc
0c06b4ad-4a2b-4daf-9c5e-5d1402f0fb69,4tu-shoreline-evolution-model-and-appendix-a-supplementary-data-to-the-manuscript-challenges-and-new-strategies-in-assessing-multidecadal-shore-platform-sandy-beach-evolution-from-aerial-imagery,Shoreline Evolution Model and Appendix A - Supplementary data to the manuscript Challenges and new strategies in assessing multidecadal shore platform sandy beach evolution from aerial imagery,Shoreline Evolution Model and Appendix A -Sensitive theoretical experiments to assess uncertainty in bedrock and beach face slopes definition,,
0c14c1a9-08cf-4f85-b8e9-6e31e8e99f96,4tu-vsids-learning-code-learning-variable-activity-initialisation-for-lazy-clause-generation-solvers,"VSIDS learning code: ""Learning Variable Activity Initialisation for Lazy Clause Generation Solvers""","Python code corresponding to the CPAIOR 2021 paper ""Learning Variable Activity Initialisation for Lazy Clause Generation Solvers"".",,
0c1b2e64-11ad-4c1c-8dfc-039285ef273d,4tu-python-notebook-to-generate-figure-3-panel-c-from-adaptability-and-evolution-of-the-cell-polarization-machinery-in-budding-yeast,"Python notebook to generate Figure 3 Panel C from: ""Adaptability and evolution of the cell polarization machinery in budding yeast""","This is the python code that generates the Figure 3 Panel B from paper: ""Adaptability and evolution of the cell polarization machinery in budding yeast"" with doi: https://doi.org/10.1101/2020.09.09.290510",,
0ca9a91c-ce0d-424d-8b23-733afa8fd382,petri-nets-on-hyperledger,Petri Nets on Hyperledger,,https://github.com/dl4ld/petrinet,dl4ld/petrinet
0d3b06a1-369d-403f-8464-275a2f22a787,nedextract,nedextract,"## Nedextract nedextract is developed to extract specific information from annual report PDF files that are written in Dutch. Currently it tries to do the following:  - Read the PDF file, and perform Named Entity Recognition (NER) using Stanza to extract all persons and all organisations named in the document, which are then processed by the processes listed below. - Extract persons: using a rule-based method that searches for specific keywords, this module tries to identify:     - Ambassadors     - People in important positions in the organisation. The code tries to determine a main job description (e.g. director or board) and a sub-job description (e.g. chairman or treasurer). Note that these positions are identified and outputted in Dutch.       The main jobs that are considered are:          - directeur         - raad van toezicht         - bestuur         - ledenraad         - kascommissie         - controlecommisie          The sub positions that are considered are:         - directeur         - voorzitter         - vicevoorzitter         - lid         - penningmeester         - commissaris         - adviseur          For each person that is identified, the code searches for keywords in the sentences in which the name appears to determine the main position, or the sentence directly before or after that. Subjobs are determine based on words appearing directly before or after the name of a person for whom a main job has been determined. For the main jobs and sub positions, various ways of writing are considered in the keywords. Also before the search for the job-identification starts, name-deduplication is performed by creating lists of names that (likely) refer to one and the same person (e.g. Jane Doe and J. Doe).  - Extract related organisations:     - After Stanza NER collects all candidates for mentioned organisations, postprocessing tasks try to determine which of these candidates are most likely true candidates. This is done by considering: how often the terms is mentioned in the document, how often the term was identified as an organisation by Stanza NER, whether the term contains keywords that make it likely to be a true positive, and whether the term contains keywords that make it likely to be a false positive. For candidates that are mentioned only once in the text, it is also considered whether the term by itself (i.e. without context) is identified as an organisation by Stanza NER. Additionally, for candidates that are mentioned only once, an extra check is performed to determine whether part of the candidate org is found to be a in the list of orgs that are already identified as true, and whether that true org is common within the text. In that case the candidate is found to be 'already part of another true org', and not added to the true orgs. This is done, because sometimes an additional random word is identified by NER as being part of an organisation's name.      - For those terms that are identified as true organisations, the number of occurrences in the document of each of them (in it's entirety, enclosed by word boudaries) is determined.     - Finally, the identified organisations are attempted to be matched on a list of provided organisations using the `anbis` argument, to collect their rsin number for further analysis. An empty file `./Data/Anbis_clean.csv` is availble that serves as template for such a file. Matching is attempted both on currentStatutoryName and shortBusinessName. Only full matches (independent of capitals) and full matches with the additional term 'Stichting' at the start of the identified organisation (again independent of capitals) are considered for matching. Fuzzy matching is not used here, because during testing, this was found to lead to a significant amount of false positives.   - Classify the sector in which the organisation is active. The code uses a pre-trained model to identify one of eight sectors in which the organisation is active. The model is trained on the 2020 annual report pdf files of CBF certified organisations.",https://github.com/Transparency-in-the-non-profit-sector/nedextract,Transparency-in-the-non-profit-sector/nedextract
0d75b7e9-f6eb-4c1a-8767-be5334e80663,piqmie,PIQMIe,"* integrates peptide/protein identifications and quantitations in a single database file (SQLite) * provides a concise summary of proteomics experiments   * enables integrated data analyses using R/Python   PIQMIe is a web-based tool that aids in reliable and scalable data management, analysis and visualization of semi-quantitative mass spectrometry (MS)-based proteomics experiments. PIQMIe readily integrates peptide and (non-redundant) protein identifications and quantitations, as obtained from MS data processed by the MaxQuant/Andromeda software, with additional biological information from the UniProtKB database, and makes the inter-linked data available in the form of a light-weight relational database (SQLite). Using the web interface, users are presented with a concise summary of their proteomics experiments in numerical and graphical forms, as well as with a searchable protein grid and interactive visualization tools to aid in the rapid assessment of the experiments and in the identification of proteins of interest. The web server not only provides data access through a web interface but also supports programmatic access through RESTful API.",https://github.com/arnikz/PIQMIe,arnikz/PIQMIe
0db45e54-3e58-4119-aced-a17437144a68,pism,Parallel Ice Sheet Model (PISM),"The Parallel Ice Sheet Model (PISM) is an open-source modelling framework for ice sheets and glaciers. It is parallel, thermodynamically-coupled and capable of high resolution. PISM has been [widely adopted as a tool for doing science](https://www.pism.io/publications/).  Development of PISM is supported by NASA grants 20-CRYO2020-0052 and 80NSSC22K0274 and NSF grant OAC-2118285.  ![](https://www.pism.io/img/basics/IceSheetDyn_Julius.jpg) *Graphic: J. Garbe*  PISM was used in several studies that contributed to major advances in the understanding and model representation of key physical processes which control the behavior of ice sheets, for example:  * Basal sliding: [Bueler et al. (2009)](https://doi.org/10.1029/2008JF001179) * Energy conservation: [Aschwanden et al. (2012)](https://doi.org/10.3189/2012JoG11J088) * Ice-front motion: [Albrecht et al. (2011)](https://doi.org/10.5194/tc-5-35-2011) * Calving: [Levermann et al. (2012)](https://doi.org/10.5194/tc-6-273-2012) * Fracture dynamics: [Albrecht and Levermann (2012)](https://doi.org/10.3189/2012JoG11J191) * Sub-shelf melt: [Reese et al. (2018)](https://doi.org/10.5194/tc-12-1969-2018) * Surface melt: [Zeitz et al. (2021)](https://doi.org/10.5194/tc-15-5739-2021) * Marine ice-cliff instability (MICI): [Schlemm et al. (2022)](https://doi.org/10.5194/tc-16-1979-2022)  ## Sea-level projections  ![](https://www.pism.io/img/basics/SLR_Edwards2021.png) *Source: [Edwards et al. (2021)](https://doi.org/10.1038/s41586-021-03302-y)*  The ice sheets on Greenland and Antartica are the largest freshwater reservoirs with a combined sea-level rise potential of more than 65 meters. Their mass loss and future contributions to sea-level rise in response to different scenarios of changing atmospheric and oceanic conditions can be estimated by applying PISM, as previously accomplished by e.g.:  * Antarctica   * ISMIP6: [Seroussi et al. (2020)](https://doi.org/10.5194/tc-14-3033-2020)   * LARMIP-2: [Levermann et al. (2020)](https://doi.org/10.5194/esd-11-35-2020)   * [Reese et al. (2020)](https://doi.org/10.5194/tc-14-3097-2020)   * [Winkelmann et al. (2015)](https://doi.org/10.1126/sciadv.1500589)   * SeaRISE: [Nowicki et al. (2013a)](https://doi.org/10.1002/jgrf.20081) * Greenland   * ISMIP6: [Goelzer et al. (2020)](https://doi.org/10.5194/tc-14-3071-2020)   * [Aschwanden et al. (2019)](https://doi.org/10.1126/sciadv.aav9396)   * SeaRISE: [Nowicki et al. (2013b)](https://doi.org/10.1002/jgrf.20076) * Both ice sheets   * [Edwards et al. (2021)](https://doi.org/10.1038/s41586-021-03302-y)   * [Golledge et al. (2019)](https://doi.org/10.1038/s41586-019-0889-9)   * [Winkelmann and Levermann (2013)](https://doi.org/10.1007/s00382-012-1471-4)   * SeaRISE: [Bindshadler et al. (2013)](https://doi.org/10.3189/2013JoG12J125)  ## Glacial cycle simulations  ![](https://www.pism.io/img/basics/GlacialCycle_Albrecht2020.png) *Source: [Albrecht et al. (2020b)](https://doi.org/10.5194/tc-14-633-2020)*  Long-term model simulations are helpful for the reconstruction of the glacial-interglacial history of the Earth's ice sheets. This contributes to a better understanding of dynamic threshold behavior and sea-level change in the past but also in the future. For the Antarctic Ice Sheet, an ensemble of glacial-cycle simulations in order to constrain paleo parameter sensitivities and boundary conditions has, for example, been performed using PISM by:  * [Albrecht et al. (2020a)](https://doi.org/10.5194/tc-14-599-2020), [Albrecht et al. (2020b)](https://doi.org/10.5194/tc-14-633-2020)  ## Long-term stability of ice sheets  ![](https://www.pism.io/img/basics/Hysteresis_Julius.png) *Source: Modified after [Garbe et al. (2020)](https://doi.org/10.1038/s41586-020-2727-5)*  Several positive and negative feedback mechanisms may impact the stability of ice sheets on long timescales. Examples include the positive surface-melt-elevation feedback or the negative isostatic solid-Earth rebound effect. When crossing critical thresholds, irreversible ice loss may follow. The overall effect of the interplay between various feedback mechanisms in the long term has been assessed in the following studies using PISM, among others:  * [Garbe et al. (2020)](https://doi.org/10.1038/s41586-020-2727-5) * [Golledge et al. (2017)](https://doi.org/10.1002/2016GL072422) * [Clark et al. (2016)](https://doi.org/10.1038/nclimate2923) * [Feldmann and Levermann (2015)](https://doi.org/10.1073/pnas.1512482112) * [Golledge et al. (2015)](https://doi.org/10.1038/nature15706) * [Winkelmann et al. (2015)](https://doi.org/10.1126/sciadv.1500589)   ## Coupling to other Earth system components  ![](https://www.pism.io/img/basics/CouplingComponents_Zwally2015.png) *Source: Modified after [Zwally et al. (2015)](https://doi.org/10.3189/2015JoG15J071)*  Ice sheets interact with other Earth system components, such as the atmosphere or the ocean. To take into account and study related feedback mechanisms as well as their effect on the dynamics of the Greenland and Antartic ice sheets, PISM has been coupled to other Earth system components:  * Ocean   * Potsdam Ice-shelf Cavity mOdel (PICO): [Reese et al. (2018)](https://doi.org/10.5194/tc-12-1969-2018)   * PISM-MOM: [Kreuzer et al. (2021)](https://doi.org/10.5194/gmd-14-3697-2021) * Atmosphere   * dEBM-simple: [Zeitz et al. (2021)](https://doi.org/10.5194/tc-15-5739-2021), [Garbe et al. (2023)](https://doi.org/10.5194/tc-17-4571-2023) * Solid-Earth   * Lingle-Clark: [Bueler et al. (2007)](https://doi.org/10.3189/172756407782871567)   * PISM-VILMA (work in progress)  ## Model intercomparisons  PISM has participated in numerous model intercomparison projects (MIPs). For a complete list, please see [MIPs & Collaborations](https://www.pism.io/collaborations/).  ## Support  If you are looking for help with PISM, feel free to join the PISM workspace on [Slack](https://join.slack.com/t/uaf-pism/shared_invite/enQtODc3Njc1ODg0ODM5LThmOTEyNjEwN2I3ZTU4YTc5OGFhNGMzOWQ1ZmUzMWUwZDAyMzRlMzBhZDg1NDY5MmQ1YWFjNDU4MDZiNTk3YmE). You can also contact the [developers team](https://www.pism.io/team/) directly or write an e-mail to [uaf-pism@alaska.edu](mailto:uaf-pism@alaska.edu).  ## Contributing  Bug reports, contributions of code, documentation, and tests are always appreciated. Please see [the PISM manual](http://www.pism.io/docs/contributing/) for instructions.",https://github.com/pism/pism,pism/pism
0db7809e-f1b7-4bca-919c-b866e278cfa2,4tu-software-for-3d-particle-averaging-of-smlm-data,Software for 3D particle averaging of SMLM data,Software for 3D particle averaging of SMLM data,,
0dc4dd75-c0ec-4710-92d7-1482a2982796,4tu-code-for-publication-vibration-induced-friction-modulation-for-an-oscillator-moving-on-an-elastic-rod,Code for publication: Vibration-induced friction modulation for an oscillator moving on an elastic rod,"MATLAB script to reproduce results from:  Sulollari, E., van Dalen, K.N., and Cabboi, A. (2025).   ""Vibration-induced friction modulation for an oscillator moving on an elastic rod.""   International Journal of Solids and Structures, 2025, 113572.     It corresponds to Chapter 6 of the dissertation:   ""Vibration-induced friction modulation""     The script generates the results shown in the figures of the paper.",,
0dd865b7-05f8-425c-9ae9-ca0962bc9fc9,grpc4bmi,grpc4bmi,"* Successfully used in a handful of hydrology simulation models * Models must implement the standard set of methods called the Basic Modeling Interface * Wraps a model in a server process and communicates with it via the included Python client * Possible to run multiple instance of model which could not be run within the same process before * Supports running a model in a variety of languages on the server side and interact with the model from Python * Connect to a remote model using a network connection to another server or a container like Docker or Singularity  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/grpc4bmi,eWaterCycle/grpc4bmi
0dda199c-a74f-432e-a1c7-89fa7bedc6a8,4tu-effects-of-plu-polling-interval,Effects of PLU Polling Interval,"This repository contains the code associated with the research paper titled ""The Effects of PLU Polling Interval on the Reconstructed OD Matrix: A Dutch Case Study Using a Data-Driven Method"". The paper is published in the TRR. This code repo has the overall purpose of understanding the effect of temporal resolution of data on the estimated origin destination matrix. The data source needed for running this code can be found under the name ""&nbsp;The input data associated with the publication: The Effects of PLU Polling Interval on the Reconstructed OD Matrix: A Dutch Case Study Using a Data-Driven Method"".",https://data.4tu.nl/v3/datasets/435972e3-7afe-4dd3-8fa2-83d2604b2b27.git,
0de17020-3ae9-4aa9-8b24-08d7354e86d4,4tu-software-for-simultaneous-orientation-and-3d-localization-microscopy-with-a-vortex-point-spread-function,Software for Simultaneous orientation and 3D localization microscopy with a Vortex point spread function,"This code is distributed as accompanying software for the article Simultaneous orientation and 3D localization microscopy with a Vortex point spread function by Christiaan N. Hulleman, Rasmus Ã˜. Thorsen, Eugene Kim, Cees Dekker, Sjoerd Stallinga, and Bernd Rieger.  Any reuse of this code should cite the original associated publication.",,
0e3dca4d-843b-40b6-91cb-31b697075714,mexca,mexca,"mexca is an open-source Python package which aims to capture human emotion expressions from videos in a single pipeline. The package implements the customizable yet easy-to-use Multimodal Emotion eXpression Capture Amsterdam (MEXCA) pipeline for extracting emotion expression features from videos. It contains building blocks that can be used to extract features for individual modalities (i.e., facial expressions, voice, and dialogue/spoken text). The blocks can also be integrated into a single pipeline to extract the features from all modalities at once. Next to extracting features, mexca can also identify the speakers shown in the video by clustering speaker and face representations. This allows users to compare emotion expressions across speakers, time, and contexts.  The package contains five components that can be used to build the MEXCA pipeline:  - FaceExtractor: Detects faces, encodes them into an embedding space, clusters the embeddings to link reoccuring faces, and extracts facial landmarks and action units. - SpeakerIdentifier: Performs speaker diarization, that is, detects speech and speech segments, encodes speakers into an embedding space, and clusters the embeddings. Attempts to answer the question: â€œWho speaks when?â€. - VoiceExtractor: Extracts voice features, such as pitch, associated with emotion expressions. - AudioTranscriber: Transcribes detected speech segments to text. - SentimentExtractor: Predicts sentiment scores for the transcribed text.",https://github.com/mexca/mexca,mexca/mexca
0e66487c-a9b6-44fd-a806-15141d1e6700,4tu-pybarsim,pyBarSim,pyBarSim is a Python package to simulate wave-dominated shallow-marine environments using Storms (2003)'s BarSim.     See https://github.com/grongier/pybarsim for more information about the program and how to use it.,https://data.4tu.nl/v3/datasets/b81bcaad-83ba-45c2-bb8a-b02632e0d9d2.git,
0fe8063b-d3ec-4bb1-9147-61c80e58dca1,4tu-complex-stylized-supply-chain-model-generator,complex_stylized_supply_chain_model_generator,"This repository is part of the Ph.D. thesis of&nbsp;Isabelle M. van Schilt, Delft University of Technology.     This repository presents a complex stylized supply chain discrete event simulation model of a counterfeit Personal Protective Equipment (PPE) supply chain. Additionally, this repository presents scripts for automatically generating a discrete event simulation model from a networkx graph. The generation of a large set of randomly generated networkx graphs based on real-world data, which can be automatically ran as a simulation model, is also presented. This contributes to research on structural uncertainty in models. This code is an extension of the Master Thesis of Bruno Hermans , Delft University of Technology.    The simulation models are developed in&nbsp;pydsol-core&nbsp;and&nbsp;pydsol-model&nbsp;. For the real-world data, we use the repository&nbsp;port_data_graphs&nbsp;to create various graph structures based on open-source data.",https://data.4tu.nl/v3/datasets/db19a784-ff45-46ba-a439-c5ab8e9366cf.git,
10304a64-01b4-426c-96e6-62bf3f329acd,knime-sstea,KNIME ss-TEA node,"* For bioinformaticians or cheminformaticians  who have no 3D structure of their protein, just a protein sequence and want to know which position is likely to bind a ligand. * It takes a big multi species multiple sequence alignment and a list of subfamily members to produce a score for each position in the sequence alignment. * Can tell which residue in a protein is likely to bind a ligand without 3D information * Ranked among best model in the GPCR dock 2010 competition.  ss-TEA is as part of [Snooker pharmacophore generation tool](https://pubs.acs.org/doi/abs/10.1021/ci200088d)",https://github.com/3D-e-Chem/knime-sstea,3D-e-Chem/knime-sstea
103e0e85-5f30-455f-ae58-1e92e32d04e7,4tu-code-and-data-underlying-the-publication-symmetrization-of-2d-polygonal-shapes-using-mixed-integer-programming, Code and data underlying the publication: Symmetrization of 2D Polygonal Shapes Using Mixed-Integer Programming,This repository contains the code and test dataset for the&nbsp;hypothesize-and-select-based 2D symmetrization method. The 2D shapes used in this research paper are manually created by using CAD software and can be found in the /data folder.,https://data.4tu.nl/v3/datasets/3303dc5b-8d06-4c56-8fd5-1c397d5c77be.git,
106cc59d-22b3-4658-936b-3049bc81d589,radiam,Radiam,,https://github.com/usask-rc/radiam/,usask-rc/radiam
10b80ab0-bfa8-4263-8caa-5a0905c004d6,h-gear,H-gear,"**H-GEAR** (Acronym for *Historiographing the Era of the American Revolution*) is an advanced tool for analyzing the transmission of political ideas during the American Revolution, seamlessly integrated with ShiCo (Shifting Concepts Through Time) software. This integration allows users to track shifts in key political concepts, such as liberty and freedom, by examining the evolution of language over time. thinking evolved over time.  H-GEAR leverages *ShiCo* to analyze the changing vocabulary around political ideas. By providing a set of keywords, users can explore how terms like â€œfreedomâ€ and â€œdemocracyâ€ evolved, offering deep insights into the ideological landscape of the Revolutionary era.  A key focus of the developed tool is on the distinction between liberal and republican ideas and how they were transmitted differently through correspondence networks. It combines semi-supervised topic modeling to identify and select politically-motivated letters with path-based temporal social networks to trace how political discourse spread between influential figures, accounting for the timing and flow of communication.  By focusing on correspondence data such as Founders Online and the Letters of Delegates to Congress, H-GEAR enables to map out networks of communication, pinpointing how and where political ideas were propagated. The network analysis helps identify the key actors who shaped these ideological shifts and tracks the spread of these ideas over time.  The software provides a robust framework for researchers to examine the interplay between language and social networks, offering a nuanced understanding of how political concepts were transmitted and transformed during the American Revolution. More generally, it allows for a systematic analysis of idea propagation in a digitized corpus of heterogenous letter data, revealing the central figures and networks driving change in thinking about a particular concept.",https://github.com/h-gear/revolution,h-gear/revolution
10cd1c6b-2867-49a9-8407-ecc3b893b7d9,xenon-cli,Xenon command line interface,* Provides an easy-to-use command line interface for distributed computing developers  * Makes the main methods of the Xenon library accessible on the command line * Low threshold introduction to the features of Xenon library * No programming required to use * Used in training material of Xenon library,https://github.com/NLeSC/xenon-cli,NLeSC/xenon-cli
10dd951c-bc77-415b-a63b-5c860b7e6a9d,4tu-data-underlying-the-publication-thesis-chapter-flexconv-continuous-kernel-convolutions-with-differentiable-kernel-sizes,Data underlying the publication/thesis chapter: FlexConv: Continuous Kernel Convolutions with Differentiable Kernel Sizes,"ICLR article and thesis chapter. This article contributes a novel convolutional operator that can learn to modulate its kernel size, and can be used to perform scale generalization.",https://data.4tu.nl/v3/datasets/965e9467-12b0-4998-9df3-0dba15b52db0.git,
10de5666-9b6e-44bd-aba1-3a1520065bcc,cesium-ncwms,Cesium-ncWMS,* A Web-based 3D globe visualization for NetCDF data * Allows analysis of points of interest by clicking spots on the globe * Works in collaboration with ncWMS to translate NetCDF data into images  Cesium (cesiumjs.org) based visualization using ncWMS to serve NetCDF data and D3 (d3js.org) to display graphs. A live running version of this software can be found here: http://forecast.ewatercycle.org,https://github.com/eWaterCycle/Cesium-NcWMS,eWaterCycle/Cesium-NcWMS
114599b1-1f57-4714-b297-2674ba5ca982,4tu-partile-filter-code-with-an-example-of-weight-collapse-in-importance-sampling-methods,Partile filter code  with an example of weight collapse in importance sampling methods,"We propose an implementation of the particle filter in a quasi-static case in the example of Gaussian prior with independent and identically distributed prior states and observation errors. Weight collapse occurs in the particle filter when the number of model states and observations increases for a given ensemble size. In this example, we use a synthetic experiment to illustrate how weight collapse varies in the posterior distribution.  This code provides a basis for the implementation of importance sampling methods and can be easily adapted to other problems.",https://data.4tu.nl/v3/datasets/45a30f5c-5252-416f-a45f-117343e794df.git,
117083f6-6a0d-4c73-a02a-98afbf0e1cf1,4tu-artifact-deductive-verification-of-parameterized-embedded-systems-modeled-in-systemc,[Artifact] Deductive Verification of Parameterized Embedded Systems modeled in SystemC,"This is the artifact for the paper Deductive Verification of Parameterized Embedded Systems modeled in SystemC. It contains software that enables the deductive verification of SystemC designs by transforming SystemC to PVL, as well as software to automatically generate global invariants that aid in deductive verification of global properties. Furthermore, the artifact contains files that support the paper's experiments. It contains SystemC case studies and pre-generated PVL translations that were used in the experiments. It also contains detailed instructions for general use of the included tools and for the replication of the experiments. For this, see the included README.pdf.  The artifact files are packaged in a VirtualBox virtual machine running Ubuntu 22.04.3 LTS, with all requirements pre-installed. Both username and password for the virtual machine image are ""artifact"" (without quotes). In addition to this, the relevant files of the artifact are also packaged in a separate .zip file.  All files contained in the artifact are licensed under the Mozilla Public License Version 2.0.",,
118e638a-6ca8-42cc-95d5-920271d32b62,4tu-source-code-phd-dissertation,source-code-phd-dissertation,"Collection of source code implementing methods and for reproducing experiments included in each chapter of the Ph.D. dissertation ""Safer Causal Inference: Theory &amp; Algorithms for Falsification, Trial Augmentation and Policy Evaluation"". The source code also includes methods for generating simulated datasets used in the evaluation of the methods. The goal of the of the research was to develop methods to improve treatment effect estimation, this includes: methods to detect unmeasured confounding from observational data, methods to integrate historical data into randomized experiments to improve data efficiency, methods to evaluate treatment policies under treatment interference.",https://data.4tu.nl/v3/datasets/f711a397-3fab-42bf-a8da-8ab499ffed61.git,
11a47f9e-6d84-4ad4-9e63-41c6695149a8,cvasl,cvasl,"Allows processing, application of harmonization and machine learning algorithms of derived data from neuroradiological datasets.",https://github.com/brainspinner/cvasl,brainspinner/cvasl
1251be45-94c1-4f28-9900-d6eec3c47e3b,4tu-port-data-graphs,port_data_graphs,"This repository is part of the Ph.D. thesis of&nbsp;Isabelle M. van Schilt, Delft University of Technology.     This repository is used to generate a graph of open-source sea and airport data. For this, open-source data of the shipping schedules given by MSC, Maersk, HMM, and Evergreen is used. The data is collected from the websites of the shipping companies (see also&nbsp;https://github.com/EwoutH/shipping-data). The data is then processed to generate a graph of the shipping schedules, including the distributions of the shipping schedules. The graph is used to analyze the shipping schedules and to identify the most important ports in the network. Airport data is collected from the open-source&nbsp;OpenFlights&nbsp;database.    As case study, we collect data on CN-HK to main ports in the USA, and mostly MSC data on South America to NL-BE.    This repository is used for developing various graphs on open-source data and automatically running it as a simulation model in the repository:&nbsp;complex_stylized_supply_chain_model_generator",https://data.4tu.nl/v3/datasets/7202d406-4c8a-4be2-ade8-e545d5f6ccce.git,
12cfaef0-a497-44f9-966f-05d0b57f545e,partitionedarraysjl,PartitionedArrays.jl,"# What  This package provides distributed (a.k.a. partitioned) vectors and sparse matrices like the ones needed in distributed finite differences, finite volumes, or finite element computations. Packages such `GridapDistributed` have shown weak and strong scaling up to tens of thousands of CPU cores in the distributed assembly of sparse linear systems when using `PartitionedArrays` as their distributed linear algebra back-end.  # Why  The main objective of this package is to avoid to interface directly with MPI or MPI-based libraries when prototyping and debugging distributed parallel codes. MPI-based applications are executed in batch mode with commands like `mpiexec -np 4 julia input.jl`, which break the Julia development workflow. In particular, one starts a fresh Julia session at each run, making difficult to reuse compiled code between runs. In addition, packages like `Revise` and `Debugger` are also difficult to use in combination with MPI computations on several processes.  To overcome these limitations, `PartitionedArrays` considers a data-oriented programming model that allows one to write distributed algorithms in a generic way, independent from the message passing back-end used to run them. MPI is one of the possible back-ends available in `PartitionedArrays`, used to deploy large computations on computational clusters. However, one can also use other back-ends that are able to run on standard serial Julia sessions, which allows one to use the standard Julia workflow to develop and debug complex codes in an effective way.",https://github.com/PartitionedArrays/PartitionedArrays.jl,PartitionedArrays/PartitionedArrays.jl
13108811-a425-4b7b-9941-b9baa1e3d7b8,rickview,RickView,"# Summary Search, browsing and exploration is a critical area in the linked data (LD) lifecycle. Similar to web browsers, where users enter the URL of a website and receive a display of an HTML document, in linked data browsers, users enter the URL of an RDF resource and receive a human readable representation. However in contrast to websites, RDF resources are intended to be processed by machines and do not provide a standardized way of being displayed.  An RDF graph, also called a knowledge base, is a set of RDF triples in the form of (subject, predicate, object) and typically describes a common domain under a shared namespace, which can be abbreviated with a prefix, such as `rdf`. For example, the RDF vocabulary has a ""hash namespace"" (ending with a `#` character) of `http://www.w3.org/1999/02/22-rdf-syntax-ns#`, which signifies that all resources, like `rdf:type` and `rdf:property`, of the vocabulary are described in the same document, which is suitable for small graphs such as ontologies that are not intended to be viewed by non-expert users.  RDF browsers on the other hand *resolve URIs* (URLs) of knowledge bases with ""slash namespaces"" (ending with a '/' character), so that each resource has it's associated page. Ideally, they offer a machine processable RDF serialization as well, based on either content negotation, POST parameters or URL variants such as `http://mynamespace/myresource.ttl`. LD browsers describe the direct neighbourhood of a resource in an RDF graph, that is they list all triples where the given resource is either the subject (a direct connection) or the object (an inverse connection).  We present RickView, an RDF browser with the following design goals:  * performance: low memory and CPU utilization, fast page load times and high number of handled requests * robustness: once it compiles, it runs indefinitely * standalone: no dependance on services such as a SPARQL endpoint or a web server * adaptability to any small to medium sized knowledge base via sensible defaults that can be changed in a configuration file and overridden with environment variables * containerization: offer a compact container image containing a statically linked binary * simplicity: RickView is minimalistic and only offers browsing of static data  Non goals: RickView is a read-only visualization for static data, does not offer other visualization types, such as graph based, and is not a search engine. It is not designed for data that does not fit on one machine (""big data"") or indeed data that does not fit on RAM. Large knowledge bases, such as the complete DBpedia, fit better with the traditional SPARQL endpoint paradigm.  # Statement of need  While initial enthusiasm in the Semantic Web field has led to a large amount of published knowledge bases, mainstream adoption has stagnated due to a lack of freely available performant, accessible, robust and adaptable tools.  Instead, limited duration research grants motivate the proliferation of countless research prototypes, which are not optimized for any of those criteria, are not maintained after the project ends and compete for resources on crowded servers if they do not break down completely. While there are are several existing RDF browsers, they are not optimized for performance.  # Implementation   The standard backend for LD projects are *SPARQL endpoints*, which allow expressive SQL-like *SPARQL queries*, however they are overengineered for the simple task of browsing. For example, the popular Virtuoso Open-Source Edition maps SPARQL to SQL queries on top of a relational database, which is faster than native triplestores like Apache Jena but requires tuning of parameters like memory buffer sizes for optimal resource allocation and the RDF data model can cause large amounts of joins, which negatively impacts query runtime.  RickView instead follows an alternative approach of directly querying an in-memory dataset bypassing SPARQL. RickView is an LD browser written in Rust, which enables a high level of performance comparable to C and C++ while being memory-safe and thread-safe by design.  In order to keep the focus on performance and to get a baseline of functionality for performance evaluation, the web page layout is in large parts copied over from LodView.",https://github.com/KonradHoeffner/rickview,KonradHoeffner/rickview
13177bc3-6c12-4978-9f29-f3979c52974e,hyphe,hyphe,"Hyphe is an open source web-crawler allowing researchers to build corpora made of hyperlinked webpages about a specific topic (for instance, palm oil or coronavirus).   These webpages are selected by researchers and can be grouped as Â« webentities Â», which can be single pages as well as a website, subdomains or parts of it, or even a combination of those. They represent different actors of the issue at hand (for instance, a person, an organization, etc.).  ![hyphe network](https://medialab.sciencespo.fr/static/network_f6475b93-7d69-4319-95da-df227b99b2c7.png)  By crawling them, Hyphe builds iteratively and helps visualize a network graph of the relationships between these actors through the hyperlinks connecting the webentities.  New webentities are automatically suggested after they were discovered by crawling each entities hyperlinks, and researchers can then review them in an iterative and qualitative process.  ![hyphe curation](https://medialab.sciencespo.fr/static/prospect_4ca1c375-6a43-4aa5-af73-b1494b3c056d.png)  As it allows researchers to manually choose and then tag which actors they want to add to their corpus, Hyphe should be considered as a quali-quantitative tool.  ![hyphe network](https://medialab.sciencespo.fr/static/network-tags_2e7fe049-2fb9-4b94-bef1-9821bba42f34.png)",https://github.com/medialab/hyphe,medialab/hyphe
134c587f-5ff8-4a86-939d-220be5c343fe,4tu-code-for-the-paper-synergizing-cycling-and-transit-strategic-placement-of-cycling-infrastructure-to-enhance-job-accessibility,"Code for the paper ""Synergizing cycling and transit: Strategic placement of cycling infrastructure to enhance job accessibility""","This code is used for computing the results of the paper ""Synergizing cycling and transit: Strategic placement of cycling infrastructure to enhance job accessibility"".&nbsp;     This code measures job accessibility improvements in Amsterdam when cycling can be combined with public transport and quantifies the individual contribution of every potential bicycle parking location and cycle lane in the total accessibility improvements.     This source code should be stored in a folder named code. The folder code and the folder data should be located in the same directory.     FORMAT*.mkd;  *.ipynb;  *.txt;  *.csv     RECOMMENDED HARDWARE1. Processor: IntelÂ® Coreâ„¢ i5-10210U CPU  2. RAM: 16GiB of RAM (DDR4)  3. GPU: IntelÂ® UHD Graphics GPU     RECOMMENDED OPERATING SYSTEMUbuntu 22.04.2, 64-bit     REQUIRED VERSION OF PYTHON3.9.7     REQUIRED LIBRARIES USEDsee requirements.txt     EXTRA FILEparameter.csv specifies some parameters used in the analysis.     SEQUENCE OF SCRIPTSThe scripts should be run in the following order:     1. perimeter_of_study.ipynb  2. transfer_preprocess.ipynb  3. waiting_time.ipynb  4. shortest_path.ipynb  5. compute_stops_reachable_PT.ipynb  6. compute_accessibility.ipynb  7. identifying_cr_infrastructure.ipynb",,
1350e420-c88e-41f1-b1d2-252b2b274ac9,4tu-model-underlying-the-publication-simulation-of-a-fully-coupled-3d-gia-ice-dynamical-model-for-the-antarctic-ice-sheet-over-a-glacial-cycle,Model underlying the publication: Simulation of a fully coupled 3D GIA - Ice Dynamical model for the Antarctic Ice Sheet over a glacial cycle,This folder contains all the files of the coupled ice dynamical - 3D GIA model.,,
135132b5-54ea-4264-a58e-c0d73e8dbdf7,allelic-variation-explorer,Allelic Variation Explorer,"* It provides a genome browser with a track where the samples which have the same SNPs are clustered together, it can be used by bioinformations to explain why certain samples have the same phenotype * It takes variants of a large number of samples and for the requested genomic range it performs clustering and visualizes the clusters * the clustering is done quick and on the fly  * the reference genome, gene and feature tracks are rendered from static files, so you don't need any application server just a static web server",https://github.com/nlesc-ave/ave-rest-service,nlesc-ave/ave-rest-service
1379d753-bde7-4b03-8819-410cbd633fe5,4tu-kg-tool-python-package-for-accessing-the-chemical-engineering-knowledge-graph-chemengkg,KG-tool: Python package for accessing the Chemical Engineering Knowledge Graph (ChemEngKG),"This software is a Python package for accessing the ChemEngKG). ChemEngKG is a comprehensive knowledge graph that captures the vast and diverse information landscape of chemical engineering.&nbsp;It integrates information from various sources including research publications, patents, textbooks, and industry standards to provide a structured and interconnected representation of the knowledge domain.&nbsp;Our goal is to facilitate knowledge discovery and dissemination in chemical engineering and to support various applications such as process design, optimization, and innovation.&nbsp;Whether you are a student, researcher, or practitioner in chemical engineering, we hope that ChemEngKG will serve as a valuable resource to enhance your knowledge and productivity.&nbsp;",,
14b474f8-ca99-4891-a51b-0de46aec99e6,scikit-talk,scikit-talk,"Scikit-talk is an open-source toolkit for processing collections of real-world conversational speech in Python. The toolkit aims to facilitate the exploration of large collections of transcriptions and annotations of conversational interaction, specifically tailored to applications in speech processing and Conversational AI.  With scikit-talk, you can import conversational transcripts from multiple common transcription formats, and analyse conversational properties. The package works well with its sister software talkr, an R package for plotting conversations and visualizing conversational dynamics.  Read more about scikit-talk in [this paper by Liesenfeld et al.](https://aclanthology.org/2021.sigdial-1.26/), published in 2021 in ACL Anthology.",https://github.com/elpaco-escience/scikit-talk/,elpaco-escience/scikit-talk
14ca69c9-3347-4458-8f27-fb3f17944f7e,4tu-history-vector-phase-detection,history-vector-phase-detection,"About 200 coronary angiograms were recorded from a distance in a cardiac catheterisation laboratory at the Reinier de Graaf Gasthuis, Delft, NL.  The purpose of the video recordings was to analyse workflow during procedures.  This Python repository aims to recognise workflow phases from human motion.  It analyses bodypart motion, human posture, and positioning with respect to other persons.  After encoding all these aspect into history vectors, it builds a mixture model for classification of new motions.  Additionally, it takes procedure duration into account.  Unfortunately, this approach proved unable to accurately classify workflow phases, so using this code for that purpose is not recommended.",https://data.4tu.nl/v3/datasets/9988b4b6-2716-4b58-bc7b-b82549c7e720.git,
1511fee7-f351-402a-abc2-830fa67468fd,4tu-spurious-sentience,spurious_sentience,"Code and research results for ICML 2024 position paper. Originally released here: https://github.com/pat-alt/spurious_sentience.     The research results include:     Regression tables (.tex; .html)An ""evaluations.csv"" file that contains estimated evaluation metrics for linear probes  and the baseline grouped by indicator, layer (network layer), train/test split, variable (measure), model (lin. probe/baseline).A figures/ folder containing all PNG figures that went into a) the body or b) the appendix.An interim/ folder containing results for probe predictions for each training epoch.An attacks/ folder containing the CSV files of neural network activations for attack prompts (see paper for details). Additionally, this folder contains a sentences/ subfolder with the actual textual attack prompts (.txt files).",https://data.4tu.nl/v3/datasets/21350eb5-7a23-4710-af30-de27967196d3.git,
153f4ae9-a031-4525-a257-908dbddca5ef,omuse,OMUSE,"* provides an easy-to-use environment for numerical experiments in the climate sciences * interfaces and couples simulation codes in an Python framework * allows researchers and students formulate intricate numerical experiments in simple python scripts * runs on local machines, supercomputers and on distributed computing resources",https://github.com/omuse-geoscience/omuse,omuse-geoscience/omuse
1540b396-33cc-4c2c-a557-9eb5b39fd4db,4tu-software-supporting-rna-seq-analysis-of-brassicaceae-species-under-different-irradiances,Software supporting: RNA-Seq analysis of Brassicaceae species under different irradiances,"This software package contains all scripts employed to analyze data from an RNA-Seq experiment conducted on Brassicaceae species grown under different irradiances. In this study, co-authors and I aimed to comprehend the genetic and physiological underpinnings of photosynthetic light-use efficiency (LUE) under high irradiance conditions, focusing on the plant species Hirschfeldia incana. We performed a comparison of the transcriptional signature associated to very high, ""supernatural"" irradiance in H. incana with three other Brassicaceae plants (Arabidopsis thaliana, Brassica rapa, and Brassica nigra), which previously demonstrated lower photosynthetic LUE. By utilizing a panproteome, we assessed gene expression patterns in response to high irradiance across the four species. Our findings reveal that all species actively regulate genes linked to photosynthesis. Analyzing genes associated with three key photosynthetic pathways, we observed a consistent pattern of reduced gene expression under high irradiance conditions. Notably, specific genes exhibited differential expression exclusively in H. incana, while in other instances, transcript abundance was consistently higher in H. incana regardless of light intensity. In conclusion, the study this software supports presents the first comparative transcriptome analysis of plant species grown entirely under prolonged high irradiance, rather than just briefly exposed to it. We demonstrate that, in contrast to other Brassicaceae species, H. incana subjected to intense irradiance displays enhanced gene expression related to photosynthesis through distinct mechanisms: canonical differential expression, inherent elevated expression of single-copy genes, and cumulative elevated expression via simultaneous expression of multiple gene copies. This research establishes a crucial groundwork for future endeavors aimed at comprehending elevated photosynthetic light-use efficiency and ultimately achieving highly effective photosynthesis in agricultural crops.",https://data.4tu.nl/v3/datasets/1688428e-78c1-4412-9b9f-c050f09dc2c7.git,
157044fd-f877-4578-880f-39e806a8c3bd,4tu-code-underlying-the-publication-pate-proximity-aware-time-series-anomaly-evaluation,"Code underlying the publication: ""PATE: Proximity-Aware Time series anomaly Evaluation""","This repository provides the implementation of Proximity-Aware Time Series Anomaly Evaluation (PATE), a novel metric introduced to address the limitations of existing evaluation methods for time series anomaly detection. PATE incorporates proximity-based weighting with buffer zones around anomaly intervals to account for temporal complexities such as Early or Delayed detections, Onset response time, and Coverage level. It computes a weighted version of the Area Under Precision and Recall curve, offering a more accurate and meaningful assessment of anomaly detection models. Experimental results validate PATE's ability to distinguish performance differences across various models and scenarios.",https://data.4tu.nl/v3/datasets/41905ac2-3984-46a0-8a51-9985e9698c76.git,
1616f050-848d-49d1-8955-76eed29fec85,porespy,PoreSpy,"PoreSpy relies heavily on scipy.ndimage and scikit-image also known as skimage. The former contains an assortment of general image analysis tools such as image morphology filters, while the latter offers more complex but still general functions such as watershed segmentation. PoreSpy does not duplicate any of these general functions so you will also have to install and learn how to use them to get the most from PoreSpy. The functions in PoreSpy are generally built up using several of the general functions offered by skimage and scipy. There are a few functions in PoreSpy that are implemented natively, but only when necessary.",https://github.com/PMEAL/porespy,PMEAL/porespy
162b9c85-a9bc-4950-9762-afeab98fcb65,4tu-boundary-layer-program-underlying-the-master-thesis-simulation-of-two-dimensional-steady-state-boundary-layers-applied-to-nonideal-gas-flows,Boundary-Layer-Program underlying the master thesis: Simulation of Two-Dimensional Steady State Boundary Layers Applied to Nonideal Gas Flows,"This MATLAB computer program solves the two-dimensional steady state boundary layer equations with general fluid properties for compressible flows in the ideal gas or non-ideal gas (departing from ideal gas) regime, adiabatic or including heat transfer for laminar and/or turbulent (algebraic CS-model) flows.Â        This computer program was part of the work of the master thesis Simulation of Two-Dimensional Steady State Boundary Layers Applied to Nonideal Gas Flows, (August 10, 2020) by D.D. Dijkshoorn Delft University of Technology, faculty of Mechanical, Maritime and Materials Engineering (3me), The Netherlands. Link: https://repository.tudelft.nl/islandora/object/uuid:433d4c00-e063-4614-9c78-2849870d8d7f",,
16350a37-3cc4-4a32-86e2-fa696afd1fb5,lofar-predictive-maintenance,Lofar Predictive Maintenance,,https://github.com/NLeSC/lofar-predictive-maintenance,NLeSC/lofar-predictive-maintenance
166b4e1d-2ffe-4874-b683-08ded9e50e87,4tu-coralmodel-a-python-based-model-that-resembles-the-biophysical-interactions-on-a-coral-reef,CoralModel: A Python-based model that resembles the biophysical interactions on a coral reef,"CoralModel simulates the biophysical feedback loop containing four environmental factors: (1) light, (2) temperature, (3) hydrodynamics, and (4) pH. It allows to determine the hydrodynamics using Delft3D Flexible Mesh (both D-Flow and D-Waves), and interacts with this hydrodynamic model.",,
16976876-8605-4d8d-819f-6778a74bd8a4,4tu-a-high-spatial-resolution-extended-spring-indices-database-over-american-and-european-regions,A high spatial resolution Extended Spring Indices database over American and European regions,"This repository contains a high spatial resolution database of spring onset indicators derived from the Extended Spring Indices (SI-x) phenological models as well as the code used to generate these phenological products. The SI-x models transform daily minimum and maximum temperatures into a set of consistent indices that track the timing of first leaf and first bloom for key indicator species, and that also allow the calculation of the so-called damage index by subtracting the date of first leaf from the date of the last freeze.    This dataset is available at 1 km spatial resolution and covers American (1980 to 2022) and European regions (1950 to 2020).  The first leaf and first bloom indices are validated with ground phenological observations collected over both regions.  The American product is based on DAYMET version 4 (https://daymet.ornl.gov/) and the European region on a downscaled E-Obs database version 3 (ftp://palantir.boku.ac.at/Public/ClimateData).    This product can support both scientists and decision makers in their quest to hind- and forecast climate change impacts in these areas  In the USA this product is recognized as an official climate change indicator (https://www.globalchange.gov/indicators/start-of-spring)    An interactive visualization of this database is available here: https://emma.users.earthengine.app/view/spring-onset    For more information see our publications:  A MatlabÂ© toolbox for calculating spring indices from daily meteorological data (https://doi.org/10.1016/j.cageo.2015.06.015 )Development and analysis of spring plant phenology products: 36 years of 1-km grids over the conterminous US (https://doi.org/10.1016/j.agrformet.2018.06.028)A long-term 1 km gridded database of continental-scale spring onset products (under review)",https://data.4tu.nl/v3/datasets/454afe4e-08aa-4333-b6e8-7db7cebd0f83.git,
16a14c49-bf98-4043-9fb6-9d0f2b0cd6da,arrayanalysis,ArrayAnalysis,"ArrayAnalysis is designed to make transcriptomic data analysis accessible to everyone. With its user-friendly interface, it empowers scientists, regardless of their expertise in high-throughput data analysis, to analyze and interpret transcriptomic experiments with ease. The platform supports the analysis of both RNA sequencing and microarray-based transcriptomic data. ArrayAnalysis can be run online or installed locally. So, visit [our website](https://arrayanalysis.org/) to start using ArrayAnalysis for your transcriptomic research today!",https://github.com/arrayanalysis/ArrayAnalysis_Shiny,arrayanalysis/ArrayAnalysis_Shiny
16d85ff3-c8bd-4fe1-882e-8a3fdbea3df5,4tu-stumblemeter-app-and-matlab-code,Stumblemeter: App and MATLAB code,"Use the Stumblemeter_installer.exe file to download the Stumblemeter app.  To change the MATLAB code of the Stumblemeter app, open Stumblemeter_MATLAB_code.mlapp. The files CWA_fileread.m, MLmodel1_stumble_other.mat, and MLmodel2_elevating_lowering.mat are required to run the code.",,
16ed72c0-af2e-49ee-ab58-09c16e0f75a8,4tu-bayesian-ensembles-for-exploration-in-deep-reinforcement-learning-code-underlying-the-dissertation-bayesian-model-free-deep-reinforcement-learning,"Bayesian Ensembles for Exploration in Deep Reinforcement Learning; Code underlying the dissertation ""Bayesian Model-Free Deep Reinforcement Learning""","Code underlying the dissertation ""Bayesian Model-Free Deep Reinforcement Learning"". This repository contains code for the chapter ""Bayesian Ensembles for Exploration in Deep Q-Learning"". The code is for experiments which demonstrate how ensemble-based Bayesian methods can improve exploration efficiency in reinforcement learning, with a focus on the DQN architecture. The code can be used to reproduce the results and be modified by researchers to be applied to other benchmark problems. The code has no direct applications outside of reinforcement learning research.",https://data.4tu.nl/v3/datasets/3a105521-4def-4d63-a350-5d3b4cadd35b.git,
17029879-1c53-4fa4-b227-309f5719a845,4tu-autoencodersdlsca,AutoEncodersDLSCA,"Link to GitHub repository with source code for the publication: Autoencoder-enabled model portability for reducing hyperparameter tuning efforts in side-channel analysis.  The source code uses the Python programming language. Scripts used to run the experiments are in the main directory, while the folder 'src' holds the implementations for hyperparameter tuning, loading of side-channel datasets, etc., providing some abstraction. Scripts starting with 'attack' were used to run experiments, while other scripts were helper scripts for analyzing/reading/plotting results.  Sbatch scripts were used to run experiments with TU Delft servers.  More information can be found in the publication.",https://data.4tu.nl/v3/datasets/a286c271-a5eb-41f9-9d3a-357ece7547a1.git,
17349d20-6fdf-464e-8107-4249a9add735,crisp,rcrisp,"<!-- README.md is generated from README.Rmd. Please edit that file -->  # CRiSp  <!-- badges: start -->  [![R-CMD-check](https://github.com/CityRiverSpaces/CRiSp/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/CityRiverSpaces/CRiSp/actions/workflows/R-CMD-check.yaml) <!-- badges: end -->  CRiSp (City River Spaces) provides tools to automate the morphological delineation of riverside urban areas.  ## Installation  You can install the development version of CRiSp from [GitHub](https://github.com/) with:  ``` r # install.packages(""devtools"") devtools::install_github(""CityRiverSpaces/CRiSp"") ```  ## Example  This is a basic example which shows you how to solve a common problem:  ``` r library(CRiSp)  # Set location parameters city_name <- ""Bucharest"" river_name <- ""DÃ¢mboviÈ›a"" epsg_code <- 32635  # Get base layer for plotting bucharest_bb <- get_osm_bb(city_name) bucharest_streets <- get_osm_streets(bucharest_bb, epsg_code)[, ""geometry""]  # Delineate river corridor bucharest_river <- delineate_corridor(""Bucharest"", ""DÃ¢mboviÈ›a"", crs = epsg_code) #> Warning: to_spatial_subdivision assumes attributes are constant over geometries #> Linking to GEOS 3.11.0, GDAL 3.5.3, PROJ 9.1.0; sf_use_s2() is TRUE  # Plot results plot(bucharest_river, border = ""orange"", lwd = 3) plot(bucharest_streets, add = TRUE) ```  <img src=""man/figures/README-example-1.png"" width=""100%"" />  ## Contributing  We look very much forward to contributions to the package. See the [Contributing Guide](.github/CONTRIBUTING.md) for further details.  This package is released with a [Contributor Code of Conduct](.github/CODE_OF_CONDUCT.md). By contributing to this project you agree to abide by its terms.",https://github.com/CityRiverSpaces/CRiSp,CityRiverSpaces/CRiSp
1741b6d2-bd7d-4869-859f-09abfd6e815c,4tu-pynomad-a-microscopic-pedestrian-simulation-model,PyNOMAD: A microscopic pedestrian simulation model,PyNOMAD is the python version of the microscopic pedestrian simulation model NOMAD. It enables a user to model pedestrian movements in great detail.,https://data.4tu.nl/v3/datasets/fe806f6e-76d1-4092-991b-5544d0053644.git,
17a37d13-4b71-4ce1-9896-91a8cde254a6,nebula,NEBULA,,https://github.com/esciencecenter-digital-skills/NEBULA,esciencecenter-digital-skills/NEBULA
17e75dd8-fc0d-4c53-af66-7d0297dce1e6,4tu-code-underlying-the-publication-simulating-event-based-pesticide-transport-with-runoff-and-erosion-openlisem-pesticide-v-1,"Code underlying the publication: Simulating event based pesticide transport with runoff and erosion, OpenLISEM-pesticide v.1","This repository contains the code and secondary data used to initialize and calibrate OpenLISEM-pesticide v1. The related article presents a novel pesticide transport model extension for the OpenLISEM runoff and erosion model. During water erosion events, high amounts of pesticides can be transported alongside the runoff. Simulating this process helps to mitigate adverse effects of pesticides in the environment. We conceptualized pesticide uptake during runoff with a mixing-layer, including mass transfer of dissolved pesticides into the runoff water and detachment in combination with enrichment of sorbed pesticides. Lateral transport of pesticides is modelled with a kinematic wave based on the overland flow of water. The model simulations were in line with observations for two events in a small agricultural catchment in South-Limburg, the Netherlands. A sensitivity analysis showed that pesticide transport was mainly influenced by the mass transfer rate, the soil-water partitioning and the exponent for the enrichment ratio. The accurate simulation of runoff and erosion by OpenLISEM, enables OLP to simulate transport and redistribution adequately.",https://data.4tu.nl/v3/datasets/844929e6-2c90-458f-980f-a7089a063486.git,
185475d7-d66c-4852-915c-def8bdade10f,smp-decision-tree,SMP decision tree,* The questionnaire guides you through the process of creating a SMP specifically tailored for your software.  * Provides user-friendly interface in the form of a web form.,https://github.com/SS-NES/docassemble-SMPDecisionTree,SS-NES/docassemble-SMPDecisionTree
1859cfbd-69c7-4ff2-85e4-98a06a356dfa,4tu-software-accompanying-manuscript-annual-satellite-based-ndvi-derived-land-cover-of-europe-for-2001-2019,"Software accompanying manuscript ""Annual Satellite-Based NDVI-Derived Land Cover of Europe for 2001-2019""","This software accompanies the journal publication titled ""Annual Satellite-Based NDVI-Derived Land Cover of Europe for 2001-2019"".",,
186caffc-18a5-4d80-936e-d46056c8a51f,astra-toolbox,ASTRA Toolbox,"The ASTRA Toolbox is a MATLAB and Python platform providing scalable, high-performance GPU primitives for 2D and 3D tomography, including building blocks for advanced reconstruction algorithms. One of its main design goals is geometric flexibility, allowing the toolbox to be used with many types of experimental setups, such as lab-based, synchrotron and clinical CT, electron tomography, PET and SPECT.  The ASTRA Toolbox has been developed jointly by the Computational Imaging group of CWI, and the University of Antwerp. It is used in tomographic imaging research by academics working on novel algorithms and in experimental imaging labs. It is also deployed by several industrial partners as the core of the reconstruction software in their imaging equipment and applications. Furthermore, the toolbox is a main component in a number of other applications, including software used at two of the largest synchrotrons in the world: the European Synchrotron Radiation Facility (ESRF) in Grenoble, and the Advanced Photon Source (APS) in Chicago.",https://github.com/astra-toolbox/astra-toolbox,astra-toolbox/astra-toolbox
186e8315-ab35-4b27-af9f-40fab6ab0223,4tu-data-underlying-the-msc-thesis-an-evaluation-of-the-merging-interaction-between-humans-and-interaction-aware-vehicles,Data underlying the MSc. thesis: An Evaluation of the Merging Interaction between Humans and Interaction-Aware Vehicles,"This is the dataset of the Master thesis titled ""An Evaluation of the Merging Interaction between Humans and Interaction-Aware Vehicles"". The dataset encompasses the results of 10 experiments conducted as part of the thesis, with each experiment involving 2 participants, resulting in a total of 20 participants. All participants provided informed consent before participating in the experiments. The research protocol was reviewed and approved by the ethical committee of TU Delft.     The experiments were conducted using a virtual reality driving simulator, allowing for the collection of comprehensive data on the interaction between human drivers and interaction-aware vehicles. The dataset includes raw data captured during the experiments, as well as analyzed results derived from the collected data. The data include the raw data from the experiments conducted in the virtual reality driving simulator, the eye-tracking data capturing participants' gaze and attention during the merging interactions and the analyzed results obtained from the collected data.",https://data.4tu.nl/v3/datasets/c2ccf43c-875e-4f81-af76-2fbdeae1d14c.git,
1873908e-d4e9-4f96-8723-d1b790e2e4f5,4tu-software-supporting-the-publication-background-electromagnetic-noise-received-via-ionospheric-propagation-in-a-remote-rural-location-in-spain-set-1,Software supporting the publication: Background Electromagnetic Noise Received via Ionospheric Propagation in a Remote Rural Location in Spain (set 1),"This software goes with the data set of a scientific experiment involving polarisation measurements in a remote rural location in the lower Pyrenees in Catalonia, Spain, of electromagnetic waves refracted in the ionosphere. DOI 10.4121/21888366.   The software is used for reprocessing this dataset (using adaptive filters) to obtain the ambient electromagnetic noise arriving via ionospheric propagation in a remote rural location.",,
189664d6-f395-46da-8ad8-8a252eecc02e,imas2xarray,imas2xarray,Imas2xarray is a library that makes it as simple and intuitive as possible to load an IMAS dataset in HDF5 format into Python. There is no need to manually define the paths or fiddle with the different dimensions and long keys.,https://github.com/duqtools/imas2xarray,duqtools/imas2xarray
18f35a0c-9f7e-4bd0-8b6d-bb3c0890a973,4tu-perm-map-circuits,perm_map_circuits,"This is an experimental implementation of a method to map elements accross multiple leveled homomorphic ciphertexts. Currently, only HElib ciphertexts are supported.     Installing  Install HElib following their instructionsRun `git submodule update --init --recursive` for the graph coloring library   Building  Building is simple using `cmake .`.     Executing  We implemented three different experiments:  Benchmarking within-ciphertext permutationsBenchmarking across-ciphertext permutationsBenchmarking across-ciphertext mappingsTo choose one, pass the corresponding number to the program as an argument.",https://data.4tu.nl/v3/datasets/897ad4a5-2654-4953-86e5-14b5dcb1b767.git,
1909612c-b8cd-4b1a-b728-15f12cb60c3d,neurogym,NeuroGym,,https://github.com/neurogym/neurogym/,neurogym/neurogym
1911eea9-67b2-4424-9c8c-916801c6018e,ccglib,ccglib,The Complex Common GEMM library (ccglib) provides a C++ interface to run complex-valued matrix-matrix multiplication on GPU tensor/matrix cores. The library supports both CUDA and HIP.,https://github.com/nlesc-recruit/ccglib,nlesc-recruit/ccglib
191d847c-2557-448f-9e92-a8a0a68e9294,4tu-joan-a-framework-for-human-automated-vehicle-interaction-experiments-in-a-virtual-reality-driving-simulator,JOAN: a framework for human-automated vehicle interaction experiments in a virtual reality driving simulator,"JOAN is an software package that allows to perform human-in-the loop experiments in the open source driving simulator CARLA. JOAN facilitates communication between human input devices and CARLA, the implementation of haptic feedback, systematically storing experiment data, and the automatic execution of experiments with multiple experimental conditions.",,
19594b3a-28ab-44fe-896c-d1dffd48c7b2,puregome,PuReGoMe,"* Provides frequency analyses of topics in social media messages * Generates automatic overviews of sentiment towards topics over time * Produces stance information on topics, given some gold standard data  PuReGoMe is a research project of Utrecht University and the Netherlands eScience Center. We analyze Dutch social media messages to assess the opinion of the public towards the COVID-19 pandemic mesasures taken by the Dutch government.  The software uses three data sources for social media analysis. Our main data source are Dutch tweets from Twitter. We use Dutch posts from Reddit and comments from Nu.nl as sources of verfication of the results obtained by the tweet analysis.  PuReGoMe performs analysis on three different levels: by counting messages, by determining their polarity (sentiment) and by determining their stance with respect to anti-pandemic government measures.",https://github.com/puregome/notebooks,puregome/notebooks
1970812c-2ae5-4ca4-83fc-2ddd55d78eb2,4tu-international-terrestrial-reference-frame-itrf-matlab-toolbox,International Terrestrial Reference Frame (ITRF) Matlab Toolbox ,"Matlab toolbox for coordinate and velocity transformations between International and European Terrestrial Reference Frame (ITRF/ETRF) realizations, plate motion models (PMM) and analysis of velocity and coordinate changes between reference frames.     The most recent version is available from github (https://github.com/hvandermarel/itrf-matlab-toolbox).",https://data.4tu.nl/v3/datasets/ec642f32-d8c9-406c-ade6-1de56b2a8a09.git,
1a07a6a5-f93f-4949-8536-5c5c6521404e,cmlxom,CMLXOM,,https://github.com/BlueObelisk/cmlxom,BlueObelisk/cmlxom
1a5fa847-0167-4fc5-baab-6bf809562f46,4tu-gedsys,GEDSys,Components for the Geographic Event Detection System. This system implements the RASCA architecture for event-driven application. Event-driven applications detect the occurrence of relevant geographic events on data streams produced by sensor networks.,https://data.4tu.nl/v3/datasets/a97b4bbb-93ad-4828-968f-80202ee160b8.git,
1a691928-7542-4084-816c-8759f0d299a7,gh-action-set-up-singularity,GitHub action to set up singularity,"* For developer writing a software development workflow in GitHub Actions and need to run or build Singularity images * Can be used to test code against Singularity containers * Tell workflow what you want not how you want it * No need to learn how to install singularity yourself, just copy/paste configuration",https://github.com/eWaterCycle/setup-singularity,eWaterCycle/setup-singularity
1b12d41b-b97c-4b13-8c73-0b116794197a,knime-molviewer,KNIME molecule viewer nodes,"* Provides cheminformatics trying to model proteins within KNIME a way to view them * Adds  nodes to KNIME to visualize proteins, small molecules and pharmacophores * Adds support to KNIME to  handle viewing big molecules * Used to compare pharmacophore generation tools * Used to check ligand repurposing",https://github.com/3D-e-Chem/knime-molviewer,3D-e-Chem/knime-molviewer
1b2c748a-99bd-4724-9280-34365ca89192,class-web,CLASS-web,"With CLASS-web, you can run and analyse experiments with the CLASS model right in your browser  CLASS is a conceptual model of the atmospheric boundary layer. It simulates the evolution of key variables and allows you to quickly generate plots.",https://github.com/classmodel/class-web,classmodel/class-web
1b5bf3ac-b979-4576-9d87-bcbd015a612f,s2spy,s2spy,"Producing reliable sub-seasonal to seasonal (S2S) forecasts with machine learning techniques remains a challenge. Currently, these data-driven S2S forecasts generally suffer from a lack of trust because of:  - Intransparent data processing and poorly reproducible scientific outcomes - Technical pitfalls related to machine learning-based predictability (e.g. overfitting) - Black-box methods without sufficient explanation  To tackle these challenges, we build `s2spy` which is an open-source, high-level python package. It provides an interface between artificial intelligence and expert knowledge, to boost predictability and physical understanding of S2S processes.  It can facilitate your data-driven forecasting workflow with:  - Datetime operations & data processing - Preprocessing - Dimensionality reduction - Cross-validation - Model training - Explainable AI analysis",https://github.com/AI4S2S/s2spy,AI4S2S/s2spy
1b7f1c24-e4bb-437d-8fec-d90888562f5a,xenon,Xenon,"* Provides an easy-to-use interface for distributed computing developers  * Enables the use of different file transfer protocols and scheduling systems on remote machines  * No need to learn and implement many different APIs * Successfully used in many eScience tools and projects  Many applications use remote storage and compute resources. To do so, they need to include code to interact with the scheduling systems and file transfer protocols used on those remote machines.  Unfortunately, many different scheduler systems and file transfer protocols exist, often with completely different programming interfaces. This makes it difficult for applications to switch to a different system or support multiple remote systems simultaneously.  Xenon solves this problem by providing a single programming interface to many different types of remote resources. As a result, changing from one scheduler to another, or from one file transfer protocol to another, becomes a matter of changing just a few lines of code. This is obviously much cheaper in time and money than developing, debugging, and maintaining new code that implements the same logic you had before, but for a different scheduler or a different file transfer protocol.",https://github.com/xenon-middleware/xenon,xenon-middleware/xenon
1b865c42-0505-43f6-9a33-d5b6eca4abe6,covid-19-integrated-surveillance-data-in-italy,COVID-19 Integrated Surveillance Data in Italy,"## Overview   Every week the National Institute for Nuclear Physics ([INFN](https://home.infn.it/it/)) imports an anonymous individual-level dataset from the Italian National Institute of Health ([ISS](https://www.iss.it)) and converts it into an incidence time series data organized by date of event and disaggregated by sex, age and administrative level with a consolidation period of approximately two weeks. The information available to the [INFN](https://home.infn.it/it/) is summarised in the following [meta-table](https://covid19.infn.it/iss/campi-iss.pdf).  ## Data   ### Archive  The original [data](https://covid19.infn.it/iss/) has been stored [here](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/0_archive), reorganised [here](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/1_structured_archive).   ### Input   The input data has been stored [here](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/2_input) and contain the following information: - Aggregated data in the [`daily_incidences_by_region`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/2_input/daily_incidences_by_region) folder:   * Weekly moving average and daily time series of **confirmed cases by date of diagnosis** at the regional level;   * Weekly moving average and daily time series of **ordinary hospital admissions by date of admission** at the regional level;   * Weekly moving average and daily time series of **intensive hospital admissions by date of admission** at regional level;   * Weekly moving average and daily time series of **deceased cases by date of death** at the regional level. - Disaggregated data in the [`daily_incidences_by_region_sex_age`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/2_input/daily_incidences_by_region_sex_age) folder:   - Weekly moving average time series of **symptomatic cases by date of symptoms onset** stratified by sex and age at the regional level;   - Weekly moving average time series of **confirmed cases by date of diagnosis** stratified by sex and age at the regional level;   - Weekly moving average time series of **ordinary hospital admissions by date of admission** stratified by sex and age at the regional level;   - Weekly moving average time series of **intensive hospital admissions by date of admission** stratified by sex and age at the regional level;   - Weekly moving average time series of **deceased cases by date of death** stratified by sex and age at the regional level.     ### Output   The output data has been stored [here](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/3_output/data) and contain the following information:  * Reconstructed daily time series of **confirmed cases by date of diagnosis** stratified by sex and age at the regional level; * Reconstructed daily time series of **symptomatic cases by date of symptoms onset** stratified by sex and age at the regional level; * Reconstructed daily time series of **ordinary hospital admissions** by date of admission stratified by sex and age at the regional level; * Reconstructed daily time series of **intensive hospital admissions** by date of admission stratified by sex and age at the regional level; * Reconstructed daily time series of **deceased cases by date of death** stratified by sex and age at the regional level.  ## Methodology   ### Data Organization  Raw data are downloaded from the [INFN](https://covid19.infn.it/iss/) (direct download [here](https://covid19.infn.it/iss/csv_part/iss.tar.gz)), decompressed, stored in the [`0_archive`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/0_archive) folder and then organized into the  [`1_structured_archive`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/1_structured_archive) folder via the execution of the [data_organization.jl](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/blob/main/src/data_organization.jl) script.   ### Data Processing   In general, given the moving average (or rolling mean) of a time series, it's not possible to recover the original series unless *n* original points are known where *n* is the width of the window adopted in the moving average, but since epidemiological surveillance incidence series are strictly composed of natural numbers, we can leverage this property to come up with a finite number of candidate original series, and then prune these down to as little as possible, hopefully only one, final recovered series.  The whole procedure is performed via the execution of the [main.jl](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/blob/main/src/main.jl) script and the related technical details can be found the documentation of [UnrollingAverages.jl](https://github.com/InPhyT/UnrollingAverages.jl) package.   The averaged time series to be **unrolled** (i.e. recovered, reconstructed or de-averaged) are those stored in the [`2_input/daily_incidences_by_region_sex_age`](https://github.com/COVID19-Italy-Integrated-Surveillance-Data/tree/main/2_input/daily_incidences_by_region_sex_age) folder: they are organized in .csv files, each of which reporting the 10 age-specific time series of a particular incidence in a particular region. Each dataset has two counterparts that are further stratified by sex.  Since the smaller the numbers involved the better UnrollingAverages.jl seems to perform, we opted for unrolling the sex-stratified series first and then aggregate them later. Since not all the age and sex stratified averaged series allows UnrollingAverages.jl to find an unique original series and no further sex-stratified information is provided by INFN, we attempted to directly unroll the sex-aggregated time series for which CovidStat provides additional information in the form of age-aggregated original time series, that we employed to select that combination of age-disaggregated series proposed by UnrollingAverages.jl which summed to the age-aggregated original time series provided by INFN. The utilized age and sex-aggregated may be found in the [`2_input/daily_incidences_by_region`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/2_input/daily_incidences_by_region) folder. We'll refer to the last selection algorithm as the **cross-sectional consistency constraint**.   The successfully reconstructed time series are then saved in the [`3_output/data`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/3_output/data) folder (both aggregated and disaggregated by sex), while the visualisations of those that are age-stratified and sex-aggregated may be found in [`3_output/figures`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/tree/main/3_output/figures).  ## How to Cite   If you use these data in your work, please cite this repository using the metadata in [`CITATION.bib`](https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data/blob/main/CITATION.bib).",https://github.com/InPhyT/COVID19-Italy-Integrated-Surveillance-Data,InPhyT/COVID19-Italy-Integrated-Surveillance-Data
1bdc5c3c-27d5-47fa-847b-0161044dfdca,qtl-tableminer,QTLTableMiner++,* Provides command-line access to Quantitative Trait Loci commonly presented in tables  * Makes these data increasingly FAIR in machine-readable formats * Aids in further data re-use and (semantic) integration with other types of biological data,https://github.com/candYgene/QTM,candYgene/QTM
1bf1448d-3388-4ab0-b543-8db44c30e607,4tu-urdf-environments-for-openai-gym,Urdf-Environments for OpenAI-Gym,URDF environments for gym The simulation environment facilitates simulation of robotic systems with pybullet as its physics engine. Robots are specified by the URDF-description and are simulated using pybullet. The interface is the widely used OpenAI-Gym so that reinforcement learning methods can be used. This simulation environment should facilitate motion planning research and prototyping of algorithms.,,
1c241cea-91c1-496d-a37a-b600c8e6c3b9,compas-toolkit,Compas Toolkit,"The Compas Toolkit is a high-performance C++ library offering GPU-accelerated functions for use in quantitative MRI research. The toolkit offers fast simulations of various MRI sequences and k-space trajectories commonly used in qMRI studies. While the core of the toolkit is implemented using CUDA, the functionality is accessible from both C++ and Julia.  ## Features *    Flexible API that can be composed in different ways. *    Highly tuned GPU kernels that provide high performance. *    Implemented using CUDA, optimized for Nvidia GPUs. *    Usable from Julia and C++.",https://github.com/NLeSC-COMPAS/compas-toolkit,NLeSC-COMPAS/compas-toolkit
1c5fc8b3-e843-4aae-bab8-f4399940dabf,genap,GenAP,"**The Genetics and Genomics Analysis Platform (GenAP)** is a computing infrastructure and software environment for life science researchers available since 2015. GenAP aims at facilitating the work of researchers and students by offering out of the box Web applications running on an infrastructure currently leveraging Compute Canada Cloud and HPC resources.   # Brief History  The GenAP project was initially funded by grants from CANARIE and GÃ©nome QuÃ©bec from 2013 to 2018 and included three major components: * The development of robust bioinformatics pipelines (called the MUGQIC pipelines); * The adaptation of the Cern Virtual Machine File System (CVMFS) to facilitate code and reference files distribution on Compute Canada infrastructures; * The development of a web portal to facilitate data analysis and sharing through tools with graphical interfaces integrated on Compute Canada infrastructure (including Galaxy, Datahub and UCSC Genome Browser mirror).  The pipelines were used for thousands of compute jobs during this period, using CVMFS for the distribution of their code and reference files throughout all Compute Canada infrastructures. The portal was accessed by >200 active users, >150 Datahubs were created (>25 TB of data shared) and >10k jobs were run through Galaxy.   # Service **VM with Galaxy (life sciences platform) image**  **Squid cache VM Service**",https://bitbucket.org/mugqic/genpipes/src/master,
1c759ca6-8d82-479e-b5fd-c11422a51048,openlb,OpenLB,"# OpenLB - Open Source Lattice Boltzmann Code  The OpenLB project is a C++ package for the implementation of lattice Boltzmann methods adressing a vast range of transport problems.  ## Dependencies  The only mandatory external dependency of OpenLB is GNU Make and a C++ compiler with C++20 support. This includes all reasonably recent versions of GCC, Clang and Intel ICX.  GPU support depends on Nvidia CUDA 12 or later.  ## Installation  1. Adjust the `config.mk` file to fit your local compiler environment    (examples for some common configurations are available in `config/`) 2. Build the embedded dependencies using `make` 3. Switch to any of the examples and compile it using `make`  ## Documentation  A comprehensive user guide is available in `doc/userGuide`.  Papers featuring OpenLB are collected at [1].  Up-to-date Doxygen documentation can be generated via `make doxygen` or accessed at [2].  [1]: https://www.openlb.net/articles/ [2]: https://www.openlb.net/DoxyGen/html/index.html  ## Community  The OpenLB forum [3] is an open discussion board for all aspects of LBM and OpenLB. Feel free to post any problems, questions, suggestions or contributions.  You can also reach us via mail at info@openlb.net  There is a yearly one-week Spring School [4] where you can learn about LBM and OpenLB directly from the developer team and invited guest lecturers.  A list of all present and past contributors is available at [5].  [3]: https://www.openlb.net/forum/ [4]: https://www.openlb.net/spring-school-2022/ [5]: https://www.openlb.net/authors/  ## How to cite OpenLB  Standardized citation metadata is available in the `CITATION.cff` file.  To cite OpenLB in general instead of a specific release we suggest:  ``` @article{OpenLB2021,   title = {{OpenLB - Open Source Lattice Boltzmann Code}},   year = {2021},   issn = {08981221},   doi = {10.1016/j.camwa.2020.04.033},   journal = {Computers \& Mathematics with Applications},   author = {Krause, Mathias J. and Kummerl{\""a}nder, Adrian and Avis, Samuel J. and Kusumaatmaja, Halim and Dapelo, Davide and Klemens, Fabian and Gaedtke, Maximilian and Hafen, Nicolas and Mink, Albert and Trunk, Robin and Marquardt, Jan E. and Maier, Marie-Luise and Haussmann, Marc and Simonis, Stephan}, } ```  This article is available as open access [6].  [6]: https://doi.org/10.1016/j.camwa.2020.04.033  ## Code Formatting  The basic formatting rules are described by the `.editorconfig` file [7] for automatic application in a wide variety of text editors and IDEs.  [7]: https://editorconfig.org/  ## License  OpenLB is provided as open source under the terms of the GNU GPL v2 license.  See `LICENSE` for details.",https://gitlab.com/openlb/release,
1cafc848-937f-412a-9c5f-61a7329e3fbf,4tu-czmtestkit,CzmTestKit,The python package czmtestkit works parallel to Abaqus/CAE to test user element subroutines of cohesive zone models subjected to mode dependant load cases.,,
1cdaaf80-4798-45b1-be3b-0572210c3566,4tu-a-model-for-modifying-the-public-transport-service-patterns-to-account-for-the-imposed-covid-19-capacity,A model for modifying the public transport service patterns to account for the imposed COVID-19 capacity,"Description:This repository contains the software code related to deriving dynamic service patterns in order to comply with the pandemic-imposed vehicle capacity limits in public transport operations.  Currently, this repository contains:  The model_case_study.py script which is the source code of the devised dynamic service pattern model introduced in the paper ""A model for modifying the public transport service patterns to account for the imposed COVID-19 capacity"", which is currently under scientific review. This script contains all necessary functions to calculate the solution of the mathematical program for the scenario described in the case study of the scientific paper.  The model_demonstration.py script that contains the implementation of the demonstration scenario in the aforementioned scientific paper.  Referencing:In case you use this code for scientific purposes, you can cite the paper ""A model for modifying the public transport service patterns to account for the imposed COVID-19 capacity"" once it is publicly available.    License:MIT License   Dependencies:Note that the script model_case_study.py is written in Python. Running or modifying this script requires an installed version of Python 3.6. In addition, the mathematical model is solved with the use of Gurobi 9.0.3. You would need a Gurobi license to obtain an optimal solution.   Research Project:This software code is developed in the research programme 'COVID 19 Wetenschap voor de Praktijk', project number:10430042010018. The project is funded by the Dutch Research Organization for Health Research and Development (ZonMw)",,
1cddbcb2-7db9-4916-8512-bf560a9b7bfc,4tu-dataset-of-human-gait-in-a-domestic-setup-collected-with-an-lfmcw-mimo-radar,Dataset of human gait in a domestic setup collected with an LFMCW MIMO radar,"Radar technology is a strong candidate to analyze human gait. This dataset contains gait captures of 60 healthy individuals in a laboratory emulating a domestic scenario. The data is captured with the 24 GHz LFMCW MIMO radar Inras RadarBook 2 and with the Microsoft Azure Kinect camera. All the measurements are recorded with the approval of the ethics committee of TU Delft, under the project DARE-NERD. For further information regarding the radar and the Kinect configuration, and the experiment conditions, please refer to an article currently submitted for review.  The dataset contains a "".zip"" file for each participant with:  3 "".mat"" files, corresponding to each trial performed by the subject. Each file contains the radar data in the variable ""tima_data"". The dimensions are (fast time) x (slow time) x (channel index). Moreover, the radar configuration is contained in the variable ""Ana"".3 "".csv"" files, corresponding to each trial performed by the subject. Each file contains the 32 joint locations extracted with the Microsoft Azure Kinect camera for each time frame.A "".csv"" file with the time shifts that need to be applied to the Kinect data to synchronize it with the radar data.Moreover, the dataset provides   An "".xlsx"" file containg the identifier, gender, age, height, weight and shoe size of each participant.A matlab script to extract the radar data from the "".mat"" file.A matlab script to extract the joint data from the "".csv"" file.   The authors would like to thank the participants in the experiment for their availability.",,
1d426b73-0a70-49d1-8b86-d13e5d288dbb,openpnm,OpenPNM,OpenPNM is an open source project to provide porous media researchers with a ready-made framework for performing a wide range of pore network simulations.,https://github.com/OpenMS/OpenMS,OpenMS/OpenMS
1d7ba860-bf99-46fb-8a4d-cd5a1937f043,4tu-emi-ert-freshwater,EMI-ERT_Freshwater,"This repository holds Jupyter Notebooks to perform an integrated inversion of frequency domain electromagnetic induction measurements and electrical resistive tomography. The purpose of the study was to characterize the subsoil's electrical properties. Field data was acquired in De Cocksdorp, The Netherlands",https://data.4tu.nl/v3/datasets/929e4b16-9c14-41c9-8a9a-ada6f124dd88.git,
1dfa05e9-fe7d-462f-ac27-29c33466231a,4tu-code-used-to-support-generating-the-pulse-water-discharge-patterns,Code used to support generating the pulse water discharge patterns,"Code used to support generating the pulse water discharge patterns Model considering fish safety assessment is developed to support generating the pulse water discharge patterns, which can balance the contradiction between hydropower development and environmental projection.",,
1ea23893-26d1-4d84-8cf8-de4e349b1ff1,inseq,Inseq,,https://github.com/inseq-team/inseq,inseq-team/inseq
1ecdfc89-8a30-4018-8c63-e484d3a2c10f,4tu-greenspaceaccessibility-github-code-repository-spatial-analysis-underlying-the-journal-paper-measuring-children-s-and-adolescents-accessibility-to-greenspaces-from-different-locations-and-commuting-settings,"GreenspaceAccessibility (Github) code repository: spatial analysis underlying the journal paper ""Measuring children's and adolescents' accessibility to greenspaces from different locations and commuting settings""","Notebooks 1-9 in this repository allow you to calculate the accessibility of urban greenspaces by children and adolescents. It calculates accessibility not only to home locations, but also to educational facilities (e.g., schools, colleges, and university buildings), and during the commute in between them. This code is developed for three cities in The Netherlands (i.e., Amsterdam, Rotterdam, and The Hague) and can be adapted to fit other geographical contexts.  Additional examples: Notebooks to generate pedestrian walksheds around greenspaces; to compare effects of different greenspace accessibility measures; and to generate commuting heatmaps for cohort data.",,
1f0d8edf-f4d8-401c-918b-62fd3e280407,4tu-queueing-times-of-automated-vehicles-in-a-bottleneck-matlab-code,Queueing Times of Automated Vehicles in a Bottleneck: MATLAB Code,"This folder contains MATLAB code used to generate congestion graphs in the paper 'Departure Time Choice and Bottleneck Congestion with Automated Vehicles: Role of On-board Activities', as well as the resulting graphs. The graphs illustrate the development of queueing times in a single bottleneck scenario, where travellers use either conventional or automated vehicles. Automated vehicles are differentiated depending on whether they support home-, work-activities, or both home- and work-activities during travel.Code was created in MATLAB R2018b.",,
1f5baaef-1872-4f0b-8b0c-3b590b7d47f5,adhtools,adhtools,"* Create a BlackLab index of your corpus * Works on text files in OpenITI format  * Use SAFAR's stemmers and morphological analyzers  Adhtools can be used to process corpora of OpenITI text files, together with the corpus metadata. There are workflows for creating BlackLab indices and notebooks with various analyses. The workflows are built on top of nlppln. The tools use SAFAR for morphological analyses of Arabic. Unfortunately, SAFAR is not properly licensed so we don't distribute it with our code, and it needs to be downloaded separately in order to run the workflows.",https://github.com/arabic-digital-humanities/adhtools,arabic-digital-humanities/adhtools
1f749929-ff9f-4abc-890b-81541ab70c0a,resurfemg-dashboard, ReSurfEMG-dashboard,ReSurfEMG-dashboard is a dashboard developed using [Dash](https://dash.plotly.com/) for visualizing and analyzing respiratory EMG signal. The analysis is performed using of the [ReSurfEMG library](https://github.com/ReSurfEMG/ReSurfEMG).,https://github.com/ReSurfEMG/ReSurfEMG-dashboard,ReSurfEMG/ReSurfEMG-dashboard
206ed167-441b-49c9-93ff-30ea8e6a09a5,taxotagger-webapp,TaxoTagger Webapp,"![TaxoTagger-Webapp](https://github.com/MycoAI/taxotagger-webapp/blob/1faf3d6a13b04e51d86555f4abb66180661bce4f/images/TaxoTagger-webapp.gif?raw=true)  ## Installation  1. Clone this repository ```bash git clone https://github.com/MycoAI/taxotagger-webapp.git ```  2. Install the required packages ```bash # Go to the taxotagger-webapp directory cd taxotagger-webapp  # Create a new conda environment `taxotagger-webapp` conda create -n taxotagger-webapp python=3.10  # Go to the conda environment conda activate taxotagger-webapp  # Install the required packages pip install -r requirements.txt ```  ## Running the webapp  1. Set the environment variables `MYCOAI_HOME`  Set the environment variable `MYCOAI_HOME` to the path of the `data` directory in this repository. This directory contains the example vector databases for demo purposes.  ```bash # On Linux or MacOS export MYCOAI_HOME=/path/to/taxotagger-webapp/data  # Or on Windows set MYCOAI_HOME=C:\path\to\taxotagger-webapp\data ```  2. Start the webapp ```bash # Make sure you are in the taxotagger-webapp directory and the conda environment is activated cd taxotagger-webapp conda activate taxotagger-webapp  # Run the webapp streamlit run app.py ```  Then you can open the webapp in your browser by visiting the URL http://localhost:8501.  > NOTE! > For the first time running, the webapp will download the embedding model files. This may take a few minutes depending on the internet connection speed.  ## For production deployment  The vector databases provided in the `data` directory are for demo purposes only. To use the webapp in production, you should prepare the vector databases using the production data. To build the vector database, you can follow the instructions in the [TaxoTagger Doc](https://mycoai.github.io/taxotagger/latest/quickstart/#build-a-vector-database).",https://github.com/MycoAI/taxotagger-webapp,MycoAI/taxotagger-webapp
20b7db64-8ce2-4354-9509-302324c54dd8,4tu-lth-readiness-dutchhomes,LTH_readiness_Dutchhomes,This repository contains the code developed and data generated during January 2023 â€“ May 2024 at Delft University of Technology. This contents of the repository was used to support the research paper titled â€œPreparing for Lower-Temperature Heating: Evaluating Building-Level Parameters for Lower-Temperature Heating Readiness: A Sampling-based Approach to Addressing the Heterogeneity of Dutch Housing Stockâ€,https://data.4tu.nl/v3/datasets/22118ab3-febd-49c3-a1a9-12d430a777dc.git,
20cfaf99-466e-475a-b365-7dbe4c83b169,admtools,admtools,,https://github.com/MindTheGap-ERC/admtools,MindTheGap-ERC/admtools
210364b1-2c04-49f5-8ebb-306a98927a21,opensim-creator,OpenSim Creator,,https://github.com/ComputationalBiomechanicsLab/opensim-creator,ComputationalBiomechanicsLab/opensim-creator
2110d4b4-e5a1-4a0d-8a43-45b0110e1adb,4tu-matlab-scripts-and-musescore-scripts-accompanying-the-book-the-perceptual-structure-of-sound,"Matlab scripts and MuseScore scripts accompanying the book ""The Perceptual Structure of Sound""","Matlab scripts, Matlab functions, source files, and MuseScore files used for generating the sound files (.wav)&nbsp;and figures (.jpg) of the book ""The Perceptual Structure of Sound""",,
2156c668-c641-44f4-a3a1-c36f54c12eb1,cudawrappers,cudawrappers,"This library is a C++ wrapper for the Nvidia C libraries (e.g. CUDA driver, nvrtc, cuFFT etc.). The main purposes are:  1. _easier resource management_, leading to _lower risk of programming errors_; 2. _better fault handling_ (through exceptions); 3. _more compact user code_.  This library also supports AMD GPUs through the HIP: C++ Heterogeneous-Compute Interface for Portability.  Originally, the API enforced RAII to even further reduce the risk of faulty code, but enforcing RAII and compatibility with (unmanaged) objects obtained outside this API are mutually exclusive.  ## Requirements  | Software    | Minimum version | | ----------- | ----------- | | CUDA        | 10.0 or later | | ROCM        | 6.1.0 or later | | CMake       | 3.17 or later | | gcc         | 9.3 or later  | | OS          | Linux distro (amd64) |  | Hardware    | Type | | ----------- | ----------- | | NVIDIA GPU  | [Pascal](https://www.nvidia.com/en-in/geforce/products/10series/architecture/) or newer| | AMD GPU     | RDNA2 or newer, CDNA2 or newer |    ## Usage  CMake is used to build cudawrappers. To build the library, run:  ```shell git clone https://github.com/nlesc-recruit/cudawrappers cd cudawrappers cmake -S . -B build make -C build ```  This creates a `build` directory. Since cudawrappers is header-only, no library objects will be built. For more details on the building requirements and testing, see the [developer documentation](README.dev.md).  To install cudawrappers to a specific location (e.g., ~/.local), use: ```shell cmake -DCMAKE_INSTALL_PREFIX=$HOME/.local -S . -B build make -C build make -C build install ```  You can optionally select what cudawrappers components should be enabled by specifying the `CUDAWRAPPERS_COMPONENTS` option. To build all components, specify: `-DCUDAWRAPPERS_COMPONENTS=all`. By default, only the `cu` and `nvrtc` components are enabled. The other components include: `cufft`, `nvml` (CUDA only) and `nvtx`.  ### Enabling HIP Support  To enable HIP:  1. Add the following code to your project's `CMakeLists.txt` file:      ```cmake     enable_language(HIP)     set(CUDAWRAPPERS_BACKEND ""HIP"")     ```      Alternatively, use the `-DCUDAWRAPPERS_BACKEND=HIP` argument when running CMake.  2. Ensure that every target your project builds is 'hipfied'.     This is done by marking relevant source files as 'HIP' compatible:      ```cmake     set_source_files_properties(source.cpp PROPERTIES LANGUAGE HIP)     ```  3. Optionally, use `#ifdef (__HIP__)` directives in your source code to enable/disable certain sections for HIP.  4. **Build**: ensure that the `hipcc` compiler is selected.     This can be done via the command line:      ```shell     CXX=hipcc cmake -B build     ```  **Note**: When building for both NVIDIA and AMD HIP, using **seperate** build folders (e.g, `build_nvidia` and `build_amd`) is encouraged. Additionally, please note that contrary to CUDA, the HIP backend does not implement GPU contexts. For library interoperability, it provides a (non-functioning) mock implementation. Still, the usage of `cu::Context` is strongly discouraged, and has been marked deprecated since cudawrappers version `0.9.0`. Support will likely be dropped in a future release.",https://github.com/nlesc-recruit/cudawrappers,nlesc-recruit/cudawrappers
215e604a-7a5b-4b8a-be91-d81afc2e2cc2,cosmologyjl,Cosmology.jl,,https://github.com/JuliaAstro/Cosmology.jl,JuliaAstro/Cosmology.jl
216efaa7-59a0-40b9-8f58-664642d7549c,4tu-data-underlying-the-master-thesis-modelling-and-assessment-of-an-autonomous-ride-sharing-service-s-urban-utilization,Data underlying the master thesis: Modelling and Assessment of an Autonomous Ride-Sharing Serviceâ€™s Urban Utilization,"This collection represents the empirical portion of the authorâ€™s MSc thesis research, which involved modelling the private vehicle traffic flow for the city of Rotterdam and the simulation of an autonomous ride-sharing service's impact on CO2 emission using that model on SUMO. It has four subfolders: net_files, python_code_files, sim_runs, and output_excel_files.       The folder contains Rotterdam's processed road network configuration, zonal data, and edge specifications along with the entirety of the written python code files for the construction of the traffic model and the trip merging component of the ride-sharing service.       Three separate scenarios were simulated for this project and compared to the baseline simulation for determining the emission reduction rates, and all ten iterations of these simulations are also included in this zip file.       Finally, here are several excel sheets detailing the results and calculations for zonal attraction ranking.Â    The code contains extensive comments that can be easily used to replicate or build on the published work. The readers may refer to the related thesis document for further explanations.",,
21d13da1-afac-40a5-9ba6-96323156cd03,3d-e-chem-knime-silicos-it,KNIME Silicos-it nodes,"* For cheminformaticians that want to use the Silicos-it software in the KNIME workflow platform * A thin wrapper around the Silicos-it software, the node can be configured using dialog instead of command line arguments * The Silicos-it shape-it is used to align molecules based on shape and align-it is used to align molecules based on their pharmacophore  * The Silicos-it binaries come bundled with the KNIME node installation so you don't need to download and compile the Silicos-it software yourself",https://github.com/3D-e-Chem/knime-silicos-it,3D-e-Chem/knime-silicos-it
21dc50a7-3028-435f-9a7e-c6814dc0b4dd,4tu-mlatjs,MLATjs,"This software is designed to visualise large areas (using Bing map), and then allow the user to create virtual ground stations. Then, based on the created (location, precision of measurements), it then (location, precision of measurements), it creates a visualisation showing how precise the measurements in each subarea (represented by a pixel) in the best case&nbsp;(i.e. no external interference).",https://data.4tu.nl/v3/datasets/e5d89b2f-19e1-4104-8585-6601d3a400ce.git,
21f262d2-be66-49a2-b6f0-8fb6c922f2f5,4tu-factor-model,Factor_Model,"The factor model problem focuses on the decomposition of a covariance matrix $\Sigma$ to low-rank and diagnonal positive semidefinite matrices.  In practice, $\Sigma$ is often not available, and only its empirical counterpart, $\hat{\Sigma}$, is available. To robustify to this approximation error, a common practice is to consider a family of covariance matrices in the vicinity of $\Sigma$.  The linked repository contains MATLAB files corresponding to the numerical results of the paper 'A Saddle Point Algorithm for Robust Data-Driven Factor Model Problems'.",https://data.4tu.nl/v3/datasets/5b51c2b9-4aba-4fdb-a3c8-1ec2c7bf2a93.git,
2220f7b9-f9e4-46a8-88c5-35d15894c2cc,4tu-custom-code-created-for-the-purposes-of-the-thesis-applications-of-statistical-theory-to-sensor-data-analysis,"Custom code created for the purposes of the thesis: ""Applications of statistical theory to sensor data analysis""",This is the custom code repository for replicating the results of the thesis. Three main routines are contained within this repository.  A new quality measure is proposed in the thesis for the purposes of assessing the quality of predictors in human activity recognition problems. The related code can be found in the file: measures.py  A postprocessing scheme is proposed in the thesis to remove unrealistically short activities from the classification given by the predictor. The related code can be found in the file: postprocessing.py  A new formulation of the null hypothesis in a permutation test for no effect is proposed in the thesis. The viability of the test is presented based on the simulation study. This simulation study can be found in the files: sim_study_lin_reg.ipynb and sim_study_nn.ipynb.,,
222a223a-fd1b-4392-9a89-fc9d678b04c0,4tu-retrieving-multi-temporal-max-min-temperature-of-target-area-into-a-csv-file,Retrieving multi-temporal max/min temperature of target area into a csv file,Retrieving multi-temporal max/min temperature data for specific locations from gridded nc files. The data are written in the CSV format.,,
238caeda-7bfd-4474-8222-07259567c289,infra-stemmus-scope,infra-stemmus-scope,,https://github.com/EcoExtreML/infra,EcoExtreML/infra
238e46b5-b661-4e8f-9738-3fdc25a19ee3,2022-angourakis-001,Reference evapotranspiration (FAO-56),"Calculate a daily value of reference evapotranspiration, useful for vegetation (incl. crop) models. The module code is based on FAO-56 Penman-Monteith method. The implementation is based on several sources (see moduleReferences), but particularly useful was the Evapotranspiration R package (<a href='https://cran.r-project.org/web/packages/Evapotranspiration/index.html' target='_blank'>Guo et al. 2016, 2022 (v1.16)</a>). Note: values of `C_n` and `C_d` are fixed for using the grass cover reference (900 and 0.34); values for the alfalfa reference are 1600 and 0.38.  ![Interface screenshot](netlogo_implementation/documentation/referenceEvapotranspiration%20interface.png)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |par_elevation|float|elevation above sea level of the soil where the reference evapotranspiration is to be calculated.| |par_temperature_daily_mean|float|Mean daily air temperature above the ground surface where the reference evapotranspiration is to be calculated.| |par_temperature_daily_min, par_temperature_daily_max|float|Minimum and maximum daily air temperature above the ground surface where the reference evapotranspiration is to be calculated.| |par_netSolarRadiation|float|Solar radiation energy received by the soil surface, discounting loss by albedo reflection.| |windSpeed|float|Wind speed at 2m above the soil surface (fixed to 2 as constant in code, as per Allen et al. 1998 recommendation in absence of data).| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |ETr|float|The evapotranspiration in millimeters of water per day assuming a reference surface that is not short of water and covered by a hypothetical grass (or alfalfa) reference crop with specific characteristics. It is referenced in the bibliography also as ETo.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Angourakis-001,
23b6ecf5-9816-4fb0-b121-657ab7a2f41b,4tu-agda-core,agda-core,"This repository contains source code for the paper. We develop a correct-by-construction typechecker for a subset of Agda's internal language. This code assumes that the input has already been scope-checked and is presented to the core in a well-scoped form. The typechecker uses erasure extensively and doesn't develop any meta-theory, relying on the typing rules as the source of truth. The implementation was developed by the authors of the paper through 2023 and 2024.",https://data.4tu.nl/v3/datasets/d178585d-169e-4b12-abf6-fd92dd3c7c8a.git,
243cd0b6-5e78-4411-8369-268a1183c3bf,4tu-data-and-code-underlying-the-paper-can-we-predict-the-most-replayed-data-of-video-streaming-platforms,"Data and code underlying the paper: ""Can we predict the Most Replayed data of video streaming platforms?""","Predicting which specific parts of a video users will replay is important for several applications, including targeted advertisement placement on video platforms and assisting video creators. In this work, we explore whether it is possible to predict the Most Replayed (MR) data from YouTube videos. To this end, we curate a large video benchmark, the YTMR500 dataset, which comprises 500 YouTube videos with MR data annotations. We evaluate Deep Learning (DL) models of varying complexity on our dataset and perform an extensive ablation study. In addition, we conduct a user study to estimate the human performance on MR data prediction. Our results show that, although by a narrow margin, all the evaluated DL models outperform random predictions. Additionally, they exceed human-level accuracy. This suggests that predicting the MR data is a difficult task that can be enhanced through the assistance of DL. In this repository, we provide our code and dataset. The code includes our trained and tested models, our user studies and results analysis. The YTMR500 dataset is provided through an H5 file.",https://data.4tu.nl/v3/datasets/49e15d64-83de-474b-bc86-e29ca7551898.git,
2448f407-7c09-472d-8c46-f38bc5e63e2e,4tu-mpcrl-cbf,mpcrl-cbf,Source code for the implementation and simulation of Model Predictive Control-based RL algorithms leveraging probabilistic Control Barrier Function formulations to enforce safety of state trajectories with arbitrary probability,https://data.4tu.nl/v3/datasets/1dd601c1-2977-43f4-9a92-f97ee37c559b.git,
24bf0bf9-266d-4cb1-bee0-0aa0cbb544ff,restape,RESTful APE,"| Badges | | |:----:|----| | **Fairness** |  [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/8082/badge)](https://www.bestpractices.dev/projects/8082) | | **Packages and Releases** |  [![Latest release](https://img.shields.io/github/release/sanctuuary/RESTAPE.svg)](https://github.com/sanctuuary/APE/releases/latest) [![Static Badge](https://img.shields.io/badge/RSD-RESTfulAPE-ape)](https://research-software-directory.org/software/restape) | | **Build Status** | ![build](https://github.com/sanctuuary/RestAPE/actions/workflows/maven.yml/badge.svg) [![CodeQL](https://github.com/sanctuuary/restape/actions/workflows/codeql.yml/badge.svg)](https://github.com/sanctuuary/restape/actions/workflows/codeql.yml) | | **Documentation** | [![Documentation Status](https://readthedocs.org/projects/ape-framework/badge/?version=latest)](https://ape-framework.readthedocs.io/en/latest/docs/restful-ape/introduction.html) | | **DOI** | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10048236.svg)](https://doi.org/10.5281/zenodo.10048236) | | **License** |  [![GitHub license](https://img.shields.io/github/license/sanctuuary/RESTAPE)](https://github.com/sanctuuary/RESTAPE/blob/master/LICENSE) |   A RESTful API for the APE library (RESTful APE) provides a way for users to interact with APE's automated pipeline exploration capabilities through HTTP requests. APE is a command line tool and Java API that automates the exploration of possible computational pipelines from large collections of computational tools.   The RESTful API allows users to submit requests to the APE server for pipeline exploration, which returns results in a standard format such as JSON or XML. Users can interact with APE through a web browser or any other HTTP client, and the API can be integrated into other applications for seamless pipeline exploration.   Overall, the RESTful API for APE provides a powerful and flexible way for users to leverage APE's capabilities in their scientific workflows.",https://github.com/sanctuuary/restape,sanctuuary/restape
25027c67-d25e-495c-a86c-f297d11b6bea,4tu-softshell-calculator-app-version-1,SoftShell Calculator App Version 1,"The purpose of the software is to analyze and visualize microdrill measurement data in .rgp format, taken from wooden piles, and to identify the ""soft shell"" (areas with varying degree of decay). It provides a classification of timber condition into four quality zones, where zones 1 and 2 represent the soft shell. The classifications are visualized along with a plot of the measurement data. Final results can be exported as a .csv or .xlsx file containing the calculated data. The plots can be exported as a .pdf file.",,
2511b8a3-f858-4989-ac19-2883abf0e38b,4tu-decade,decade,"DECADE is a tool for disctributed scatterer (DS) InSAR processing. It uses contextual information such as land registry data (for multilooking) and meteorological data (for functional models).      It is intended for use with Sentinel-1 IW mode acquisitions, processed by the DORIS software, but can be adapted for use with other sensors/ interferometric software by modifying the read functions. DECADE processes both point (PS) and distributed scatterers (DS), but the time series processing is focused on DS. PS processing is done for the purpose of removing atmospheric and other non-displacement phases.      DECADE is developed with the intention of monitoring soft soil motion in the Netherlands. However, the framework can be modified to accept other sources of contextual information to work with other locations or appliucation domains.",https://data.4tu.nl/v3/datasets/c0578cb8-9982-4e75-b4ae-642aacf62489.git,
252dfc07-3f7c-43e9-945c-80cf4d495ee1,4tu-simulink-model-for-lifetime-analysis-of-igbt-power-modules-in-passively-cooled-tidal-turbine-converters,SIMULINK model for Lifetime Analysis of IGBT Power Modules in Passively Cooled Tidal Turbine Converters,"The SIMULINK model is meant to estimate the junction temperature thermal cycling in a power electronic converter coupled with a tidal turbine converter. The temperature cycling information is useful for estimating the useful lifetime of the IGBT power modules in the converter. For lifetime value, you need to have a lifetime model separately, which is not provided here. The details about the modeling (incl. lifetime model) can be found in the linked reference. For confidentiality purposes, data has not been provided, and as such the model cannot be 'run' right away. However, after supplying the missing data the user can make use of the models.",,
2538157c-80e4-48c2-bbed-874ef208adf1,ivresse-jupyterlab-haddock3-configurator,i-VRESSE jupyterlab haddock3 configurator,You want to run [HADDOCK3 molecular modelling software](https://www.bonvinlab.org/haddock3) and you need to create a configuration file. This Jupyter Lab extension helps you write the configuration file.,https://github.com/i-VRESSE/jupyterlab-haddock3-configurator,i-VRESSE/jupyterlab-haddock3-configurator
255a6cef-cee6-4e30-803e-29ad771c6cd6,4tu-psi-dust-size-distribution,PSI_dust_size_distribution,"This repository contains the scripts required to generate all the figures for the publication: ""Polydisperse Formation of Planetesimals: The dust size distribution in clumps"" by Matthijsse, Aly &amp; Paardekooper (2025). This research investigates the Polydisperse Streaming Instability using the publicly available hydrodynamical code FARGO3D. This repository contains the scripts to process and visualize the hydrodynamical codes FARGO3D output files (.dat) of the simulation into time series in the form of (.csv) files. Calculations were done using the publicly available code PSITOOLS.",https://data.4tu.nl/v3/datasets/c90ed1d4-3cbd-492c-9b6b-db722ba00bcd.git,
25a64cbc-ef11-4346-b13b-56f66380fddd,4tu-the-model-and-the-data-part-of-the-master-thesis-integrating-reuse-in-matrace-models-an-implementation-and-evaluation,The model and the data part of the master thesis â€œIntegrating Reuse in MaTrace Models: An implementation and evaluationâ€,The presented model and the data are part of the master thesis â€œIntegrating Reuse in MaTrace Models: An implementation and evaluationâ€ by Raphael Elbing handed in in September 2022 to obtain the Master of Science in Industrial Ecology. Part of the code is explained in the file â€œmodel_application_manual.htmlâ€. The composition of the data is explained in the thesis itself. The ziped folders need to be unziped to run the code.,,
25ac7630-b959-4446-a484-84ebab43232b,kernel-launcher,Kernel Launcher,"![logo](https://kerneltuner.github.io/kernel_launcher/_images/logo.png)  Kernel Launcher is a **C++ library** that makes it easy to dynamically compile **CUDA** kernels at run time (using NVRTC) and call them in an easy **type-safe** way using C++ magic. Additionally, Kernel Launcher supports exporting kernel specifications from your application, to enable tuning by [**Kernel Tuner**], and importing the tuning results, known as wisdom files, back into your application. The result: blazing fast GPU code that is maintainable and performance portable, all with minimal effort.  [**Kernel Tuner**]: https://research-software-directory.org/software/kernel-tuner",https://github.com/KernelTuner/kernel_launcher,KernelTuner/kernel_launcher
25f8b240-593b-45ad-ae0d-5b48861f5b5a,eecology-annotation,eEcology Annotation Tool,* Ecologists studying bird behavior using the UvA-BiTS system can visualize and annotate the data gathered for a bird in a certain time range * The tool retrieves GPS measurements from the UvA-BiTS database and visualizes them so they can be annotated by hand or annotations classified by a machine can be verified * It shows all the GPS tracker data in context with the satellite imagery and your own data and annotations * It shows acceleration data (seconds timescale) and GPS information (days timescale) in the same interface,https://github.com/NLeSC/eEcology-Annotation-WS,NLeSC/eEcology-Annotation-WS
260d80e0-e984-4489-beaf-5c9af09bc0c9,4tu-argon-laser-plasma-thruster-design-and-test-of-a-laboratory-model-processing-and-analysis-code,Argon Laser-Plasma Thruster - Design and Test of a Laboratory Model - Processing and Analysis Code,"This is an archive of the processing and analysis code (Python) and the LaTeX manuscript source written for the following thesis:     Duplay, Emmanuel. â€œArgon Laser-Plasma Thruster - Design and Test of a Laboratory Model.â€ M.Sc. Thesis, Delft University of Technology, 2024. http://resolver.tudelft.nl/uuid:3a853f2e-2d8c-496b-a016-9e9855e8069c     This version was published at the time of thesis completion. Some changes may have been made since then. The latest version of this code is available on Github: https://github.com/eeduplay/MScThesis",https://data.4tu.nl/v3/datasets/9d23051c-d0a7-4b9b-a3b1-f859a0bda754.git,
26eefbce-0ecb-4c9c-9c84-f47baea9f135,4cat,4CAT Capture & Analysis Toolkit,,https://github.com/digitalmethodsinitiative/4cat,digitalmethodsinitiative/4cat
26f41e35-6aaf-4a2a-932f-086fcc8121b7,4tu-value-preferences-estimation-and-disambiguation-in-hybrid-participatory-systems-code,Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems - code,"Code for the paper ""Value Preferences Estimation and Disambiguation in Hybrid Participatory Systems"", under review at JAIR. This code contains five value preferences estimation methods based on participants' choices and textual justifications for a survey. Further, it implements an Active Learning pipeline with three sampling strategies---random, uncertainty, and driven by the estimation of value preferences. The pre-trained Dutch mode RobBERT is used to perform value classification.",https://data.4tu.nl/v3/datasets/99c10aa9-36b4-47a1-81a3-dde5a702cdd6.git,
2700f7f1-7fc2-4976-86eb-37238f558adf,convolutionalneuraloperatorjl,ConvolutionalNeuralOperators.jl,,https://github.com/DEEPDIP-project/ConvolutionalNeuralOperator.jl,DEEPDIP-project/ConvolutionalNeuralOperator.jl
271db795-36be-4835-99e8-7c7282b88a9f,4tu-mscthesis,MScThesis,"The aim of this MSc thesis is to propose proper definitions of heatwave sources and sinks as well as exploring their influence on Europe using complex network variables and AI/ML respectively. Such complex network variables show relations amongst geographical locations representing the movement of heat from a heatwave. The thesis was split up into two parts, where part one related to investigating the data to come up with proposed definitions for sources and sinks, and part two would take these definitions and propose machine learning methods to generalize the definitions to the available climatological period of 1990 to 2020. In the end, it was found that the input degree (ID), and output degree (OD) can best define sources and sink, as they describe the movement of heat amongst the network/ geographical locations. This resulted in sources and sinks being defined daily, where sources are defined by the maximum OD and minimum ID on a given day, whereas sinks would be defined by the minimum OD and maximum ID on a given day. Following these definitions, multiple nodes can be classified as a source or sink at the same time resulting in cloud-like structures. The definitions were further investigated using climatological expertise and unsupervised learning (i.e. one-class SVM) to explore if a clear distinction can be made between the two. Next, the proposed definitions were used to identify optimal models for generalizing to the available climatological period of 1990 to 2020. After extensive training and testing of six machine learning models (i.e. decision trees, random forest, adaptive boosting, gradient boosting, support vector machine, and gaussian naÃ¯ve bayes), gaussian naÃ¯ve bayes (GNB), gradient boosting (GB), and random forest (RF) were able to outperform the remaining three models recording higher f1-scores.&nbsp;",,
27228ebb-f6ab-499a-9619-514bbd7fac82,pidimehs-library,PIDIMEHS library,"* Showcase of interactive queries and visualization of textual data using ElasticSearch in Python for Digital Humanities * Query and summarize language usage from large textual databases, apply advanced relevance filtering, visualize trends over time  Showcases a diachronic analysis of the Dutch Royal Library's newspaper archive, answering questions about Dutch 20th century cultural and political segregation. Combines Pandas, Jupyter Notebooks and other PyData stack tools with ElasticSearch.",,
27a34819-ab27-4063-a20b-c85f0aca003e,4tu-scripts-for-the-master-thesis-life-cycle-fatigue-damage-estimation-for-military-off-road-vehicles,Scripts for the Master Thesis Life Cycle Fatigue Damage Estimation for Military Off-Road Vehicles,Function of the m-files and their dependancies is described in the full thesis at the TU Delft repository.,,
27c68851-8510-4627-8b3f-b81abdba1cb1,4tu-sample-dataset-and-software-for-fast-em-array-tomography,Sample dataset and software for FAST-EM array tomography ,"This repository contains a sample electron microscopy dataset produced with ""FAST-EM array tomography"" [1] and accompanying software to produce a 3D reconstruction of this dataset. FAST-EM array tomography is a technique for acquiring and reconstructing volume (3D) multibeam electron microscopy datasets. The sample dataset is part of a larger volume (3D) electron microscopy dataset included in the publication Kievits et al. (2024). This sample dataset is used for (automatic) testing of the software presented in the publication and as a demonstration example of 3D reconstruction.     Datasets  MCF-7 cells  3 serial sections of cultured Michigan Cancer Foundation-7 (MCF-7) cells (partial dataset). The dataset contains 25 images per section in a 5 x 5 grid, covering an area of 120 x 120 microns in each section of the sample. The `megafield_field_meta_data.yaml` contains metadata from the acquisition and is used in the processing.     Full dataset visualization  Partial dataset link: https://webknossos.tnw.tudelft.nl/links/BIduioDOrpiBh_EDFull dataset link: https://webknossos.tnw.tudelft.nl/links/9TwKDPdjNa4JHIYt   Organization     â”Œ 20231107_MCF7_UAC_test (project)  â”œâ”€â”€â”¬ S001 (section --&gt; z)  â”‚&nbsp;â”œâ”€â”€ 000_000_0.tiff  â”‚&nbsp;â”œâ”€â”€ 000_001_0.tiff  â”‚ â”œâ”€â”€ 000_002_0.tiff  â”‚ â”‚&nbsp;&nbsp;...  â”‚ â”œâ”€â”€ 004_004_0.tiff  â”‚ â”œâ”€â”€ mega_field_meta_data.yaml  â”œâ”€â”€â”€ S002  â””â”€â”€â”€ S003     Software   scripted-render-pipeline : https://zenodo.org/doi/10.5281/zenodo.12733385  Automated pipeline for processing FAST-EM array tomography datasets  Automated post-correction of imagesImport to render-wsExport to (self-managed) WebKnossos instances   interactive-render-workflow: https://zenodo.org/doi/10.5281/zenodo.12733814  Interactive workflow for aligning FAST-EM array tomography datasets  Interactive post-correction of imagesImport to render-wsInteractive 2D stitchingInteractive 3D alignmentExport to local WebKnossos instances   fastem-sofima : https://zenodo.org/doi/10.5281/zenodo.12733904  Scalable Optical Flow-based Image Montaging and Alignment (SOFIMA [2]) of FAST-EM array tomography datasets     References  [1] Kievits, Arent J., Duinkerken, B. H. Peter, Lane, Ryan, de Heus, Cecilia, van Beijeren Bergen en Henegouwen, Daan, HÃ¶ppener, Tibbe, Wolters, Anouk H. G., Liv, Nalan, Giepmans, Ben N. G. and Hoogenboom, Jacob P.. ""FAST-EM array tomography: a workflow for multibeam volume electron microscopy"" Methods in Microscopy, 2024. https://doi.org/10.1515/mim-2024-0005  [2] https://github.com/google-research/sofima     License   Data is published under a CC0 license. The software repositories are published under separate GPL-3.0 licenses.",,
27f16ca2-6269-412a-8d68-5c3588e98947,noodles,Noodles,"- Enables scientists to execute and restart parallel workflows by readable and easily maintainable Python code - Helps to scale computations in Python with complex dependencies to a parallel environment - No need to leave the comfort of Python: Noodles is a thin layer, unobtrusively handling complexity of a concurrent environment.   Often, a computer program can be sped up by executing parts of its code _in parallel_ (simultaneously), as opposed to _synchronously_ (one part after another).  A simple example may be where you assign two variables, as follows  ``a = 2 * i``  and  ``b = 3 * i``.  Either statement is only dependent on ``i``, but whether you assign ``a`` before ``b`` or vice versa, does not matter for how your program works. Whenever this is the case, there is potential to speed up a program, because the assignment of ``a`` and ``b`` could be done in parallel, using multiple cores on your computer's CPU. Obviously, for simple assignments like  ``a = 2 * i``,  there is not much time to be gained, but what if ``a`` is the result of a time-consuming function, e.g.   ``a = very_difficult_function(i)``?  And what if your program makes many calls to that function, e.g.   ``list_of_a = [very_difficult_function(i) for i in list_of_i]``?  The potential speed-up could be tremendous.  So, parallel execution of computer programs is great for improving performance, but how do you tell the computer which parts should be executed in parallel, and which parts should be executed synchronously? How do you identify the order in which to execute each part, since the optimal order may be different from the order in which the parts appear in your program. These questions quickly become nearly impossible to answer as your program grows and changes during development. Because of this, many developers accept the slow execution of their program only because it saves them from the headaches associated with keeping track of which parts of their program depend on which other parts.  Enter Noodles.  Noodles is a Python package that can automatically construct a _callgraph_ for a given Python program, listing exactly which parts depend on which parts. Moreover, Noodles can subsequently use the callgraph to execute code in parallel on your local machine using multiple cores. If you so choose, you can even configure Noodles such that it will execute the code remotely, for example on a big compute node in a cluster computer.",https://github.com/NLeSC/noodles,NLeSC/noodles
284d1910-0a6e-4a5c-97ab-74a2d674cef6,4tu-what-does-a-text-classifier-learn-about-morality-an-explainable-method-for-cross-domain-comparison-of-moral-rhetoric-code,What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric - code,"Code for the paper ""What does a Text Classifier Learn about Morality? An Explainable Method for Cross-Domain Comparison of Moral Rhetoric"", published at ACL '23. This code implements Tomea, an Explainable AI method for investigating the difference in how language models represent morality across domains. Given a pair of datasets and models trained on the datasets, Tomea generates 10 m-distances and one d-distance to measure the difference between the datasets, based on the SHAP method. We make pairwise comparisons of seven models trained on the MFTC datasets (available at this DOI: 10.4121/646b20e3-e24f-452d-938a-bcb6ce30913c).",https://data.4tu.nl/v3/datasets/14693061-797f-4105-b4b3-812c7cbcd759.git,
29c13ae2-1e02-42ea-95ff-5d601f619085,democracy-text-classification,Democracy text classification,,https://github.com/backdem/democracy-text-classification,backdem/democracy-text-classification
2a009b07-83dc-41b3-b0eb-a0246965f312,massivepotreeconverter,Massive PotreeConverter,"* For people wanting to visualize country sized point cloud data-sets using the Potree viewer, a way to convert the huge data-set for viewing in reasonable time. * This converter provides a way to convert the point cloud data to potree readable format in parallel instead of sequentially * The converter offered by potree will take a very long time to convert a huge data set as it will process each point in sequence, this software breaks the data-set up into pieces which can be converted in parallel, shortening the conversion time significantly * The converter was used for the visualization (http://ahn2.pointclouds.nl/) of the 640 billion point data-set of the Netherlands",https://github.com/NLeSC/Massive-PotreeConverter,NLeSC/Massive-PotreeConverter
2a53b113-6c4c-4d1c-85f7-7a41fa1cb195,canadian-advanced-network-for-astronomical-research-canfar,Canadian Advanced Network for Astronomical Research (CANFAR),"# Purpose  The Canadian Advanced Network for Astronomy Research is a consortium that serves data-intensive storage, access, and processing needs of university groups and centers engaged in astronomy research.  To this end, CANFAR develops and operates user-facing and integrated services, such as:  * Research Data Management * User-managed storage and cloud processing * Specialized visualization and analytics services * Support to researchers in adapting the system to their needs  # Mission  * To maintain and develop services in data and computationally intensive research that enable Canadian researchers and their international collaborators to generate the greatest possible scientific return on Canadaâ€™s investment in telescopes and computational resources.  * To coordinate efforts and resources in multi-use applications and services across Canadian groups, by generating integrated grant proposals and funding requests that enable the combination and integration of the science application domain (predominantly located at the universities) and the technical expertise (e.g. at CADC). Such grant proposals and funding request would be directed to any of the Canadian agencies involved in Science, Innovation, Computing and Big Data, such as NSERC, CFI, Canarie or NRC.  * Interface on behalf of the astronomy community with Compute Canada to ensure that services meet the astronomy domain specific needs.  * Reduce barriers and accelerate adoption of new emerging technologies in Big Data and Computing in the astronomy community.  * Share capabilities developed in the astronomy community with other domains.  # Membership  The consortium membership reflects the necessity of present and future science challenges to integrate those groups and institutions that specialize in the scientific aspects (typically at the universities and research centers and institutes) and the groups that have developed the technical and computer engineering expertise to respond to the needs of science programs.  Consequently the following groups are eligible for membership in the CANFAR consortium:  * Individual university researchers and their groups * University centers or institutions engaged in astronomy research * Multi-institutional or multi-investigator collaborations, including their international partners * Individuals and teams from the CADC, or teams from Compute Canada, WestGrid or other entities that provide computational and data resources and infrastructure   CANFAR's services: * Batch Cloud Processing Service - [Source code](https://github.com/canfar/canfarproc) * Virtual File Persistence Service - [Source code](https://github.com/canfar/storage-prototype) * Virtual Machine On-Demand System * Virtual Storage Service",https://github.com/orgs/canfar/repositories,
2b46c0ee-eced-445d-9630-17d3ec5ae582,4tu-cities-for-citizens-identification-of-public-value-conflicts-in-urban-spaces-accompanying-code,Cities for Citizens: Identification of Public Value Conflicts in Urban Spaces - Accompanying Code,"This is the accompanying code to the Master Thesis research project ""Cities for Citizens: Identification of Public Values and their Conflicts in Urban Spaces"" conducted in cooperation with the CUSP Lab @ TPM TU Delft and the City Science Lab Hamburg. Aiming to identify public values and their spatial conflicts, a total of nine Jupyter notebooks and one R script follow the research flow as outlined in the Master thesis document.",,
2b731abf-56f2-4a61-8e38-6c97f3344b35,4tu-hybridization-toolbox-for-model-predictive-control,Hybridization Toolbox for Model Predictive Control,"This toolbox can be used to hybridize any nonlinear function given as its input argument, which can be either a nonlinear prediction model or the nonlinear function expressing the boundary of the feasible region, i.e. the nonlinear constraints.     A grid is generated on the function domain and the toolbox returns the hybridized form of the nonlinear function. The user can select the type and form of approximation based on the problem type:     For model approximation, the options areselecting the grid type andspecifying the number of affine modes in the MMPS formulation.For constraint approximation, the options arespecifying the number of subregions,selecting between polytopic (MMPS-based) or ellipsoidal approximation, andchoosing between boundary-based or region-based approximation.",,
2bda67ce-7ef8-4bb2-8200-bf7bb89d1f94,4tu-the-role-of-foreign-exchange-reserve-on-foreign-public-debt-in-ethiopian-economy-ardl-model-approach,The role of Foreign Exchange Reserve on Foreign Public Debt in Ethiopian Economy: ARDL Model Approach,"The purpose of this data is to analyze the link between foreign debt and reserve taking experiences from Ethiopian economy. The data is collected from secondary sources, World Bank datasets.",,
2be1b220-0d1a-45fd-bd06-04363c7b9bea,4tu-data-for-performance-metrics-for-the-continuous-distribution-of-entanglement-in-multi-user-quantum-networks,Data for 'Performance metrics for the continuous distribution of entanglement in multi-user quantum networks',"This is the data displayed and discussed in the paper 'Performance metrics for the continuous distribution of entanglement in multi-user quantum networks' (arXiv:2307.01406). The data corresponds to the simulation of a protocol for continuous distribution of entanglement in quantum networks. The data is stored in a single zip file, see README file for more information. The (Python) code used to generate the data can be found in this GitHub repository: https://github.com/AlvaroGI/optimizing-cd-protocols .",,
2c04067c-a0be-4ae6-b7de-4d86d9d86da2,esmvaltool,ESMValTool,"* Facilitates the complex evaluation of ESMs and their simulations submitted to international Model Intercomparison Projects (e.g., CMIP). * Standardized model evaluation can be performed against observations, against other models or to compare different versions of the same model. * Wide scope: includes many diagnostics and performance metrics covering different aspects of the Earth System (dynamics, radiation, clouds, carbon cycle, chemistry, aerosol, sea-ice, etc.) and their interactions. * Well-established analysis: standard recipes reproduce specific sets of diagnostics or performance metrics that have demonstrated their importance in ESM evaluation in the peer-reviewed literature. * High flexibility: new diagnostics and more observational data can be easily added. * Multi-language support: Python, R, NCL, and other open-source languages are possible. * CF/CMOR compliant: data from many different projects can be handled (CMIP, obs4mips, ana4mips, CCMI, CCMVal, AEROCOM, etc.). Routines are provided to CMOR-ize non-compliant data. * Integration in modeling workflows: for EMAC, NOAA-GFDL and NEMO, can be easily extended.   The Earth System Model eValuation Tool (ESMValTool) is a community diagnostics and performance metrics tool for the evaluation of Earth System Models (ESMs) that allows for routine comparison of single or multiple models, either against predecessor versions or against observations. The priority of the effort so far has been to target specific scientific themes focusing on selected Essential Climate Variables, a range of known systematic biases common to ESMs, such as coupled tropical climate variability, monsoons, Southern Ocean processes, continental dry biases and soil hydrology-climate interactions, as well as atmospheric CO2 budgets, tropospheric and stratospheric ozone, and tropospheric aerosols. The tool is being developed in such a way that additional analyses can easily be added. A set of standard recipes for each scientific topic reproduces specific sets of diagnostics or performance metrics that have demonstrated their importance in ESM evaluation in the peer-reviewed literature. The ESMValTool is a community effort open to both users and developers encouraging open exchange of diagnostic source code and evaluation results from the CMIP ensemble. This will facilitate and improve ESM evaluation beyond the state-of-the-art and aims at supporting such activities within the Coupled Model Intercomparison Project (CMIP) and at individual modeling centers. Ultimately, we envisage running the ESMValTool alongside the Earth System Grid Federation (ESGF) as part of a more routine evaluation of CMIP model simulations while utilizing observations available in standard formats (obs4MIPs) or provided by the user.",https://github.com/ESMValGroup/ESMValTool,ESMValGroup/ESMValTool
2c06e73c-9522-4f56-96ed-c7969c9375b7,4tu-code-used-in-the-experiment-associated-with-the-publication-whose-agenda-is-it-anyway-the-effect-of-disinformation-on-covid-19-vaccination-hesitancy-in-the-netherlands,"Code used in the experiment associated with the publication ""Whose Agenda Is It Anyway? The Effect of Disinformation on COVID-19 Vaccination Hesitancy in the Netherlands""","This code was used in the study of disinformation spread in public media and its relationship with vaccination willingness. The repository contains the code that was used to perform the study, including, web scrapping tools, words/bigram counting, qualitative analysis information, the tool used to query Google Trends and Twitter, and finally the statistical analysis of the results. This tool was create as part of the project: ""Who's Agenda is it anyway?"" - a study on disinformation and its influence on vaccination willingness in the Netherlands.",,
2c1d9e82-be0e-46cd-9d43-649bcd0cf91f,4tu-source-code-of-the-swan-model-free-infragravity-waves-in-the-north-sea,Source code of the SWAN model: Free Infragravity Waves in the North Sea,"This dataset contains the source code of the SWAN model (v41.31, http://swanmodel.sourceforge.net/) extended with an empirical source of free infragravity waves. This modified version of the SWAN model can be used to simulate the evolution of free infragravity waves in coastal/oceanic waters that are radiated from adjacent shorelines. The model can be applied to study the evolution of free infragravity waves at a regional to global scale, ranging from inland seas, marginal seas, to ocean basins. In an accompanying paper (10.1002/essoar.10506505.1), the model was used to understand the spatial and temporal variation of free infragravity waves in the North Sea.",,
2cbd7a4d-8bcb-4158-b775-c38fd3f86324,4tu-netlogo-agent-based-model-on-energy-poverty-in-the-switch-to-a-heat-network-in-delft,NetLogo Agent-Based Model on energy poverty in the switch to a heat network in Delft,Agent-based model to explore inequalities in energy poverty and access to energy of households as a result of the switch to a heat network in specific neighbourhoods in Delft. Developed as part of a master's thesis in Industrial Ecology. Zip file includes the NetLogo code and required input data files.,,
2cbfb113-a9d8-4b5c-81b2-21e1d8f5dbb4,haddock3-ui,haddock3-ui,The [haddock3 web application](https://github.com/i-VRESSE/haddock3-webapp) had several components that could be used outside of the web application. This package contains those components.  - Components to render a 3D molecular structure from [PDB file](https://www.wwpdb.org/) using [NGL](https://nglviewer.org/) - Components to select residues in a molecule either passive or active - Components to handle files - Components to render clusters or structures in a sortable table,https://github.com/i-VRESSE/haddock3-ui,i-VRESSE/haddock3-ui
2cdd6cad-884d-402f-b4d4-e86936694d24,4tu-code-from-paper-palladium-zero-mode-waveguides-for-optical-single-molecule-detection-with-nanopores,Code from Paper Palladium Zero-Mode Waveguides for Optical Single Molecule Detection with Nanopores,Different python scripts to plot and analyze the attached data. Please see the readme for more information.,,
2cf8bc0c-01b1-4cdb-871e-7be41e048822,jaspprocess,jaspProcess,"Test and compare classical and Bayesian conditional process models in JASP using mediation, moderation, and moderated mediation analyses.",https://github.com/jasp-stats/jaspProcess,jasp-stats/jaspProcess
2d666b7e-28af-48c6-9cd5-ae60d307912b,4tu-source-code-for-the-publication-learning-safety-in-model-based-reinforcement-learning-using-mpc-and-gaussian-processes,Source code for the publication: Learning safety in model-based Reinforcement Learning using MPC and Gaussian Processes,Source code for a safety-aware MPC-based RL framework via Gaussian Processes,,
2d71d038-a1fe-40ea-a079-c3f43977f22c,2022-verhagen-001,"Get fertility rates (females, 5-year cohort)","This NetLogo code snippet provides fertility estimates of females for use in demographic simulations. The figures used are based on two different sources, one for Roman Egypt and one for modern populations, and will be assigned per 5-year age cohort.  The code snippet is a procedure that can be inserted into a NetLogo model and will not run independently.",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Verhagen-001,
2e4ab42d-5749-4d24-a496-1d78b1883f33,4tu-ijnme-computational-homogenization-of-poroelastic-media,IJNME computational homogenization of poroelastic media,"A computational homogenization framework is proposed for solving transient wave propagation in the linear regime in heterogeneous poroelastic media that may exhibit local resonances due to microstructural heterogeneities. The microscale fluid-structure interaction problem and the macroscale are coupled through an extended version of the Hill-Mandel principle, leading to a variationally consistent averaging scheme of the microscale fields. The effective macroscopic constitutive relations are obtained by expressing the microscale problem with a reduced-order model that contains the longwave basis and the so-called local resonance basis, yielding the closed-form expressions for the homogenized material properties. The resulting macroscopic model is an enriched porous continuum with internal variables that represent the microscale dynamics at the macroscale, whereby the Biot model is recovered as a special case. Numerical examples demonstrate the frameworkâ€™s validity in modeling wave transmission through a porous layer.     This software publication provides the MATLAB script and COMSOL models supporting the article.",https://data.4tu.nl/v3/datasets/051c8648-cacf-41b9-bdd6-9769b44ca9b8.git,
2ea126b8-d23a-463b-b54e-1e56410f5450,qans,QANS,"QANS (Questionnaire And Nomination Software) is a collection of software components to enable longitudinal research using sociometric (peer nomination, peer ranking) surveys. This includes a personal-data subsystem, extensions/plugins to the online survey tool LimeSurvey, and an exporter for the answer data generated by LimeSurvey.",https://gitlab.socsci.ru.nl/qans/qans,
2ee98bf8-2d49-482c-81cf-44844d5c3b0f,4tu-ml-mocap-controller-and-camera-module-images,ML-MoCap - Controller and Camera module Images,This image is part of the ML-MoCap system to build the controller and/or a camera module. For more info and installation manuals check the Readme on the related ML-MoCap project Github page.,,
2ee9af1d-2340-4d10-8784-76febc43d7a6,4tu-data-underlying-the-publication-data-efficient-challenges-in-visual-inductive-priors-a-retrospective,Data underlying the publication: Data-Efficient Challenges in Visual Inductive Priors: A Retrospective,"Code used to create the article ""Data-Efficient Challenges in Visual Inductive Priors: A Retrospective"". This article summarizes the findings of four successive years of open challenges in the field of Computer Vision, aiming to stimulate research into data-efficient computer vision.",https://data.4tu.nl/v3/datasets/b818ce6f-68ae-430d-b7bf-8663478e957c.git,
2eed759f-4ed9-40d8-bf2a-cea0e614a3f3,champ,CHAMP-EU,,https://github.com/filippi-claudia/champ,filippi-claudia/champ
2f040204-b0fe-4b2e-a389-e9096c7ddb85,i-emic,I-EMIC,,https://github.com/nlesc-smcm/i-emic,nlesc-smcm/i-emic
2f598456-6ddd-41ab-9788-632b447135b5,2021-galan-001,Random walk in data frames,"![Example of output: sequential random walk in continuous 2D space](r_implementation/2D-Random-walk_files/figure-html/sequential-2Dcontinuous-1.png)  [Code and demonstration in Rmarkdown](http://htmlpreview.github.io/?https://github.com/Archaeology-ABM/NASSA-modules/blob/main/2021-Galan-001/r_implementation/2D-Random-walk.html)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |initialX, initialY|float (numeric)|initial position x and y coordinates.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |trajectory|data frame|A dataframe (tibble in R) with three columns holding numeric variables (named ""steps"", ""X"", and ""Y"") where each row correspond to the index and xy coordinates of a step in a random walk.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2021-Galan-001,
2f59e0e8-4257-46a8-bcfc-f1c78b251269,4tu-macroscopic-fundamental-diagram-walking-experiment-software,Macroscopic Fundamental Diagram walking experiment software,"This software gives the tools to prepare and record a walking experiment in which an macroscopic fundamental diagram is created. Tools are twofold:  1) Tool to create cards of routes  2) Tool to have the counting of the number of people in the experiment logged, and run simultaneously on two mobile devices.     Version 2.0 (Nov 2025) adds Python tools for the preparation and analyses as alternative to the Matlab files. Note that the tools (part 2) above still combines html and php.",,
3004cb95-e88d-4970-af19-6c81a9c76b39,knime-archetype,KNIME node archetype,* For KNIME node developers who like to have  quick start and automated builds * Generates a project skeleton that contains all you need to develop your own KNIME nodes * Automates tests and deploying the node to an update site * Includes dependency management and test coverage * Used to create all KNIME nodes in the 3D-e-Chem project,https://github.com/3D-e-Chem/tycho-knime-node-archetype,3D-e-Chem/tycho-knime-node-archetype
30282a6a-ec19-42c9-b3f6-00af730ae4d9,4tu-pybanshee-python-based-open-source-of-the-matlab-toolbox-banshee,"PyBanshee, Python-based open source of the MATLAB toolbox BANSHEE.","PyBanshee is a Python-based open source of the MATLAB toolbox BANSHEE.       Â Bayesian Networks (BNs) are probabilistic, graphical models for Â representing complex dependency structures. They have many applications Â in science and engineering. Their particularly powerful variant â€“ Â Non-Parametric BNs â€“ are Â implemented as a Matlab toolbox and an Â open-access scriptable code, in the form of a Python-based package.",,
306730a5-8160-4f93-b86e-c3f80147a5a0,nanopub,nanopub,"* Provides a high-level, user-friendly python interface for the nanopub server. * Enables searching, publishing and retracting nanopublications. * Instead of worrying about the details of composing and interacting with RDF yourself, you can us nanopub's easy to use API. nanopub is the only existing client for the nanopub server.   # nanopub The ```nanopub``` library provides a high-level, user-friendly python interface for searching, publishing and retracting nanopublications.  Nanopublications are a formalized and machine-readable way of communicating the smallest possible units of publishable information. See [the documentation](https://nanopub.readthedocs.io/en/latest/what-are-nanopubs.html) for more information.   ## Publishing The nanopub library provides an intuitive API that makes publishing nanopublications much easier. The rationale is that you often do not want to worry about the details of composing the RDF that is often the same in each nanopublication. Instead you should focus on the content of your nanopublication: the assertion.  ## Searching The `NanopubClient` provides methods for searching the nanopub server. It provides an (uncomplete) mapping to the nanopub server grlc endpoint.  ## Fetching You can fetch nanopublications from the nanopub server using `NanopubClient.fetch()`. The resulting object is a `Publication` object that you can use to inspect the nanopublication.",https://github.com/fair-workflows/nanopub,fair-workflows/nanopub
30709d5c-dc2e-4332-bcf9-d315dc1c6a09,4tu-spacy-extensions,spacy-extensions,"This repository contains the code for EAGLE by the authors of the Threat Intelligence Processing paper. Please cite EAGLE when using it in academic publications. This repository provides a stable artefact version of EAGLE. For the most up-to-date version that receives updates, please see the Github repository.",https://data.4tu.nl/v3/datasets/fbf2ce6b-207e-4049-80f1-e5914d4c4519.git,
308b6f5e-045a-4334-bfc5-6158538a4056,4tu-structure-calibration-sparse-data,structure_calibration_sparse_data,"This repository is used to calibrate the underlying structure of a stylized supply chain simulation model of counterfeit Personal Protective Equipment (PPE). For this, we use four calibration techniques: Approximate Bayesian Computing using&nbsp;pydream, Bayesian Optimization using&nbsp;bayesian-optimization, Genetic Algorithms using&nbsp;Platypus, and Powell's Method using&nbsp;SciPy. The calibration is done with sparse data, which is generated by degrading the ground truth data on noise, bias, and missing values. We define the structure of a supply chain simulation model as a key value of a dictionary (sorted on betweenness centrality), which is a set of possible supply chain models. The integer is, thus, the decision variable of the calibration.     To use this repository, we need a simulation model developed in&nbsp;pydsol-core&nbsp;and&nbsp;pydsol-model&nbsp;. Additionally, we need a dictionary with various different simulation structures as input, as well as the ground truth data. For this project, we use the repository&nbsp;complex_stylized_supply_chain_model_generator&nbsp;as simulation model.     This repository is an extension of the&nbsp;celibration&nbsp;library, making it easy to plugin different calibration models, distance metrics and functions, and data.     This repository is also part of the Ph.D. thesis of&nbsp;Isabelle M. van Schilt, Delft University of Technology.",https://data.4tu.nl/v3/datasets/0da30dbc-541f-44f3-a73d-8fd852b7cb0d.git,
30b68214-d92e-4ff0-8350-c71b11ccba15,mss-improv,MSS-IMProv,"Integrative structural biology requires the aggregation of multiple distinct types of data to generate models that satisfy all inputs. Here, we present IMProv, an app in the Mass Spec Studio that seamlessly combines XLâ€“MS data with other structural data, such as cryo-EM densities and crystallographic structures, for integrative structure modeling using IMP (www.integrativemodeling.org).",,
30baa07a-06a4-4d14-907f-e0e0c06b0786,4tu-deepcase,DeepCASE,"This repository contains the code for DeepCASE by the authors of the IEEE S&amp;P DeepCASE paper [PDF]. Please cite FlowPrint when using it in academic publications. This repository provides a stable artefact version of FlowPrint. For the most up-to-date version that receives updates, please see the Github repository.",https://data.4tu.nl/v3/datasets/b80935fb-cada-467b-847c-90073d939693.git,
30c9d4a1-c25b-4387-9e30-924b61d20a8a,jupyterdaskonslurm,JupyterDaskOnSLURM,"A repository that contains instructions and material to setup and run a Jupyter server and a Dask cluster on a SLURM system, such as the Spider data processing platform and the Snellius supercomputer, both hosted by SURF.",https://github.com/RS-DAT/JupyterDaskOnSLURM,RS-DAT/JupyterDaskOnSLURM
31165921-55cd-46cc-bf1b-6cae5c0d6925,ewaterleaf,eWaterLeaf,"* Simple visualisation of Geospatial data. * Web based Visualisation of NetCDF data. * Little to no configuration necessary.  eWaterLeaf is a very simple Web based Visualization for Gespatial data. It supports any NetCDF-CF compliant file, the most supported format in Geosciences.  eWaterLeaf is based on the widespread Web Map Service (WPS) standard. Next to a Javascript visualization based on Leaflet (hence the name), it also includes a server (based on ncWMS) for serving any NetCDF-CF file.",https://github.com/eWaterCycle/eWaterleaf,eWaterCycle/eWaterleaf
31cd845f-7e4d-4921-8d85-037e21f2ded7,4tu-python-code-for-the-analysis-of-aeolis-grain-size-scenarios,Python code for the analysis of AeoLiS grain size scenarios,"This repository contains the Python code that was used to analyze grain size scenarios simulated with AeoLiS, as presented in:  - van IJzendoorn, C.O., Hallin, C., A.J.H.M. Reniers, de Vries, S. (submitted) Modeling multi-fraction coastal aeolian sediment transport with horizontal and vertical grain size variability.     A second zip file is attached that contains the output that can be created with the scripts that are available in this repository and at Github.",,
31deff18-adf2-48ff-8137-05c88bbbebba,4tu-enabling-inter-organizational-collaboration-through-platforms-the-role-of-trust-the-freightbooking-game-trans-sonic-project,Enabling Inter-Organizational Collaboration Through Platforms: The Role of Trust - The FreightBooking game (Trans-SONIC project),"The FreightBooking game was conducted as part of the Trans-SONIC project,  funded by The Dutch Research Council (NWO).&nbsp;     The FreightBooking game is played with 4 different groups and organized as such.  - Group A: Gaming experts  - Group B: Master student, MBA Students with expertise in logistics, and supply chain. Some have work experience in the field  - Group C: Master students with expertise in supply chain networks and serious games  - Group D: Master students with expertise in game design     For the study, two questionnaires were designed and analyzed (one pre-game questionnaire and one post-game questionnaire).  - Pre-game questionniare included items to measure participants' usage of platforms in daily-life, the risk they perceived and their work experience.  - Post-game questionnaire included items to meausre participants' game play (i.e., decisions during the game), game experience and disposition to trust.  The pre- and post-game questionnaires were combined with the gameplay data.&nbsp;     The gameplay data consists of:  - GamePlayData. This type of dataset is used to analyze the gameplay.     The dataset contains Per group the pre- and post-game questionnaire (PDF and .csv files), GamePlay data (fully anonymized, .csv files)     Additionally, zipfiles were created of the FreightBooking game:  - trustgame-admin-main.zip  - trustgame-main.zip  - trustgame-register-main.zip  - trustgame-scores-main.zip",,
31f95f2e-140f-4d9d-9699-45c186f48aac,ecearth,EC-Earth,"Changes in our climate are leading to a range of varied and significant impacts affecting ecosystems and human systems such as the agriculture, water resources, natural resources, economic activities and infrastructures. To limit risks and identify opportunities associated with the changes, we need to understand how society is affected, and what can be done to adapt, as well as to reduce climate change.  An Earth System Model (ESM) is a tool that integrates all our knowledge on the Earth system and can be used to address scientific hypothesis on climate and environmental change. As such, these models allow scientists to study the complex interactions within the Earth System and are an essential tool for understanding and predicting climate variability and climate change. EC-Earth is developed as part of a Europe-wide consortium thus promoting international cooperation and access to knowledge and a wide data base. EC-Earthâ€™s main objective is to develop and apply an ESM based on ECMWF's seasonal forecasting system for providing trustworthy climate information to climate services and to advance scientific knowledge on the Earth system, its variability, predictability and long-term changes resulting from external forcing.  EC-Earth has become a prominent state-of-the-art model within the European landscape of Earth System Models, as shown by the involvement in many European projects and its significant contribution to CMIP5 & CMIP6.",https://git.smhi.se/ec-earth,
3223da28-ac27-49ee-82a5-608ccd99cecf,resurfemg,ReSurfEMG,"ReSurfEMG supports functionality for pre-processing, analysis and machine learning for analysis of respiratory electromyography (EMG) signals. This functionality can be accessed by command line, Jupyter notebook or a supported graphical user (no code) tool.",https://github.com/ReSurfEMG/ReSurfEMG,ReSurfEMG/ReSurfEMG
325e550b-742e-45dc-aad0-c9c90f898404,magic-castle,Magic Castle,"The Digital Research Alliance of Canada provides HPC infrastructure and support to every academic research institution in Canada. The Alliance uses CVMFS, a software distribution system developed at CERN, to make its research software stack available on its HPC clusters, and anywhere else with internet access. This enables replication of the user experience outside of The Alliance physical infrastructure.  From these new possibilities emerged an open-source software project named Magic Castle, which aims to recreate the HPC user experience in public clouds. Magic Castle uses the open-source software Terraform and HashiCorp Language (HCL) to define the virtual machines, volumes, and networks that are required to replicate a virtual HPC infrastructure. The infrastructure definition is packaged as a Terraform module that users can customize as they require. After deployment, the user is provided with a complete HPC cluster software environment including a Slurm scheduler, a Globus Endpoint, JupyterHub, LDAP, DNS, and over 3000 research software applications compiled by experts with EasyBuild. Magic Castle is compatible with AWS, Microsoft Azure, Google Cloud, OpenStack, and OVH.",https://github.com/ComputeCanada/magic_castle,ComputeCanada/magic_castle
326d863b-5c3a-43d4-86ad-f63f066e0bf4,research-portal-for-secure-data-discovery-access-and-collaboration,"Research Portal for Secure Data Discovery, Access, and Collaboration",,https://researchportal.indocresearch.org/,
32cea337-1f1f-4f20-9abf-449032c34788,protein-detective,protein-detective,,https://github.com/haddocking/protein-detective,haddocking/protein-detective
3356e353-1210-444e-af27-653e4a017603,4tu-swmm-gnn-repository-paper-version,SWMM_GNN_Repository_Paper_version,"Repository of the code for the GNN based metamodel of SWMM. This code is linked to the paper ""Transferable and Data Efficient Metamodeling of Storm Water System Nodal Depths Using Auto-Regressive Graph Neural Networks"" by Alexander GarzÃ³n, Zoran Kapelan, Jeroen Langeveld, and Riccardo Taormina.    This repository contains the code for developing machine learning metamodels of SWMM.  In brief, this code allows to create a dataset from SWMM simulations, train a machine learning model, and evaluate the model. The code is designed to work with SWMM simulations of storm water systems. The code is based on PyTorch and PyTorch Geometric.    This work is supported by the TU Delft AI Labs programme.  This repository was supported by the Digital Competence Centre, Delft University of Technology.",https://data.4tu.nl/v3/datasets/3e58770f-ae91-482a-b9c9-b51fa2692bf0.git,
33d90560-e732-4242-a7e2-b5800fdcf311,4tu-fame-amr-model,FAME_AMR_model,This dataset contains the results of numerical simulations performed to investigate the influence of various layering strategies on the performance of active magnetocaloric regenerators (AMRs) for heat pump applications. These results were published in Applied Thermal Engineering 232 (2023) 120962. The simulations were performed using a one-dimensional numerical model of an AMR implemented in Python.,https://data.4tu.nl/v3/datasets/43678a8e-4de4-4636-8a2f-4a5ea17d770e.git,
346d63db-3a21-42a0-961d-e15178f5bace,4tu-code-for-detecting-anomalous-misconfigurations-in-aws-identity-and-access-management-policies,Code for Detecting Anomalous Misconfigurations in AWS Identity and Access Management Policies,"This repository contains the code for Misdet by the authors of the CCSW '22 paper [PDF]. Please cite Misdet when using it in academic publications. This repository provides a stable artefact version of Misdet. For the most up-to-date version that receives updates, please see the Github repository.",https://data.4tu.nl/v3/datasets/85847a14-262b-4c55-9b19-ae9117b50090.git,
346f61d1-6c0b-4f4d-a7f8-d645b57ef459,sharetrait-website,ShareTrait website,"Share Trait web application is a web application that allows users to deposit trait data in the Share Trait database.  A dataset can be added using a multi step form. The steps roughly are  1. Source metadata like manuscript DOI where trait was published 2. Location of where trait of an individual was measured 3. Environment of where the individual was measured, like depth and temperature. 4. Description of the population of the individual, like taxonomy from Catalogue of Life and GBIF Backbone. 5. Individual information like sex and parent. 6. The experimental setup for example in a glass respiratory chamber with aquatic breathing mode. 7. Lastly the measurement which is a value together with a trait like egg development time, an individual and experimental setup",https://github.com/ShareTraitProject/ShareTraitWebsite,ShareTraitProject/ShareTraitWebsite
34881094-59fe-4ccf-b319-394c569a5610,nlesc-python-template,Netherlands eScience Center Python Template,"* Template for a basic Python project structure * Set up with testing and documentation * Comply to the software development guide * Used in both eScience and external projects  When starting a new Python project, consider using this template. It provides a basic project structure, so you can spend less time setting up and configuring your new Python package, and comply with the software guide right from the start. The 'empty' Python package created with this template is set up with documentation and testing. A README specifying details about the project setup is also included in your new Python package.",https://github.com/NLeSC/python-template,NLeSC/python-template
350daa73-cfd5-4036-b98c-e869cd3af60e,4tu-travia-a-traffic-data-visualization-and-annotation-tool-in-python,TraViA: a Traffic data Visualization and Annotation tool in Python,a Traffic data Visualization and Annotation tool - version 1.1 as published in the Journal of Open-Source Software,,
350db460-5109-4acc-af99-b2ac776c7bdb,vqls-prototype,vqls-prototype,Use the VQLS approach to solve linear systems on IBM-Q hardware or simulators,https://github.com/QuantumApplicationLab/vqls-prototype,QuantumApplicationLab/vqls-prototype
353b9b70-1127-4630-b0c1-5810bdcd0e32,4tu-integrated-model-predictive-fuzzy-control-for-disaster-victim-detection-path-planning,Integrated-Model-Predictive-Fuzzy-Control-for-Disaster-Victim-Detection-Path-Planning,"This script simulates various configurations of UAVs in search and rescue&nbsp;(SAR) operations using different control architectures, environment&nbsp;models, and agent behaviors. The aim is to evaluate their performance&nbsp;across a range of scenarios by analyzing the objective function,&nbsp;optimization times, and other metrics. This guide explains each section&nbsp;of the script and how to customize it for specific simulation setups.",https://data.4tu.nl/v3/datasets/78beaf74-3260-4a07-b235-c733929e9112.git,
356b3c2b-fdf2-47df-aa00-7493a120a7e3,emo-spectre,Emo-spectre,"* Feature extraction from video * Visualization and exploration of features   Emo-spectre is a web application for interactive visualization and exploration of multi-modal time-series data and video.  Preloaded data can be visualized when the video is available or you can process data yourself by providing the required files through the upload interface. Features (facial features, audio and sentiment) are then extracted from the data and saved in a database that can subsequently be explored.",https://github.com/emrecdem/emo-spectre,emrecdem/emo-spectre
3571d01a-cd7e-47d8-8805-f70c5d8e6474,recipy,recipy,"* Keep track of what code you ran to generate results (e.g., graphs or data) * Add a single statement to enable provenance tracking in your Python script * Search your runs using a command line interface or GUI * Customize provenance tracking for each project  Imagine the situation: Youâ€™ve written some wonderful Python code which produces a beautiful graph as an output. You save that graph, naturally enough, as graph.png. You run the code a couple of times, each time making minor modifications. You come back to it the next week/month/year. Do you remember how you created that graph? What input data? What version of your code? Frustratingly, the answer will often be 'no'. Of course, you then waste lots of time trying to work out how you created it, or even give up and never use it in that journal paper that will win you a Nobel Prizeâ€¦  ReciPy (from recipe and python) is a Python module that will save you from this situation! (Although it canâ€™t guarantee that your paper will win a Nobel Prize!) With the addition of a single line of code to the top of your Python files, ReciPy will log each run of your code to a database, keeping track of the input files, output files and the version of your code, and then let you query this database to find out how you actually did create graph.png.",https://github.com/recipy/recipy,recipy/recipy
35c0bb73-b8b1-4e34-8205-627b13dac24c,sedtrails,SedTRAILS,,https://github.com/sedtrails/sedtrails,sedtrails/sedtrails
35fbd58b-4c18-4933-b54e-606300dc49d6,citation-file-format,Citation File Format,"1. Users of your software can easily cite it using the metadata from `CITATION.cff`. 2. If your repository is hosted on GitHub, they will [show the citation information in the sidebar](https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/about-citation-files), which makes it easy for visitors to cite your software or dataset correctly. 3. When you publish your software on Zenodo via the [GitHub-Zenodo integration](https://guides.github.com/activities/citable-code/), they will use the metadata from your `CITATION.cff` file. 4. People can import the correct reference to your software into the [Zotero](https://www.zotero.org) reference manager via a [browser plugin](https://www.zotero.org/download/).",https://github.com/citation-file-format/citation-file-format,citation-file-format/citation-file-format
36002d85-a508-453b-8c43-cb0c72684a55,emvoice,emvoice,"**emvoice** is a pure Python package for computing emotion expression-related features from speech signals. It uses similar algorithms as in [Praat](https://www.praat.org) and [openSMILE](https://github.com/audeering/opensmile/) but also includes more recent methods from the [librosa](https://librosa.org/doc/latest/index.html) package. Currently, most features from the extended Geneva Minimalistica Acousting Parameter Set ([eGeMAPS](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7160715)) are implemented.  Given that it is entirely written in Python, it is easier to include emvoice in other Python-based applications, especially if they use numpy or scipy.",https://github.com/mexca/emvoice,mexca/emvoice
36188a3b-d99e-4bb8-9879-a465650b9815,jube,JUBE,"Benchmarking a computer system and the execution of complex workflows usually involves numerous tasks, involving several runs of different applications. Configuring, compiling, and running those applications on several platforms with the accompanied tasks of result verification and analysis needs a lot of administrative work and produces a lot of data, which has to be analysed and collected in a central database. Without a workflow and benchmarking environment all these steps have to be performed by hand.  The JUBE workflow and benchmarking environment provides a script based framework to easily create workflow and benchmark setups, execute them on different computer systems and evaluate the results. It is actively developed by the JÃ¼lich Supercomputing Centre of Forschungszentrum JÃ¼lich, Germany.  **Contact** jube.jsc@fz-juelich.de",https://github.com/FZJ-JSC/JUBE,FZJ-JSC/JUBE
3670a21c-2696-4d49-a478-f067d1b2c6a7,4tu-uncertainty-specification-and-analysis-for-thermosphere-observations,Uncertainty Specification and Analysis for Thermosphere Observations,"The software can specify the uncertainty of thermosphere&nbsp;density observations derived from accelerometer and GNSS&nbsp;tracking data. The user must specify the accuracy of input&nbsp;parameters describing the measurements, satellite&nbsp;geometry, surface properties, atmospheric conditions, and&nbsp;radiation fluxes. These uncertainties are propagated to&nbsp;the uncertainty of density observations. The software will&nbsp;generate several visualizations to assess and analyze the&nbsp;accuracy of the density observations. The software is&nbsp;implemented in Matlab and uses several external libraries&nbsp;linked via Matlab's MEX functionality.",https://data.4tu.nl/v3/datasets/56b96866-f82f-4967-9c1d-0f1e1a1ba383.git,
36cec5d5-f4f2-4e6f-8922-d3a9df8496c0,4tu-probabilisticvalidation,ProbabilisticValidation,"MATLAB code to reproduce results presented in the paper ""Privacy-Preserving Data Aggregation with Probabilistic Range Validation"".     The source code is available as a git repository.     The source code was published by the paper's authors several years after the paper was published.     Git repository  Relevant code is stored in the src directory.     Measures and visualises various metrics shown in the paper. The settings in the various scripts correspond exactly to those used to achieve the results in the paper. The code is fully deterministic and gives the exact same results each time.     Unfortunately, Figure 6 in the paper was generated with a version of this code in which the seed for the random number generator was not configured correctly, and as a result Figure 6 cannot be recreated exactly. However, the outputs of the scripts in the repository are not significantly different from the published Figure 6, and do not undermine or alter the conclusions in any significant way.     See ARTIFACT-EVALUATION.md in the root folder for detailed end-user instructions.",https://data.4tu.nl/v3/datasets/8403bde5-f0cd-4cb3-a30f-d0365df23318.git,
3731425d-fe1d-4fda-8b57-c201d668cb71,4tu-code-underlying-the-publication-the-effect-of-the-covid-19-pandemic-on-a-mooc-in-aerospace-structures-and-materials,Code underlying the publication: The Effect of The Covid-19 Pandemic on a MOOC in Aerospace Structures and Materials.,"This dataset contains the Juniper notebook code used to extract relevant data from the edX and pre- and post- survey data of the 2019 and the 2020 run of the Aerospace Structures and Materials MOOC as supplied by the TU Delft Extension School. The actual underlying data that is being analyzed is not permitted to be shared publicly. The research objective for which the analysis was carried out was: To investigate how the COVID-19 epidemic affected the motivation and the way learners interact with our course material by using learning analytics approaches to analyze the log data available from the edX platform and the data from pre- and post-course evaluations of two runs of the same MOOC (2019 and 2020).The main research question was: How does the COVID-19 pandemic affect the motivation and the way learners interact with the course material in the MOOC introduction to Aerospace Structures and Materials on edX? The data was analysed statistically after being extracted from the supplied datasets using this Juniper notebook. The work was reported in a conference paper: Jivet, I., &amp; Saunders-Smits, G.N., 2021, THE EFFECT OF THE COVID-19 PANDEMIC ON A MOOC IN AEROSPACE STRUCTURES AND MATERIALS, SEFI Annual Conference 2021.",,
374fe5f5-aa14-4ee0-88d7-ab9672e8260a,4tu-code-underlying-the-publication-zero-shot-day-night-domain-adaptation-with-a-physics-prior,Code underlying the publication: Zero-Shot Day-Night Domain Adaptation with a Physics Prior,"Code corresponding to ICCV 2021 submission ""Zero-Shot Day-Night Domain Adaptation with a Physics Prior"".     Abstract  We explore the zero-shot setting for day-night domain adaptation. The traditional domain adaptation setting is to train on one domain and adapt to the target domain by exploiting unlabeled data samples from the test set. As gathering relevant test data is expensive and sometimes even impossible, we remove any reliance on test data imagery and instead exploit a visual inductive prior derived from physics-based reflection models for domain adaptation. We cast a number of color invariant edge detectors as trainable layers in a convolutional neural network and evaluate their robustness to illumination changes. We show that the color invariant layer reduces the day-night distribution shift in feature map activations throughout the network. We demonstrate improved performance for zero-shot day to night domain adaptation on both synthetic as well as natural datasets in various tasks, including classification, segmentation and place recognition.",https://data.4tu.nl/v3/datasets/97147efb-7946-40aa-8053-299da69f22f0.git,
37827c21-3201-430a-b13c-9d34d0f90954,swyft,swyft,"* Estimates likelihood-to-evidence ratios for arbitrary marginal posteriors; they typically require fewer simulations than the corresponding joint.  * Performs targeted inference by prior truncation, combining simulation efficiency with empirical testability.  * seamlessly reuses simulations drawn from previous analyses, even with different priors.  * integrates dask and zarr to make complex simulation easy.  swyft is designed to solve the Bayesian inverse problem when the user has access to a simulator that stochastically maps parameters to observational data. In scientific settings, a cost-benefit analysis often favors approximating the posterior marginality; swyft provides this functionality. The package additionally implements our prior truncation technique, routines to empirically test results by estimating the expected coverage, and a dask simulator manager with zarr storage to simplify use with complex simulators.",https://github.com/undark-lab/swyft,undark-lab/swyft
378da8d9-4ded-48a1-8074-4d1acf415bd0,4tu-deeplog,DeepLog,"This repository contains the code for for DeepLog that was implemented as part of the IEEE S&amp;P DeepCASE paper [PDF], it provides a Pytorch implementation of DeepLog [PDF]. We ask people to cite both works when using the software for academic research papers.",https://data.4tu.nl/v3/datasets/62fe208e-98d5-4451-8e07-784893414fcc.git,
37c73eee-5e36-4960-b119-c54435faac01,one-button-compute,One button compute,"* Provides a web interface to run a CWL workflow on each input file * Downloads input files from remote storage, runs a CWL workflows and uploads the output files to remote storage * The user needs to supply the input files and workflow, he/she does not need to care about how the workflow is run",https://github.com/surf-eds/one-button-compute,surf-eds/one-button-compute
37e5e7d9-c150-42a0-8331-a8e48c14bb35,review-argumentation-at-scale,Review Argumentation at Scale,,https://github.com/davideceolin/FAReviews,davideceolin/FAReviews
386cfb81-73cd-46d7-862c-3cb19a97312f,satsense,Satsense,"- Provides a framework for performing land use classification on satellite images - Comes with easy to use Jupyter notebook examples - Provides an implementation of hand-crafted features commonly used for detecting deprived neighbourhoods in satellite images, like HoG, Lacunarity, NDXI, Pantex, Texton, SIFT - Will provide various metrics for measuring performance   Satsense is a Python library for land use classification, with a particular focus on deprived neighbourhood detection. However, many of the algorithms made available through Satsense can be applied in other domains. Detection of deprived neighbourhoods is a land use classification problem that is traditionally solved using hand crafted features like HoG, Lacunarity, NDXI, Pantex, Texton, and SIFT with very high resolution satellite images. One of the problems with assessing the performance of these kind of algorithms for this application, is that there is no easy to use open source reference implementation of such features, a problem that Satsense solves. In the future Satsense will also provide metrics to assess the performance. Satsense is built in a modular way which makes it easy to add your own hand-crafted feature or use deep learning instead of hand crafted features.",https://github.com/DynaSlum/satsense,DynaSlum/satsense
3889cfc4-5ecc-435e-a2fd-9d22872f3485,4tu-course-introduction-map,course-introduction-map,"Our â€œcourse introduction mapâ€ is an interactive tool designed for large soil and landscape-related courses, providing a brief (5-minute) introduction. Students access a link to mark their place of origin or current residence on a map of the Netherlands, with the collective data of all participants displayed alongside a summary information about the related physical-geographic regions. This approach immediately engages students, and allows teachers to display and reference the groupâ€™s results throughout the landscape-related course.&nbsp;",https://data.4tu.nl/v3/datasets/fe55c944-32ed-4f39-ae21-80742c18677d.git,
389d49ff-7b40-4d23-892e-1111c8de83d9,4tu-code-supporting-the-paper-stability-of-a-one-dimensional-morphoelastic-model-for-post-burn-contraction,Code supporting the paper: Stability of a one-dimensional morphoelastic model for post-burn contraction,"This online resource shows an archived folder including Matlab files used for the paper Stability of a oneâ€‘dimensional morphoelastic model for postâ€‘burn contraction. Within the archived folder, one finds two folders called Convergence and Stability. The Convergence folder contains codes for the convergence validation and has a Matlab script (Convergence.m). The Stability folder contains codes for the stability validation and has a Matlab live script (Stability.mlx). Some of the scripts are for graphical recordings, such as videos and figures, others are for assembling the matrices and vectors. Some of the code in the live script is commented out, because we had to end these studies (because of NaN's). In the fifth study, we used different plotting data and therefore we defined a vector plotdays_study5.mat. Within the scripts, there are comments explaining the code.",,
38ddaa59-1cb5-4343-b739-41a7131ecdd4,bird-cloud-gnn,bird-cloud-gnn,"bird-cloud-gnn is a Python package developed to generate graph representations from point cloud data. Utilising the Keras library, the software processes the point cloud data and further feeds it into a Graph Neural Network (GNN) for classification. Each point within the dataset is individually classified through the GNN.  A notable feature of the software is its ability to handle situations with limited labeled data. The software itself was motivated to overcome shortage of labeled data and augment it which significantly increased the amount labeled data by a factor 1000 (depending on chosen settings), thereby enhancing the model's performance and robustness.  In summary, bird-cloud-gnn leverages advanced neural network methodologies to efficiently classify point cloud data, addressing common challenges related to data scarcity through substantial augmentation.  Software can:  - Generate graph representations from point cloud data. - Classify each point within the dataset using a Graph Neural Network (GNN). - Utilise the Keras library for efficient data processing and neural network implementation. - Address situations with limited labeled data through extensive data augmentation techniques. - Increase the amount of labeled data by a factor of approximately 1000, improving model performance. - Enhance the robustness of the classification model through significant data augmentation. - Leverage advanced neural network methodologies for efficient data classification. - Provide a flexible and scalable solution for point cloud data classification tasks.",https://github.com/point-cloud-radar/bird-cloud-gnn,point-cloud-radar/bird-cloud-gnn
38de344d-96f1-4798-a70a-a0f9c931ac71,eucp-atlas,Atlas of climate projections,"* Climate projections can be useful for many people, from communities to policymakers to businesses.",https://github.com/eucp-project/atlas,eucp-project/atlas
38f67bf7-80ab-4241-8e2b-311436a68b28,geospatial-python,Introduction to Geospatial Raster and Vector Data with Python,"Python is one of the most popular programming languages for data science and analytics, with a large and steadily growing community in the field of Earth and Space Sciences.   This lesson material helps participants with a basic knowledge of Python to familiarize with a set of tools from the Python ecosystem to work with geospatial raster and vector data.   In particular, it considers satellite images and public geo-datasets and it demonstrates  how these can be opened, explored, manipulated, combined, and visualized using Python.",https://github.com/carpentries-incubator/geospatial-python,carpentries-incubator/geospatial-python
39090afa-2e87-499c-90ed-8fd8ed6532a1,pathvisio,PathVisio,PathVisio enables the curation of biological pathways for WikiPathways (get started with pathway curation with the WikiPathways Academy http://academy.wikipathways.org/).,https://github.com/pathvisio/pathvisio,pathvisio/pathvisio
39cce3e4-df2a-4572-97d9-78acdd1fae84,4tu-matlab-scripts-describing-a-nonlinear-steady-state-cornering-model-for-an-electric-city-bus,MATLAB-scripts describing a nonlinear steady-state cornering model for an electric city bus,"The MATLAB-files contained within this dataset describe a nonlinear  steady-state cornering model for a six-wheel city bus. Due to the employed  multi-body approach, the wheel-configuration of the model can be changed  easily. The wheel location and orientation is completely parameterized,  allowing for the analysis of different types of vehicles. Emphasis is placed  on the calculation of the cornering resistance power and power lost due to  scrub losses to research the effect of these tire-effect on the vehicle energy  consumption.    The current model includes six wheels, where the two sets of double rear  wheels have individual rotational velocities. In the model derivation,  linearizations are avoided: large angles are allowed and the non-linear Magic  Formula is employed to calculate the tire forces. Additionally, lateral load  transfer effects, due to the elevated center of gravity (CoG), are included.  The developed non-linear model has four degrees of freedom. Steady-state  solutions of the model are determined iteratively using an adapted Newton  scheme. The model enables the calculation of all tire velocities and tire  forces for a given cornering situation characterized by the cornering radius  rho and the vehicle velocity v. This project has received funding from the European Unions Horizon 2020  research and innovation programme under grant agreement No. 713771  (EVERLASTING).",,
39f7a734-a1bc-410b-9f6a-65e6f616dbf0,4tu-cluster-analysis-of-temporal-patterns-of-travel-production,Cluster Analysis of Temporal Patterns of Travel Production,"This Dataset contains the code associated with the research paper titled ""A Cluster Analysis of Temporal Patterns of Travel Production in the Netherlands: Dominant within-day and day-to-day patterns and their association with Urbanization Levels"". The paper is published in the EJTIR. This code repo is intended to perform analysis on the temporal patterns of travel demand in the Netherlands. The data source needed for running this code can be found under the name ""The input data associated with the publication: A Cluster Analysis of Temporal Patterns of Travel Production in the Netherlands: Dominant within-day and day-to-day patterns and their association with Urbanization Levels"".",https://data.4tu.nl/v3/datasets/eb48bba3-04c5-45a2-9a5a-5ec0c656398c.git,
39f8304d-09e1-4e07-a755-369d54d53537,4tu-dataset-maximumentropy-qbmm-framework,Dataset-MaximumEntropy_QBMM_Framework,"Population balance methods utilised in multiphase flow simulations mark a significant advancement in computational fluid dynamics. However, existing approaches exhibit shortcomings, such as being prone to inaccuracies or being computationally prohibitive. Addressing these challenges, a recent innovation in closure for the method of moments is the introduction of quadrature based moments methods (QBMM). Discretising a distribution by a number of discrete elements, QBMM facilitate efficient and accurate tracking of density distributions, particularly for particle size distributions (PSD). However, obtaining the full particle size distribution information using these methods requires reconstructing the distribution from a finite set of moments, which is not a trivial step.  This study introduces a novel integration of the maximum entropy reconstruction (MER) into QBMM, establishing a robust and rapid framework for the time evolution and reconstruction of PSDs. As proof of concept for this framework, we focus on the direct quadrature method of moments (DQMOM) with spatially homogeneous and monovariate distributions. We show that coupling of MER with DQMOM has numerous advantages. To verify the framework, special cases of constant growth, aggregation, and breakage are considered for which analytical solutions can be found.  Furthermore, we show the advantage of using DQMOM with volume-based over length-based distributions, and address numerical as well as theoretical issues. Validation of the framework is successfully conducted on the evolution of the PSD from a twin-screw wet granulation dataset, considering all primary physical mechanisms inherent in a wet granulation process, namely growth, aggregation, and breakage. This showcases the consistency of the proposed framework and underscores its applicability to real-world scenarios.  The software consists of a fully working MATLAB (Version R2023a) script for a non-dimensional and dimensional direct quadrature method of moments coupled to a maximum entropy reconstruction. Additional methods that are needed to run the quadrature method of moments (reading in data, adaptive Wheeler algorithm, kernels, etc.) and the maximum entropy reconstruction (Gaussian qudrature, Cholesky inversion, etc.) including input data consisting of different density distributions from a twin-screw wet granulation dataset are available. The input data was derived from the dataset by Plath et al. 2021 (https://doi.org/10.4121/14248433.v1).",https://data.4tu.nl/v3/datasets/ff0b483b-be58-4db5-b9fe-c51c494bc188.git,
39fe12a8-2169-4f36-b293-b6de4fb7c7a0,nature-counts,Nature Counts,"# What is NatureCounts  NatureCounts is the interactive data portal for Birds Canadaâ€™s National Data Centre. NatureCounts allows people interested in birds, whether for conservation, research or their personal enjoyment, to interact with one of the largest datasets on birds anywhere in the world.  NatureCounts is bringing together observation data on birds and other biodiversity including data from:  * bird-monitoring * bird-banding * broad-scale citizen-based bird-surveillance programs.  NatureCounts is a portal of the [Avian Knowledge Network](https://avianknowledge.net/) (AKN). While the focus of NatureCounts is primarily on Canadian birds, you will also find other data from projects coordinated by Birds Canada and its partners, including data on other taxonomic groups (e.g. the Trinational Monarch Knowledge Network portal), and some ranging to other regions (mainly within North America). Some of the same data are available through other AKN nodes, but others are only available from this site. You can access a complete list of all datasets available for download from NatureCounts [here](https://naturecounts.ca/nc/default/datasets.jsp), and you can use our [data request page](https://naturecounts.ca/nc/default/searchquery.jsp) to submit a request. You can also consult a list of other [AKN partners](https://avianknowledge.net/?page=nodes) where you can access additional data.  One of the core strengths of NatureCounts is an extensive database that combines data from hundreds of different sources, all using a common standard defined by the [Avian Knowledge Network](https://avianknowledge.net/), called the [Bird Monitoring Data Exchange](https://naturecounts.ca/nc/default/nc_bmde.jsp) (BMDE). This format makes it easy to combine datasets and removes some of the usual barriers related to accessibility and formats, rapid discovery, access, and analysis of these vast resources.  The goals of NatureCounts and the Avian Knowledge Network are to:  * educate the public on the dynamics of bird populations * provide interactive decision-making tools for land managers * make available a data resource for scientific research * advance new exploratory analysis techniques to study bird populations.  **NatureCounts** is a node of the Avian Knowledge Network, and aims to provide access to bird monitoring data and facilate the understanding of the patterns and dynamics of bird populations across the Western Hemisphere.  # What data are included in NatureCounts  There are literally hundreds of datasets available in NatureCounts, representing well over 100 million occurrences of birds. These include virtually all of Birds Canadaâ€™s monitoring initiatives, such as Breeding Bird Atlases, eBird Canada, the Marsh Monitoring Program, Nocturnal Owl Surveys, as well as many other major initiatives to which BSC contributes in some capacity (Canadian Migration Monitoring Program, Hawk Count and the Raptor Population Index, the Trinational Monarch Knowledge Network, the Breeding Bird Survey, the Christmas Bird Count, etc.).  While several of the datasets are primarily managed by our partners, such as the Cornell Lab of Ornithology, National Audubon Society, Environment Canada, etc., others are managed Birds Canada through one of our data entry tools within NatureCounts.  It should also be mentioned that one of the other resources managed by Birds Canadaâ€™s National Data Centre, but currently not available through NatureCounts, is the [Motus Wildlife Tracking System}(https://motus.org/), a network of automated telemetry stations allowing us to track individual birds fitted with radio transmitters as they move through the landscape, and feeding vast amounts of data into BSCâ€™s national databases.  # What can you do with the data?  * You can explore the data resources of NatureCounts via interactive maps that allow you to view the distribution of bird populations any time of the year and across a variety of different sources. * You can obtain and visualize information on long term [population trends](https://naturecounts.ca/nc/default/popindices.jsp) of Canadian birds. * You can explore an individual species pattern of distribution or the species richness of a particular location via dynamically generated [summary tables and graphs](https://naturecounts.ca/nc/default/datasummaries.jsp). * Finally, much of the observational data can be directly [accessed and downloaded](https://naturecounts.ca/nc/default/searchquery.jsp)  # How are NatureCounts data being used?  NatureCounts has provided mission critical data to thousands of users, for a wide range of applications, including academic research, student projects and a diverse array of conservation uses (COSEWIC status reports, consultant industries, etc.). We are always looking for examples of how NatureCounts data are being used. If you have published manuscripts or reports using bird monitoring data made available by NatureCounts, or developed a data visualization or analysis, [please share with us](https://naturecounts.ca/nc/default/contact.jsp)!  NatureCounts data also serve as the basis for tracking bird status within Canadian [Important Bird and Biodiversity Areas](https://ibacanada.org/) (IBA). The species status and summaries you will find available within the IBA web site are directly generated from the NatureCounts database.  # Want to contribute?  If you are a participant in any of the monitoring initiatives already included in NatureCounts, congratulations, you are already contributing to NatureCounts and the Avian Knowledge Network! For instance, simply by submitting your bird observations to any of the [Canadian Breeding Bird Atlases](https://www.birdscanada.org/volunteer/atlas/index.jsp?lang=EN), [eBird Canada](https://ebird.org/canada/home) or [eButterfly](https://www.e-butterfly.org/), you are participating in this important initiative and contributing to our global knowledge on bird populations.  Birds Canada is also interested in hosting additional bird observational data, particularly those within Canada. These may help our ongoing monitoring efforts (e.g. atlases) or be made available to the thousands of NatureCounts users. If you are interested in contributing data to NatureCounts or if you have any questions, please feel free to [contact us](https://naturecounts.ca/nc/default/contact.jsp).",https://github.com/BirdsCanada/naturecounts,BirdsCanada/naturecounts
3a01a72c-a555-4989-af09-4555904aeb63,overture-ego,Overture Ego,"Ego provides single sign-on through Facebook, Google and Github, as well as providing an intuitive GUI for painless user management.   Ego is a vital service within the Overture research software ecosystem. With our genomics data management solutions, scientists can significantly improve the lifecycle of their data and the quality of their research. See our related products for more information on what Overture can offer.",https://github.com/overture-stack/ego,overture-stack/ego
3aa7f13f-0c5c-4f8b-b5d8-3f7e5b318036,bestietemplatejl,BestieTemplate.jl,,https://github.com/JuliaBesties/BestieTemplate.jl,JuliaBesties/BestieTemplate.jl
3aae02ce-887b-4330-a8fb-c4fb6df0b825,4tu-software-and-code-for-the-research-project-interdependence-and-communication-style-in-human-agent-robot-teamwork,Software and code for the research project: Interdependence and Communication Style in Human-Agent/Robot Teamwork,This repository was used to conduct an experiment with the aim of studying the effects of interdependence and robot communication style on human-robot teamwork. Human participants were presented with a virtual (urban search and rescue inspired) task and robot. Robot communication style varied between participants while interdependence between human and robot varied within participants. The goal for the human-robot team was to search and rescue all victims as soon as possible. We wanted to gain insights into the following:   - How does robot communication style influence human-robot teamwork?   - How does interdependence between human and robot influence human-robot teamwork?   - How does interdependence influence the relationship between robot communication style and human-robot teamwork?,,
3ae1e588-2d54-4b8c-a520-e85618dc7072,opendarts,open-DARTS,"OpenDARTS has a hybrid design combining C++ and Python code. It utilizes advanced numerical methods such as fully implicit thermo-hydro-mechanical-chemical formulation, highly flexible finite-volume spatial approximation, operator-based linearization for nonlinear and physics-based preconditioning for linear solutions.",https://gitlab.com/open-darts/open-darts,
3af59f28-8158-4832-aa16-15b2d930119d,marzipan,marzipan,"* Provides an easily configurable framework to configure and deploy clusters of VMs on 'bare metal' managed by OpenNebula and deploy services on them  * (Re-)deploy defined VM+software configurations with a single command  * Scale, instantiate, and tear down clusters of arbitrary size using a simple config file  * Interface with `Emma` to deploy fully customisable ansible scripts  * supports cluster wide service management  Automated instantiation and deployment of (clusters of) virtual machine(s) on bare metal using the OpenNebula platform, as well as subsequent provisioning and deployment of services incl., e.g. Dask.  marzipan consists of the core marzipan.py python module providing a high level interface to the OpenNebula cloud, as well as an accompanying Docker framework and configurable deployment scripts providing a fully automated instantiation and provisioning environment.  For provisioning marzipan makes use of the emma_marzipan fork ansible playbooks.  marzipan is based off and strongly draws from Lokum, but is updated to make use of current versions of Ansible as well as python 3, and circumvents recurrent synchronicity and timeout issues arsing from the interplay of terraform, the runtastic OpenNebula provider for terraform, and various (legacy) OpenNebula versions.  marzipan has been tested on the SURFsara HPC cloud, but should work for any OpenNebula platform.",https://github.com/NLeSC-GO-common-infrastructure/marzipan,NLeSC-GO-common-infrastructure/marzipan
3b049f77-cb99-4262-9166-4d01ad1c4610,turing-data-safe-haven,Turing Data Safe Haven,"Many of the important questions we want to answer for society require the use of sensitive data. In order to effectively answer these questions, we need Trusted Research Environments (TRE) to analyse that data.  The Turing Data Safe Haven software incudes a set of infrastructure-as-code tools which will allow anyone to deploy their own TRE on MS Azure.  To get started, checkout our [documentation](https://data-safe-haven.readthedocs.io/en/latest/index.html) and [GitHub repo](https://github.com/alan-turing-institute/data-safe-haven).",https://github.com/alan-turing-institute/data-safe-haven,alan-turing-institute/data-safe-haven
3b393bb8-ff2e-42ba-86ff-8f2f782fff0e,4tu-code-for-publication-a-general-partitioning-strategy-for-non-centralized-control,Code for publication: A general partitioning strategy for non-centralized control,"Partitioning is a fundamental challenge for non-centralized control of large-scale systems, such as hierarchical, decentralized, distributed, and coalitional strategies. The problem consists of finding a decomposition of a network of dynamical systems into system units for which local controllers can be designed. Unfortunately, despite its critical role, a generalized approach to partitioning applicable to every system is still missing from the literature. This paper introduces a novel partitioning framework that integrates an algorithmic selection of fundamental system units (FSUs), considered indivisible entities, with an aggregative procedure, either algorithmic or optimization-based, to select composite system units (CSUs) made of several FSUs. A key contribution is the introduction of a global network metric, the partition index, which quantitatively balances intra- and inter-CSU interactions, with a granularity parameter accounting for the size of CSUs, allowing for their selection at different levels of aggregation. The proposed method is validated through case studies in distributed model predictive control (DMPC) for linear and hybrid systems, showing significant reductions in computation time and cost while maintaining or improving control performance w.r.t. conventional strategies.",https://data.4tu.nl/v3/datasets/2246b13d-f3db-49ee-b9ab-264eb5584bba.git,
3b54652a-6c64-4a92-b447-e7de7689fd47,litstudy,LitStudy,"![Logo](https://raw.githubusercontent.com/nlesc/litstudy/master/docs/logo.png)  LitStudy is a Python package that enables analysis of **scientific literature** from the comfort of a Jupyter notebook. It provides the ability to select scientific publications and study their metadata through the use of visualizations, network analysis, and natural language processing.  In essence, this package offers five main features:  * Extract metadata from scientific documents from various **data sources**. The data is presented in a standardized interface, allowing for the combination of data from different sources. * **Filter**, **select**, **deduplicate**, and **annotate** collections of documents. * Compute and plot general **statistics** for document sets, such as statistics on authors, venues, and publication years. * Generate and plot various **bibliographic networks** as interactive visualizations. * Topic discovery using **natural language processing** (NLP) allows for the automatic discovery of popular topics.",https://github.com/NLeSC/litstudy/,NLeSC/litstudy
3b69589a-0792-4308-a9f9-6fb99be77b6b,taxotagger,TaxoTagger,"[![pypi badge](https://img.shields.io/pypi/v/taxotagger.svg?color=blue)](https://pypi.python.org/project/taxotagger/) [![Static Badge](https://img.shields.io/badge/ðŸ„_Docs_ðŸ„-826644)](https://mycoai.github.io/taxotagger)[](https://mycoai.github.io/taxotagger)  TaxoTagger is a Python library for DNA barcode identification, powered by semantic searching.  Features: - ðŸš€ Effortlessly build vector databases from DNA sequences (FASTA files) - âš¡  Achieve highly efficient and accurate semantic searching - ðŸ”¥ Easily extend support for various embedding models   ## Installation  TaxoTagger requires Python 3.10 or later.  ```bash # create an virtual environment conda create -n venv-3.10 python=3.10 conda activate venv-3.10  # install the `taxotagger` package pip install --pre taxotagger ```   ## Usage  ### Build a vector database from a FASTA file  ```python from taxotagger import ProjectConfig from taxotagger import TaxoTagger  config = ProjectConfig() tt = TaxoTagger(config)  # creating the database will take ~30s tt.create_db('data/database.fasta') ```  The `data/database.fasta` is available at repo [data folder](https://github.com/MycoAI/taxotagger/tree/main/data).  By default,  the `~/.cache/mycoai` folder is used to store the vector database and the embedding model. The [`MycoAI-CNN.pt`](https://zenodo.org/records/10904344) model is automatically downloaded to this folder if it is not there, and the vector database is created and named after the model.    ### Conduct a semantic search with FASTA file ```python from taxotagger import ProjectConfig from taxotagger import TaxoTagger  config = ProjectConfig() tt = TaxoTagger(config)  # semantic search and return the top 1 result for each query sequence res = tt.search('data/query.fasta', limit = 1) ```  The [`data/query.fasta` file](https://github.com/MycoAI/taxotagger/tree/main/data) contains two query sequences: `KY106088` and `KY106087`.   The search results `res` will be a dictionary with taxonomic level names as keys and matched results as values for each of the two query sequences. For example, `res['phylum']` will look like:  ```python [     [{""id"": ""KY106088"", ""distance"": 1.0, ""entity"": {""phylum"": ""Ascomycota""}}],     [{""id"": ""KY106087"", ""distance"": 0.9999998807907104, ""entity"": {""phylum"": ""Ascomycota""}}] ] ```  The first inner list is the top results for the first query sequence, and the second inner list is the top results for the second query sequence.  The `id` field is the sequence ID of the matched sequence. The `distance` field is the cosine similarity between the query sequence and the matched sequence with a value between 0 and 1, the closer to 1, the more similar. The `entity` field is the taxonomic information of the matched sequence.   We can see that the top 1 results for both query sequences are exactly themselves. This is because the query sequences are also in the database. You can try with different query sequences to see the search results.   ## Docs Please visit the [official documentation](https://mycoai.github.io/taxotagger) for more details.  ## Question and feedback Please submit [an issue](https://github.com/MycoAI/taxotagger/issues) if you have any question or feedback.",https://github.com/MycoAI/taxotagger,MycoAI/taxotagger
3b89becf-77c9-4570-879c-4b5e606e1df8,4tu-megawes-matlab-simulink-model-underlying-the-publication-six-degrees-of-freedom-simulation-model-for-multi-megawatt-airborne-wind-energy-systems,[MegAWES] Matlab/Simulink model underlying the publication: Six Degrees-of-Freedom Simulation Model for Multi-Megawatt Airborne Wind Energy Systems,"MegAWES is a Matlab/Simulink model of an airborne wind enrgy (AWE) system based on a tethered rigid wing that is operated in pumping cycles producing multiple megawatt of electricity. The framework is a further development of the graduation project of Dylan Eijkelhof which was jointly supervised by TU Delft, ETH Zurich and DTU. The ultimate purpose is to provide a reference model of a megawatt-range AWE system and a computational framework to simulate its operation. The simulink framework includes the following model components:  Pre-calculated look-up tables for aircraft's aerodynamic behaviour.Segmented tether with a single attachment point at the kite's centre of gravity.Choice between 3DoF point-mass and 6DoF rigid-body dynamic solver.Aircraft controller for power generation flight controls and path tracking.Set-force controlled dynamic winch.",,
3b9abb62-ccb9-499a-add2-a03b4a336e2f,xtas,xtas,"* easy access to numerous text processing and analysis tools * full support for Dutch and English * can use Elasticsearch for document storage * can be run as a service  xtas is a collection of natural language processing and text mining tools, brought together in a single software package with built-in distributed computing and support for the Elasticsearch document store.  xtas functionality consists partly of wrappers for existing packages, with automatic installation of software and data; and partly of custom-built modules coming out of research. Currently offered are various parsers for Dutch and English (Alpino, CoreNLP, Frog, Semafor), named entity recognizers (Frog, Stanford and custom-built ones), a temporal expression tagger (Heideltime) and a sentiment tagger based on SentiWords.  A basic installation of xtas works like a Python module. Built-in package management and a simple, uniform interface take away the hassle of installing, configuring and using many existing NLP tools.  xtasâ€™s open architecture makes it possible to include custom code, run this in a distributed fashion and have it communicate with Elasticsearch to provide document storage and retrieval.",https://github.com/NLeSC/xtas,NLeSC/xtas
3bdb523a-568a-4763-89a0-396ab15d9744,4tu-programs-to-evaluate-superoptimizer-stoke,Programs to evaluate superoptimizer STOKE.,"STOKE is one of the Superoptimizers which are programs that given a function and a set of instructions of a processor, traverse through a space of programs that compute a given function and try to find the optimal usually in terms of execution speed or size of the binary. Authors of STOKE make some extraordinary claims. They suggest that it is able to produce programs that are multiple times faster than programs without any optimization, and programs which are at least as efficient as programs produced by gcc -O3 and sometimes expert handwritten assembly. The goal of this paper is to check these claims. In this paper classes of programs that STOKE may handle particularly well and any class of programs that stochastic optimization might not be able to handle will be identified. We conclude from the experiments described in this paper that STOKE is able to fulfill that statement in some cases. The searching algorithm of STOKE is not always able to find programs that are at least as efficient as programs optimized by gcc -O3. STOKE works particularly well in programs where a lot consecutive logical and mathematical operations are calculating (e.g. counting bits). It is often not that successful with programs containing loops where it sometimes canâ€™t find a solution at all.",,
3c1d20bb-bceb-4a00-85b3-4c6dc52cd5d8,4tu-code-underlying-the-publication-forecasting-estuarine-salt-intrusion-in-the-rhine-meuse-delta-using-an-lstm-model,Code underlying the publication: Forecasting estuarine salt intrusion in the Rhine-Meuse delta using an LSTM model,"&nbsp;  Machine learning model for predicting salt concentrations in the Rhine-Meuse delta.  The folder 'Data'&nbsp;contains processed data, identical to 'Features.csv'&nbsp;in the raw dataset.  The folder 'Models' contains an ensemble of LSTM models created with the script 'LSTMv1.py'.  The script 'preprocessing.py' was used to convert the raw data to the daily data in 'Features.csv'.",,
3c774795-9c53-4228-b724-1b5157d44206,4tu-ceconv,CEConv,"Code corresponding to NeurIPS 2023 conference paper ""Color Equivariant Convolutional Networks"".     Abstract  Color is a crucial visual cue readily exploited by Convolutional Neural Networks (CNNs) for object recognition. However, CNNs struggle if there is data imbalance between color variations introduced by accidental recording conditions. Color invariance addresses this issue but does so at the cost of removing all color information, which sacrifices discriminative power. In this paper, we propose Color Equivariant Convolutions (CEConvs), a novel deep learning building block that enables shape feature sharing across the color spectrum while retaining important color information. We extend the notion of equivariance from geometric to photometric transformations by incorporating parameter sharing over hue-shifts in a neural network. We demonstrate the benefits of CEConvs in terms of downstream performance to various tasks and improved robustness to color changes, including train-test distribution shifts. Our approach can be seamlessly integrated into existing architectures, such as ResNets, and offers a promising solution for addressing color-based domain shifts in CNNs.",https://data.4tu.nl/v3/datasets/2ab28fae-8cbf-4640-8da3-c284e89dc06c.git,
3d34d2cf-6f1e-4c94-ab6b-bee60ea4f24d,swiftmhc,swiftmhc,,https://github.com/X-lab-3D/swiftmhc,X-lab-3D/swiftmhc
3d74fd96-4a0a-480c-b5cc-0dcf7df9157d,comparems2,compareMS2,"compareMS2 calculates the similarity between tandem mass spectrometry datasets and visualizes the results in a phylogenetic tree.  * Only mass spectrometry data is needed * Relative distances between samples immediately become clear * A Graphical User Interface simplifies usage * Optionally, sample files can be mapped to species names * Phylogenetic trees can be saved in several graphics file formats for direct use in publications * Input consists of MS2 data in MGF format * Output file formats are compatible with common phylogenetic tree software (for instance [MEGA](https://www.megasoftware.net/)) for further processing",https://github.com/524D/compareMS2,524D/compareMS2
3daaec1d-c7a8-43ef-aad8-d1da1aca4e11,newsgac-platform,NEWSGAC platform,"NEWSGAC was a research project which aimed at transparent automatic classification of genres of newspaper articles. The project was a cooperation between the University of Groningen, the Amsterdam Center for Mathematics and Computer Science and the Netherlands eScience Center.  In the project, we developed an online platform for applying machine learning models to text data, with the opportunity to closely analyze the performance of the models.",https://github.com/newsgac/platform,newsgac/platform
3dbf20a8-b244-403b-9419-2845c1d6097a,pyxenon,PyXenon,- Provide easy access to compute and storage resources - A Python interface for Xenon,https://github.com/xenon-middleware/pyxenon,xenon-middleware/pyxenon
3df5256f-fc65-41ff-84c6-6bfdaf5e197c,open-computational-multiphysics,Open Computational Multiphysics ,"**OpenCMP** is a computational multiphysics software package based on the finite element method. It is primarily intended for physicochemical processes involving significant convective flow. OpenCMP uses the NGSolve finite element library for spatial discretization and provides a configuration file-based interface to pre-implemented models and time discretization schemes. It provides built-in post-processing and error analysis and also integrates with Netgen, Gmsh, and ParaView for meshing and visualization of simulation results.  OpenCMP development follows the principles of ease of use, performance, and extensibility. The configuration file-based user interface is intended to be concise, readable, and intuitive. Similarly, the code base is structured such that experienced users can add their own models with minimal modifications to existing code. Inclusion of the finite element method enables the use of high-order polynomial interpolants for increased simulation accuracy. OpenCMP also offers the discontinuous Galerkin method which is locally conservative and improves simulation stability for convection-dominated problems. Finally, OpenCMP implements the diffuse interface method, a form of immersed boundary method which allows the use of non-conforming structured meshes for even complex simulation domains to improve simulation stability and sometimes speed.  Examples and tutorial files can be found in the ""Examples"" directory. For more information on how to use and contribute to OpenCMP visit our [website](https://opencmp.io/).",https://github.com/uw-comphys/opencmp,uw-comphys/opencmp
3df87ae8-77d8-4d69-a064-0d0f301c7605,mixed-meal-model-sbml,Mixed Meal Model SBML,"This software generates the [SBML](https://sbml.org/) version of the [Mixed Meal Model](https://www.cell.com/iscience/fulltext/S2589-0042(22)01478-X?_returnURL=https%3A%2F%2Flinkinghub.elsevier.com%2Fretrieve%2Fpii%2FS258900422201478X%3Fshowall%3Dtrue), for predicting the glucose level evolution after the intake of a meal.   It is possible to generate the model and run the simulations on [this webpage](https://mixedmealmodel.win.tue.nl/) where the software is deployed.  A Julia implementation of the model can be found [here](https://github.com/max-de-rooij/MealModel.jl).",https://github.com/Computational-Biology-TUe/mixed_meal_model_sbml,Computational-Biology-TUe/mixed_meal_model_sbml
3e016f6d-6dd3-4ceb-88e6-af118afb1a9c,4tu-code-supporting-the-paper-high-speed-predictions-of-post-burn-contraction-using-a-neural-network-trained-on-2d-finite-element-simulations,Code supporting the paper: High-speed predictions of post-burn contraction using a neural network trained on 2D-finite element simulations,"This online resource shows three archived folders: Matlab, Python, and App that contain relevant code and data for the article: High-speed predictions of post-burn contraction using a neural network trained on 2D-finite element simulations.Â        Within the Matlab folder, one finds the codes used for the generation of the large dataset. Here, the file Main.m is the main file and from there, one can run the Monte Carlo simulation.       Within the Python folder, one finds the codes used for training the neural networks and creating the online application. The file Data.mat contains the data generated by the Matlab Monte Carlo simulation. The files run_bound.py, run_rsa.py, and run_tse.py train the neural networks, of which the best scoring ones are saved in the folder Training. The DashApp folder contains the code for the creation of the Application.       Within the App folder, one finds the executable nn_R2_app.exe that one can run, once the archived folder is unzipped. When running the app, it opens in a browser. This was checked in Windows.",,
3e10d008-921d-4368-907c-daec0b5c7a48,flame-gpu,FLAME GPU,* Provides the ability for agent based modellers to focus on specifying agent behaviour and run simulations without explicit understanding of CUDA programming or GPU optimisation strategies. * Simulation performance of an agent based model is significantly increased in comparison with desktop CPU alternatives. This allows simulation of far larger model sizes (or ensemble experiments) with high performance at a fraction of the compute/energy cost of grid based alternatives. * Massive agent populations can be visualised in real time as agent data is already located on the GPU hardware. * Utilise Python to specify models.,https://github.com/FLAMEGPU,
3e2d0f89-a907-4a12-b30d-0e30c7278003,h2-scenario-preprocessing,H2 scenario preprocessing,,https://github.com/ESI-FAR/h2-scenario-preprocessing,ESI-FAR/h2-scenario-preprocessing
3e3800ed-5707-4ea1-b5b5-8dd9a55a3849,4tu-tworit,TWORIT,"This tool is about calculating the S parameters and Far Fields of a cascaded cylindrical waveguide structure or a conical waveguide of arbitrary cross-sections. It is based on the Mode Matching Technique. The tool also contains the optimization of arbitrarily profiled smooth conical horn antennas based on a desired reflection coefficient. The same code can be modified to add certain other optimization goals, such as, the cross polarization levels and antenna efficiencies.     Special features:  It is faster than the conventional EM solvers because it is completely analytical.Based on the input frequency range by the user, it does a smart selection of modes per waveguide to reduce computational complexity and to get more accurate results.More details can be found in the README.pdf file.",https://data.4tu.nl/v3/datasets/bf7a0d71-636f-4156-8fdd-bec07ecbc294.git,
3e3f35f2-9b6d-4454-93bc-a68af36a7b80,4tu-online-cop-heuristic-learning-code-online-learning-of-variable-ordering-heuristics-for-constraint-optimisation-problems,"Online COP heuristic learning code: ""Online Learning of Variable Ordering Heuristics for Constraint Optimisation Problems""",This repository contains the source code for the algorithm designed to learn on-the-fly (variable ordering) heuristics for constraint optimization problems (COPs). To apply heuristics to COPs the Geocde solver is used and adapted.       The corresponding paper is:       Online Learning of Variable Ordering Heuristics for Constraint Optimisation Problems   Floris Doolaard and Neil Yorkeâ€Smith   Annals of Mathematics and Artificial Intelligence   https://doi.org/10.1007/s10472-022-09816-z   published online 2022,,
3e86fee6-546f-4a79-8961-97c30549194b,4tu-tester-ghg-abatement-option-selection-tool,TESTER GHG abatement option selection Tool,"TESTER is a tool that can select an optimal combination of abatement options for a vessel in the concept design stage to stay below the selected limits at the lowest overall costs. These costs are split up between CAPEX, OPEX and Costs to Society (CTS). The tool also allows the user to prioritize which element is most important for him or his client. With the development of the tool the application is not restricted to the vessels within the NAVAIS project but could be easily extended to include any type of vessel and any new abatement technologies or regulations.",,
3eb0b7fa-5cf2-4899-bddc-37bd3fe87e92,arena-crowds,Arena-Crowds,TODO fill me with something proper,https://github.com/ArenA-Crowds/Crowds,ArenA-Crowds/Crowds
3ec00112-4abc-4f41-b9a8-0736550b8b21,rsd-ng,Research Software Directory,,https://github.com/research-software-directory/RSD-as-a-service,research-software-directory/RSD-as-a-service
3f830169-b2d8-4f4a-a8cd-3698d3f79966,arise-biocloud,ARISE biocloud,"The ARISE (Authoritative and Rapid Identification System for Essential biodiversity information) is a collaboration between several research institutes and universities supported by SURF and the eScience Center and part of the National Roadmap for Large-Scale Research Facilities. ARISE aims to develop a digital infrastructure for semi-automated species identification and biodiversity monitoring in the Netherlands. To achieve this goal, ARISE relies on a novel open data management architecture, known as the data lakehouse, which enables the integration and management of a wide variety of data types and metadata standards. The architecture is designed to support FAIR principles (Findable, Accessible, Interoperable, Reusable), ensuring that data and metadata are well-described, accessible, and usable by both humans and machines. The Netherlands eScience Center plays a crucial role in the project by providing expertise in software development, data management, and data analysis. Through their collaboration, ARISE and the eScience Center are working to create a sustainable, innovative and scalable data infrastructure that will benefit not only the scientific community but also policymakers and society as a whole.",https://gitlab.com/arise-biodiversity/biocloud,
3f8b9b08-092c-4b1e-8c2e-6c622da3f8cf,pypfilt,pypfilt,"If there is a system or process that can be:  * Described (modelled) with mathematical equations; and  * Measured repeatedly in some (noisy) way.  Then you can use pypfilt to estimate the state and/or parameters of this system.  The [Getting Started](https://pypfilt.readthedocs.io/en/latest/getting-started/index.html) guide introduces the *Lorenz-63* system of ordinary differential equations, which has chaotic solutions for some parameter values and initial states. This guide demonstrates:  * How to build a simulation model for the Lorenz-63 system;  * How to generate simulated observations from this model;  * How to fit the simulation model to observations; and  * How to generate forecasts that predict the future behaviour of the Lorenz-63 system.  The [How-to Guides](https://pypfilt.readthedocs.io/en/latest/how-to/index.html) cover a range of further topics, including how to generate forecasts with:  * Continuous-time Markov chain (CTMC) models;  * Discrete-time Markov chain (DTMC) models;  * Ordinary differential equation (ODE) models; and  * Stochastic differential equation (SDE) models.",https://bitbucket.org/robmoss/particle-filter-for-python/,
400e3f64-46e0-4d03-b46e-c008e6a13f2c,anonymouus,anonymoUUs,anonymoUUs is a Python package for replacing identifiable strings in multiple files and folders at once. It can be used to pseudonymise data files and therefore contributes to protecting personal data.,https://github.com/UtrechtUniversity/anonymouus,UtrechtUniversity/anonymouus
4061d197-9fe9-407e-ae1f-4fd032ffa1bc,4tu-scripts-and-data-accompanying-publication-manure-matters-prospects-for-regional-banana-livestock-integration-for-sustainable-intensification-in-south-west-uganda,"Scripts and data accompanying publication: ""Manure matters: Prospects for regional banana-livestock integration for sustainable intensification in South-West Uganda""","This repository contains the R scripts  and data used for the data analysis and modelling for the publication: Manure matters: Prospects for regional banana-livestock integration for sustainable intensification in South-West Uganda, by den Braber et al. in the International Journal for Agricultural Sustainability.",,
409fff1e-c5fe-4b46-b457-c3bccbe7ad69,4tu-cycle-tempo-automatisation,Cycle Tempo Automatisation,"A python script that runs cycle tempo simulations for different input values, speeding up the analysis of modelled systems in Cycle Tempo.",,
410e296d-a448-4c10-81e9-a1d247d7db67,clumped-processing,clumped-processing,,https://github.com/UtrechtUniversity/clumped-processing,UtrechtUniversity/clumped-processing
41222b5f-5d61-4d8a-a3a6-07b38a48af27,4tu-artefact-of-javabip-meets-vercors-towards-the-safety-of-concurrent-software-systems-in-java,Artefact of: JavaBIP meets VerCors: Towards the Safety of Concurrent Software Systems in Java,"This artefact contains an implementation of the Verified JavaBIP toolset as presented in the paper ""JavaBIP meets VerCors: Towards the Safety of Concurrent Software Systems in Java"", as well as the Casino case study discussed in the paper.     The artefact contains all binaries needed to evaluate the toolset, ready to be installed and run into the FASE'23 VM. For that goal there are instructions in the file README.pdf in the artefact. In addition, the artefact also contains scripts and instructions to rebuild the artefact. However, for this an active internet connection is necessary. Instructions for this are in the file AUTHORS_README.md. For the purpose of running the artefact, this author readme can be ignored.     To use, load zip into FASE'23 VM: https://doi.org/10.5281/zenodo.7446277",,
415ecd83-77a9-4d23-a253-c2e23de1096f,matchms,matchms,"- import mass spectrometry data - process mass spectrometry data - compute mass spectra similarities - perform queries on spectrometry data  Matchms is an open-access Python package to import, process, clean, and compare mass spectrometry data (MS/MS). It allows to implement and run an easy-to-follow, easy-to-reproduce workflow from raw mass spectra to pre- and post-processed spectral data. Spectral data can be imported from common formats such mzML, mzXML, msp, metabolomics-USI, MGF, or json (e.g. GNPS-syle json files). Matchms then provides filters for metadata cleaning and checking, as well as for basic peak filtering. Finally, matchms was build to import and apply different similarity measures to compare large amounts of spectra. This includes common Cosine scores, but can also easily be extended by custom measures.",https://github.com/matchms/matchms,matchms/matchms
41ff037d-686e-4879-b9c7-0dd0e0973c72,eecology-tracker-calendar,eEcology Tracker calendar,"* Ecologists studying bird behavior using the UvA-BiTS system can visualize bird behavior for several years and detect patterns like migration strategies * Shows daily statistics like distance traveled, number of measurements, maximum altitude as a heatmap in yearly calendars * Allows users to find outliers like measurement errors by choosing their preferred color range and dynamically clip data",https://github.com/NLeSC/eEcology-script-wrapper,NLeSC/eEcology-script-wrapper
4246c7e5-90e1-44c4-bc9f-9627344fa07b,4tu-source-code-for-incs-iterative-nonlinear-contrast-source,Source code for INCS (Iterative Nonlinear Contrast Source),"All the source files, libraries and bin files that ar3 necessary for compiling and running INCS. INCS is a solver of the 3D Westervelt equation in a 4D spatiotemporal domain. It is based on a Neumann iterative scheme. More information on publication below.",https://data.4tu.nl/v3/datasets/9b4547be-55f0-44e0-aa0f-341db719db17.git,
42e13718-113e-4ebe-8b2d-ff53e04ceea3,4tu-permission-based-verification-of-red-black-trees-and-their-merging-code,Permission-based Verification of Red-Black Trees and Their Merging - Code,"These files implement a red-black tree and an algorithm to concurrently merge such trees. The code is written in Java, and annotated with JML-style comments that allow the VerCors verifier to prove the correctness of e.g. inserting and deleting nodes in the tree, and of the merging algorithm.",,
4354614a-ba8f-44cd-a888-32c62814c9ad,4tu-data-software-underlying-the-publication-robust-quantum-network-memory-based-on-spin-qubits-in-isotopically-engineered-diamond,Data/software underlying the publication: Robust quantum-network memory based on spin qubits in isotopically engineered diamond,"Optical quantum networks can enable long-range quantum communication and modular quantum computation. A powerful approach is to use multi-qubit network nodes which provide the quantum memory and computational power to perform entanglement distillation, quantum error correction,and information processing. Nuclear spins associated with optically-active defects in diamond are promising qubits for this role. However, their dephasing during entanglement distribution across the optical network hinders scaling to larger systems. In this work, we show that a single 13C spin in isotopically engineered diamond offers a long-lived quantum memory that is robust to the optical link operation. The memory lifetime is improved by two orders-of-magnitude over the longest reported value, and exceeds the best reported times for making photonic entanglement. We identify ionisation of the NV center as a newly limiting decoherence mechanism. As a first step towards overcoming this limitation, we demonstrate that the nuclear spin state can be retrieved with high fidelity after a complete cycle of ionisation and recapture. Finally, we use numerical simulations to show that the combination of this improved memory lifetime with previously demonstrated entanglement links and gate operations can enable key primitives for quantum networks, such as deterministic nonlocal two-qubit logic operations and GHZ state creation across four network nodes. Our results pave the way for test-bed quantum networks capable of investigating complex algorithms and error correction.",,
437a0afd-5867-4e01-ad6b-f7fd9d771553,4tu-source-code-underlying-the-publication-topology-based-reconstruction-prevention-for-decentralised-learning,Source code underlying the publication: Topology-Based Reconstruction Prevention for Decentralised Learning,"MATLAB code to reproduce results presented in the paper ""Topology-Based Reconstruction Prevention for Decentralised Learning"".     The source code is available as a git repository. Cached computation results are available as additional files.     Git repository  The git repository has two parts:     * FeasibilitySim contains the source code of the experiments from Section 4.4. Its main script is FeasibilitySim.m.  * PerformanceSim contains the source code of the experiments from Section 5.5. Its main script is PerformanceSim.m.     The parts are described in more detail in the file README.md in their respective folders.     Detailed usage instructions for both parts are available in the file ARTIFACT-EVALUATION.md.     Files  Cached computation results are optionally available, with filenames ending in .cache.zip. These allow inspecting the exact outputs without the need to re-run the computationally intensive experiments. Detailed usage instructions are available in the file ARTIFACT-EVALUATION.md inside the git repository.",https://data.4tu.nl/v3/datasets/801681a5-4391-449b-9aa1-ba6c5313191a.git,
4386c1e7-8f7a-41a4-8a21-9e676eeb215b,zampy,zampy,,https://github.com/EcoExtreML/zampy,EcoExtreML/zampy
4386c350-c7da-4997-a847-12d7fc993862,cangeo-project,CanGeo Project,"The goal of this project is to provide a web service providing inland waterbodies initially available on GeoBase and updated using the most recent Landsat-8 imagery. The system ingests multispectral imagery (30 meters resolution) over Canada on a monthly basis (from May to September) and updates the waterbody outlines using an image processing software developed at CRIM. Updated waterbodies are available through a GeoRSS service or through a RESTful service in XML, KML, SHP or GeoJSON formats.",,
439fa6b5-7e5a-48cb-89d6-eb723b2d7a75,4tu-basic-code-for-simulations-of-reduced-gilad-model,Basic code for simulations of reduced Gilad model,Example of the Matlab code run for the simulations in the article The existence of localized vegetation patterns in a systematically reduced model for dryland vegetation. DOI: https://doi.org/10.1016/j.physd.2020.132637,,
43c160a6-14ae-4f25-baa1-9534383e734a,4tu-hanson-s-model-in-frequency-domain-tonal-noise-of-rotors-in-uniform-inflow,Hanson's Model in Frequency Domain - Tonal Noise of Rotors in Uniform Inflow,"This MATLAB code is based on Hanson's Paper titled ""Helicoidal Surface Theory for Harmonic Noise of Propellers in the Far Field"" with DOI https://doi.org/10.2514/3.50873 and is meant to be used for tonal noise prediction of rotors with uniform inflow with propeller axis aligned with the inflow. This code is based on the frequency domain formulation and is only valid for far-field noise predictions of thickness and loading noise.",,
445b3b0b-cbfd-41ca-ae6c-8a0556873372,eitprocessing,eitprocessing,This package can read electrical impedance tomography input and display images. The package can used in processing or pre-processing these images.,https://github.com/EIT-ALIVE/eitprocessing/,EIT-ALIVE/eitprocessing
445d7ed9-910a-4f9c-89c5-7f4e32d692f5,online-behaviour-machine-learning,Hercules,* support for machine learning experiments for researchers * data conversion for tweet data in project Automated Analysis of Online Behaviour on Social Media * handles data format used in this project (csv) * used for two scientific papers   In the project Automated Analysis of Online Behaviour on Social Media researchers wanted to find out what the intention was of tweets from politicians and political journalists. For this purpose they collected recent tweets from these two groups from Twitter and analysed these with machine learning software trained on older tweets which were labeled by humans. This software package contains the software for collecting and analyzing the tweets.,https://github.com/online-behaviour/machine-learning,online-behaviour/machine-learning
44eae1af-7e21-4e60-95d0-6aef07c9bb05,benchmarkrecovery,BenchmarkRecovery,"* Generates synthetic, but realistic time series simulating forest recovery after a fire * Tests several resilience indicators over the time series  The context of this project is the study of the recovery of tropical forests after an abrupt disturbance (typically a forest fire) using satellite images as a data source.  The speed of recovery after a disturbance is known to be correlated with the concept of resilience. This is true not only for forests, but for many dynamical systems. To put it simply: forests that recover fast are more resilient. Forests that recover slowly may be in danger of permanent disappearance.  The specialized literature proposes different metrics for measuring the recovery speed. The performance of these metrics depends on many factors. Some of them are natural, such as the intensity of the perturbation or the seasonality. Others are technical, such as the sampling frequency or the spatial resolution.",https://github.com/RETURN-project/BenchmarkRecovery,RETURN-project/BenchmarkRecovery
45000155-8450-403a-957c-8d65bb00801e,gridformat,GridFormat,,https://github.com/dglaeser/gridformat.git,dglaeser/gridformat
450f2f75-96a0-477d-a3a1-9b6709f3225c,ewatercycle-wflow,ewatercycle-wflow,"Generate wflow forcing and run the wflow hydrological model in a container using the eWaterCycle package.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/ewatercycle-wflow,eWaterCycle/ewatercycle-wflow
4540e3bc-7812-473b-8a15-733c5f51b7a7,hypercanny,HyperCanny,- Computes edges in large data sets - Works in any number of dimensions - Easily accessible from Python using a Cython wrapper. The Python interface is geared towards applications in climate science. - C++ and Python interfaces - The only (findable) implementation of Canny's algorithm in more than two dimensions,https://github.com/abrupt-climate/hyper-canny,abrupt-climate/hyper-canny
45543225-b18c-4ad6-baf4-4416428d2306,4tu-software-accompanying-paper-refinement-of-parallel-algorithms-down-to-llvm,Software accompanying paper: Refinement of Parallel Algorithms down to LLVM,"Software accompanying paper ""Peter Lammich: Refinement of Parallel Algorithms down to LLVM"" accepted for publication at LIPIcs, Volume 237, ITP 2022       Isabelle-LLVM Parallel is a verification framework for Isabelle/HOL that targets LLVM as backend. The main features are:      Shallowly embedded semantics of fragment of LLVM   Code generator, to export LLVM code   Generation of header files for interfacing the code from C/C++   Separation logic based VCG   Support for stepwise refinement based verification   Support for parallel programs",,
45721a12-cce6-4b6d-9e1d-f787227c8018,4tu-childsplayaccessibility-github-code-repository-spatial-analysis-underlying-the-conference-paper-easy-as-child-s-play-co-designing-a-network-based-metric-for-children-s-access-to-play-space,"ChildsPlayAccessibility (Github) code repository: spatial analysis underlying the conference paper ""Easy as childâ€™s play? Co-designing a network-based metric for childrenâ€™s access to play space""","Notebooks 3.1-4 allow you to calculate the child's play accessibility metric: calculating the ease with which children can reach places for play in the city without supervision. The child's play accessibility metric takes into account how children may play not only in playgrounds, but also in informal play space such as schoolyards and small greenspaces. In addition, it accounts for (perceived) barriers that may exist for children in the city, such as busy roads, railways, and large natural environments. We recommend complementing our child's play accessibility metric with field work and collecting knowledge of local people, to understand all local nuances that may affects children who wish to access places for play.  The child's play accessibility metric was co-designed with experts on the built environment and children's health. This code is developed for three cities in Europe (i.e., areas in Milan, Ljubljana, and Utrecht) but can be adapted to fit other geographical contexts.",,
4643717c-dc06-4e03-98e6-784dadb50ff6,gsa-report,GSA report,,https://github.com/Basvanstein/GSAreport,Basvanstein/GSAreport
4647732f-4da9-4d5a-9873-698cd338c0e5,4tu-code-supporting-the-paper-sensitivity-of-a-two-dimensional-biomorphoelastic-model-for-post-burn-contraction,Code supporting the paper: Sensitivity of a two-dimensional biomorphoelastic model for post-burn contraction,"This online resource shows an archived folder including Matlab files used for the paper Sensitivity of a twoâ€‘dimensional biomorphoelastic model for postâ€‘burn contraction. Within the folder, one finds two live scrips called Sensitivity_1.mlx and Sensitivity_2.mlx. These files perform the analyses for the original model and the model with different cell proliferation rates, respectively. The scripts use the subfolders Chemicals, KMG1.0 [for (re)meshing] and Mechanics. Within the scripts, there are comments explaining the code.",,
4680191b-3bd3-48cf-a010-bbd269c432d0,4tu-data-software-underlying-the-publication-enumerating-all-bilocal-clifford-distillation-protocols-through-symmetry-reduction,Data/software underlying the publication: Enumerating all bilocal Clifford distillation protocols through symmetry reduction,"Entanglement distillation is an essential building block in quantum communication protocols. Here, we study the class of near-term implementable distillation protocols that use bilocal Clifford operations followed by a single round of communication.  We introduce tools to enumerate and optimise over all protocols for up to n=5 (not necessarily equal) Bell-diagonal states using a commodity desktop computer. Furthermore, by exploiting the symmetries of the input states, we find all protocols for up to n=8 copies of a Werner state. For the latter case, we present circuits that achieve the highest fidelity with perfect operations and no decoherence. These circuits have modest depth and number of two-qubit gates. Our results are based on a correspondence between distillation protocols and double cosets of the symplectic group, and improve on previously known protocols.",,
4690230a-d60c-49c1-b786-eee4f8464170,2022-brughmans-002,Importing a Roman Transport network,"By Tom Brughmans First version: Summer 2018 This version created 01/09/2018 Netlogo version used: 6.0.1 Extension used: nw (pre-packaged with Netlogo 6.0.1) https://ccl.northwestern.edu/netlogo/6.0-BETA1/docs/nw.html   Tutorial document availabe as a PDF in the [netlogo_implementtion folder](netlogo_implementation/Netlogo_Roman-transport_v0.1.pdf)  Cite this tutorial as: Brughmans, T. (2018). Importing a Roman Transport network with Netlogo, Tutorial, https://archaeologicalnetworks.wordpress.com/resources/#transport  .   This tutorial provides an introduction to finding and assembling pre-existing code to quickly create complex models. It uses code and data linked to in the https://projectmercury.eu pages. We will create a Roman transport network by reusing existing code that draws on the open access ORBIS dataset (http://orbis.stanford.edu/), we will create alternative network structures by reusing existing code, and we will explore the impact these different network structures have in light of simple economic processes. This tutorial will reveal the importance of not reinventing the wheel, of searching for appropriate existing code and letting your model building be inspired by othersâ€™ previous work.  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |orbis.graphml|file|available under netlogo_implementation/ . A .graphml file consisting of nodes representing Roman settlements and junctions, and edges representing transport routes. From ORBIS https://purl.stanford.edu/mn425tz9757| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |Network topology|network|The set of nodes and links created using the ORBIS .graphml file, and corrected through the model's data-correction procedure.| |distance distribution to Rome|frequency distribution|The frequency distribution of the distance from all nodes to Rome| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Brughmans-002,
46c32b69-f7d8-43b3-995a-8728d2e53dfc,broadlineregionsjl,BroadLineRegions.jl,,https://github.com/kirklong/BroadLineRegions.jl,kirklong/BroadLineRegions.jl
46e95405-8c79-4564-9db4-3f703e4a7461,4tu-netlogo-model-urbanism-and-geographic-crises-a-micro-simulation-lens-on-beirut,"NetLogo model: ""Urbanism and Geographic Crises: A Micro-Simulation Lens on Beirut""","NetLogo model corresponding to the Urban Planning article ""Urbanism and Geographic Crises: A Micro-Simulation Lens on Beirut""",,
47312e01-695d-4f2c-b76d-66aaac3bb900,4tu-spincontrol-jl-a-numerical-package-written-in-julia,"SpinControl.jl, a numerical package written in Julia",A Julia package to simulate the open-system dynamics of a noisy spin qubit driven by customable quantum control protocols.,,
4741169d-6a1e-4ef7-aac1-098e3837248f,4tu-patient-interaction-detection,patient-interaction-detection,"Videos of about 40 gynaecological procedures were recorded from a distance in the Leiden University Medical Center.  The purpose was to identify differences in workflow between open, minimally invasive, and robot-assisted surgery.  This code repository was used to analyse human 2D poses extracted from the videos.  It looks at movement speed and position per individual to estimate whether they interact with the operating table or not.  Additionally, it contains code to measure interaction with the patient from Noldus annotation files.",https://data.4tu.nl/v3/datasets/e5a1880f-f153-4fd0-8335-db4df7e71d7f.git,
47997e46-62b2-4845-8245-1a01d89ecf18,lifelinescsv2cdf,LifelinesCSV2CDF,"Cohort studies play a crucial role in understanding the relationships between various factors and health outcomes over time. These studies collect extensive data from participants, including demographic information, clinical variables, lifestyle factors, and biomarkers. However, analyzing cohort study data often poses challenges, particularly when assessing variables across different time points. The data for the same variable is typically scattered across multiple files, each representing a specific assessment or follow-up visit. This fragmentation makes it difficult to perform comprehensive longitudinal analyses, or in the particular case of the MyDigiTwin project, to compute multiple points in time of the same variable to map it to standards like FHIR/MedMij.  This tool transforms data from the Lifelines cohort study in CSV (Comma-Separated Values) format into a format we called CDF/JSON (Cohort Data Format), which is already used by other data analysis tools in the MyDigiTwin project. A CDF format describes all the variables, and their values over time (i.e., each assessment), of an individual study participant. This format is particularly useful for the generation of FHIR/MedMij-compliant data (one of the aforementioned analysis tools).",https://github.com/MyDigiTwinNL/LifelinesCSV2CDF,MyDigiTwinNL/LifelinesCSV2CDF
479b13cb-b602-46ea-92ae-26c9b8e2057e,4tu-prediction-of-trip-production,Prediction of Trip Production,"This repository contains the codes associated with the research paper titled ""The Role of Spatial Features and Adjacency in Data-driven Short-term Prediction of Trip Production: An Exploratory Study in the Netherlands"". The paper is currently under review for publication in the IEEE Transactions on Intelligent Transportation Systems. This code repo is intended to perform analysis on the temporal patterns prediction of travel demand in the Netherlands. The data source needed for running this code can be found under the name ""&nbsp;The input data associated with the publication: The Role of Spatial Features and Adjacency in Data-driven Short-term Prediction of Trip Production: An Exploratory Study in the Netherlands"".",https://data.4tu.nl/v3/datasets/5d41f526-611d-4350-90ae-6d21a399aae7.git,
47bcb343-8d22-49c5-bca9-0b91f4c5fff6,htr-quality-classifier,htr-quality-classifier,,https://github.com/LAHTeR/htr-quality-classifier,LAHTeR/htr-quality-classifier
47dbd4ec-5d55-42e2-9d79-79d59aab63c6,4tu-acdc-opflow,ACDC-OpFlow,"Repository with Optimal Power Flow (OPF) library in Julia, Python, MATLAB, and C++ done as a part of the Harmony project (HARMONic stabilitY assessment of PE-penetrated power systems).  Hybrid AC/voltage source converter-based multi-terminal DC (VSC-MTDC) power grids play a crucial role in enabling long-distance power transmission and flexible interconnection between AC grids. To fully leverage the functional advantages of such systems, it is essential that they operate in or close to optimal power flow (OPF) conditions. To address this, ACDC-OpFlow is developed as an open-source and cross-language framework for solving AC/DC OPF problems. Its core innovation lies in a unified modeling structure that supports MATLAB, Python, Julia, and C++, with Gurobi used as a consistent solver backend. This framework is beginner-friendly and allows users to work in their preferred programming languages.",,
4830897c-d17e-4b55-a66d-26beed97b121,graphvizdotlangjl,GraphvizDotLang.jl,"Create Graphviz graphs straight from Julia. There exists a `Graphviz.jl` package that offers interop between Julia and the Graphviz C library. However, it seems that this package does not give us a nice interface to generate DOT language. The `GraphvizDotLang.jl` module is a much simpler approach. It gives a simple interface to build up graphs and then pass them through Graphviz for visualization.",https://github.com/jhidding/GraphvizDotLang.jl,jhidding/GraphvizDotLang.jl
4897bc12-0209-478e-8a57-ea726b59fb26,deeprank,DeepRank,"- Predefined atom-level and residue-level PPI feature types, e.g. atomic density, vdw energy, residue contacts, PSSM, etc. - Predefined target types, e.g. binary class, CAPRI categories, DockQ, RMSD, FNAT, etc. - Flexible definition of both new features and targets - 3D grid feature mapping - Efficient data storage in HDF5 format - Support both classification and regression (based on PyTorch)  DeepRank is a general, configurable deep learning framework for data mining protein-protein interactions (PPIs) using 3D convolutional neural networks (CNNs).  DeepRank contains useful APIs for pre-processing PPIs data, computing features and targets, as well as training and testing CNN models.  #### Features:  - Predefined atom-level and residue-level PPI feature types, e.g. atomic density, vdw energy, residue contacts, PSSM, etc. - Predefined target types, e.g. binary class, CAPRI categories, DockQ, RMSD, FNAT, etc. - Flexible definition of both new features and targets - 3D grid feature mapping - Efficient data storage in HDF5 format - Support both classification and regression (based on PyTorch)",https://github.com/DeepRank/deeprank,DeepRank/deeprank
48a30c14-31b2-4e8d-9f6e-f35a7d211eeb,mdstudio,MDStudio,"* Molecular dynamics workflow creation and execution for biochemists * Micro-service based workflow management system * Flexible, interactive workflow execution locally or on HPC * Support for ATB, Gromacs, PLANTS, Paradocks, and many other tools  MDStudio provides biochemistry researchers in science and industry with the easiest and most flexible solution to running  molecular dynamics-based workflows. It is community-developed open source software.",https://github.com/MD-Studio/MDStudio,MD-Studio/MDStudio
495e8ff5-ec59-4e88-9414-04f1812d3204,boatswain,boatswain,* Simplifies building each Docker container when you have a tree of dependencies,https://github.com/NLeSC/boatswain,NLeSC/boatswain
49e8874a-cc24-4a5d-8c77-9c4a02a8a4ae,sarxarray,SARXarray,# SarXarray  SarXarray is an Xarray extension to process coregistered Single Look Complex (SLC) image stacks acquired by Synthetic Aperture Radar (SAR). It utilizes Xarrayâ€™s support on labeled multi-dimensional datasets to stress the space-time character of the SLC SAR stack. It also takes the benefits from Dask to perform lazy evaluations of the operations.,https://github.com/MotionbyLearning/sarxarray,MotionbyLearning/sarxarray
49ef2b01-3f40-4490-bce1-3388a905753f,4tu-simulation-code-accompanying-the-publication-cohesin-supercoils-dna-during-loop-extrusion,"Simulation code accompanying the publication ""Cohesin supercoils DNA during loop extrusion""",Research objective: computationally determine the linking number of plasmids after SMC-mediated DNA loop extrusion with twist and topoisomerase-mediated Lk change  Type of research: Comparison of computational model to experimental bulk plasmid assays  Method of data collection: 1D Monte Carlo simulations  Type of data: custom python code (.py) and .json data file,,
4a1a5d9b-4204-40b3-8c11-45db38a10af9,mibibio,mibibio,"Methods implemented for comprehensive statistical, visual and meta-analysis of large, complex microbiome datasets derived from high-throughput sequencing (16S rRNA amplicon, shotgun metagenomics or metatranscriptomics). Most routines wrapâ€”or reproduce programmaticallyâ€”the functionality of the MicrobiomeAnalyst web suite and its underlying R packages, so that analyses can be executed reproducibly from the command line or in notebooks.",https://github.com/MiBiPreT/mibibio,MiBiPreT/mibibio
4a7283c1-222e-4dbd-9c21-59389615528b,clumpedr,clumpedr,,https://github.com/isoverse/clumpedr/,isoverse/clumpedr
4a7ee2cb-e950-487d-9c14-db5425780cdc,4tu-petschallenge,PETsChallenge,"Privacy-Preserving Feature Extraction for Detection of  Anomalous Financial Transactions     ------------------------------------------------------------------------     This repository holds the code written by the PPMLHuskies for the 2nd Place solution in the PETs Prize Challenge, Track A.     Description     The task is to predict probabilities for anomalous transactions, from a  synthetic database of international transactions, and several synthetic  databases of banking account information. We provide two solutions. One  solution, our centralized approach, found in `solution_centralized.py`,  uses the transactions database (PNS) and the banking database with no  privacy protections. The second solution, which provides robust privacy  gurantees outlined in our report, follows a federated architecture,  found in `solution_federated.py` and model.py. In this approach, PNS  data resides in one client, banking data is divided up accross other  clients, and an aggregator handles all the communication between any  clients. We have built in privacy protections so that clients and the  aggregator learn minimal information about each other, while engaging in  communication to detect anomalous transactions in PNS.     The way in which we conduct training and inference in both the  centralized and the federated architectures is fundamentally the same  (other than the privacy protections in the latter). Several new features  are engineered from the given PNS data. Then a model is trained on those  features from PNS. Next, during inference, a check is made to determine  if attributes from a PNS transaction match with the banking data, or if  the associated account in the banking data is flagged. If any of these  attributes are amiss, we give it a value of 1, and a 0 otherwise.  Lastly, we take the maximum of the inferred probabilities from the PNS  model, and the result from the Banking data validation, which is used as  our final prediction for the probability that the transaction is  anomalous.     The difference between the federated and centralized logic is that in  the federated set up, where there are one or multiple partitions of the  banking data across clients, is that the PNS client engages in a  cryptographic protocol based on homomorphic encryption with the banking  clients, routed through the aggregator, to perform feature extraction.  This protocol, to ensure privacy, and that PNS does not learn anything  from the banks beyond the set membership of a select few features, is  carried out over several rounds, r. r = 7 + n, where n is the number of  bank clients.",https://data.4tu.nl/v3/datasets/2851fb4d-9c4f-498d-9ade-446399e45e08.git,
4aa84be7-b25b-4595-bb65-45678bfadb84,4tu-lov3d,LOV3D,"LOV3D is a Matlab package for obtaining the tidal response of multi-layered viscoelastic self-gravitating bodies with lateral variations of interior properties. For a given interior structure and tidal load, the software solves the mass conservation, momentum conservation and Poisson equations and computes the tidal Love numbers. For more information please see the README file and the user manual in /Docs.     This repository contains version of the software used in various publications:   v1.0 Rovira-Navarro M, Matsuyama I, Berne A. 2024. A Spectral Method to Compute the Tides of Laterally-Heterogeneous Bodies. The Planetary Science Journal. doi:10.3847/PSJ/ad381f  v2.3  Rovira-Navarro M, Matsuyama I, Dirkx D, Berne A, Calliess D, Fayolle S. 2025. Prospects of Using Tidal Tomography to Constrain Ganymede's Interior. Geophysical Research Letters. doi: 10.1029/2025GL114708     Check https://github.com/mroviranavarro/LOV3D_multi for updates.",https://data.4tu.nl/v3/datasets/d6f460fe-0753-4985-8972-0ca6d6544d35.git,
4b028342-38c9-45e6-b0c8-c29485b613a3,bgc-viewer,BGC Viewer,,https://github.com/medema-group/bgc-viewer,medema-group/bgc-viewer
4b7786d4-76dc-4a78-848e-46c64a78e025,4tu-software-and-data-underlying-the-publication-faithful-model-explanations-through-energy-constrained-conformal-counterfactuals,Software and data underlying the publication: Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals,"Code and research results for our AAAI 2024 paper ""Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals"".     The research results include the complete outcomes of the main experiments presented in the paper for all datasets. For each dataset, the results include a file ending in ""_bmk.csv"" which includes the estimated counterfactual evaluation metrics for individuals samples grouped by generator, models and evaluation metrics. Files ending in "".jls"" are the corresponding serialized Julia objects. For more information on reproducibility, see the GH repository.",https://data.4tu.nl/v3/datasets/19fb0622-57e8-4087-8988-17891753553d.git,
4b899157-1a8a-4c55-bed8-df28d9aead34,swan,Swan,* Provides several workflows to predict molecular properties using deep learning * Handles automatically all the training in GPU or CPU * Uses smiles as input  Swan is a Python library build on top of Pytorch and Pytorch geometric that allow generating statistical models to predict molecular properties. It takes as input a YAML file describing what kind of statistical model should be generated together with the path to a CSV file containing both the smiles and the molecular properties (labels) to train the model. The model can be trained either on GPU or CPU. Once the model is trained the user can ask Swan to predict numerical properties for different smiles,https://github.com/nlesc-nano/swan,nlesc-nano/swan
4b8d867a-7abe-47a6-8f95-82eeecc8a111,4tu-soil-suitability-maps-for-agriculture-for-the-province-noord-brabant-netherlands-created-with-watervissionagriculture-and-scripts-to-generate-input-data-and-to-run-the-model-wwl-tabel,Soil suitability maps for agriculture for the province Noord-Brabant (Netherlands) created with WaterVissionAgriculture and scripts to generate input data and to run the model WWL-tabel,"Scripts to generate input data for the model framework WaterVision Agriculture, to run the model WWL-tabel, and resulting maps of soil suitability for agriculture in the province Noord-Brabant (Netherlands). Soil suitability is reported in the form of yield depression of several arable crops, pasture and tree crops. Maps are available for the period 1991-2020 and for a relatively wet and dry year.",,
4bb6cd1c-9d14-43a2-95d6-c459c362459e,openpstd,openPSTD,"openPSTD is the implementation of the Fourier pseudospectral time-domain (PSTD) method for computing the propagation of sound, geared towards applications in the built environment. Being a wave-based method, PSTD captures phenomena like diffraction, but maintains efficiency in processing time and memory usage as it allows to spatially sample close to the Nyquist criterion, thus keeping both the required spatial and temporal resolution coarse. In the implementation it has been opted to model the physical geometry as a composition of rectangular two-dimensional subdomains, hence initially restricting the implementation to orthogonal and two-dimensional situations. The strategy of using subdomains divides the problem domain into local subsets, which enables the simulation software to be built according to Object-Oriented Programming best practices and allows room for further computational parallelization. The software is built using the open source components, Blender, Numpy and Python. For accelerating the software, an option has been included to accelerate the calculations by a partial implementation of the code on the Graphical Processing Unit (GPU), which increases the throughput by up to fifteen times.",https://github.com/openPSTD/openPSTD,openPSTD/openPSTD
4be69b28-e95e-48c3-856c-503bf6813cb3,excel-to-fhir,Excel-to-fhir,"# EXCEL to FHIR  This typescript tool helps you manage FHIR resources in a spreadsheet (EXCEL).    This tool converts source data in the spreadsheet to FHIR resources in JSON in following steps: - convert format: read the spreadsheet and convert it to JSON - transform schema: transform the JSON using JSONATA to FHIR resources (JSON)  We use [JSONATA](https://jsonata.org/) to transform schemas with mapping rules defined in a `.jsonata` file.  **This package aims to transform GameBus Excel to FHIR R4 (ConceptMap) resources, for which the mapping rules have been defined in [this folder](jsonata).**  If there are changes on GameBus Excel schema (e.g. changes on sheets and colomns), the mapping rules must be updated accordingly. An example of the GameBus Excel file is provided as [gamebus.xlsx](gamebus.xlsx).  ## Requirements  - [Deno](https://docs.deno.com/runtime/manual/getting_started/installation)  If you have deno installed, it's a good practice to update it to lastest version using the command: `deno upgrade`.  ## Usage  ### Run the typescript package directly:  ```shell # run conversion deno run --allow-read --allow-write https://raw.githubusercontent.com/nwo-strap/excel-to-fhir/main/excel-to-fhir.ts -f example-excel.xlsx -s sheetname1 sheetname2 -j mapping1.jsonata mapping2.jsonata  # show help deno run --allow-read --allow-write https://raw.githubusercontent.com/nwo-strap/excel-to-fhir/main/excel-to-fhir.ts -h ```  ### Run the typescript package as a command line tool  If you want to install the package as CLI tool and run:  ```shell # install deno install --allow-read --allow-write https://raw.githubusercontent.com/nwo-strap/excel-to-fhir/main/excel-to-fhir.ts  # run conversion excel-to-fhir -f example-excel.xlsx -s sheetname1 sheetname2 -j mapping1.jsonata mapping2.jsonata  # show help excel-to-fhir --help ```  ### Run Deno tasks  You can define [Deno tasks](https://docs.deno.com/runtime/manual/tools/task_runner) in the [deno.json](deno.json) file with specific inputs and options.  In the file you can find the task `transform`, which is defined to convert [gamebus.xlsx](gamebus.xlsx) file to FHIR resources with all valid sheets (with all corresponding jsonata files). You can run the conversion with the following command:  ```shell # clone the repo git clone https://github.com/nwo-strap/excel-to-fhir.git cd excel-to-fhir  # run the task deno task transform ```",https://github.com/nwo-strap/excel-to-fhir,nwo-strap/excel-to-fhir
4c0ad9f6-4cb9-4160-9505-ce1cf3835245,4tu-jat-jarkus-analysis-toolbox,JAT - Jarkus Analysis Toolbox,"The Jarkus Analysis Toolbox (JAT) is a Python-based  open-source software, that can be used to analyze the Jarkus dataset.  The Jarkus dataset is one of the most elaborate coastal datasets in the world and consists of coastal profiles of the entire Dutch coast, spaced about 250-500 m apart, which have been measured yearly since 1965. The main purpose of the JAT is to provide stakeholders (e.g. scientists, engineers and coastal managers) with the techniques that are necessary to study the spatial and temporal variations in characteristic parameters like dune height, dune volume, dune foot, beach width and closure depth. Different available definitions for extracting these characteristic parameters were collected and implemented in the JAT.Documentation: https://jarkus-analysis-toolbox.readthedocs.io/",,
4c68bcdd-10b2-4fef-9eca-86b7063902b2,bartender,i-VRESSE bartender,"The bartender is a web service that offers to run command line applications for visitors. The application input should be a configuration file with links to data files in the same directory. After authenticating with your local account or social login like ORCID or GitHub, you can upload your configuration file and data files as an archive to the web service for submission. Once the application has been executed the output files can be browsed with a web browser.  Bartender can be configured to run applications on a Slurm batch scheduler, pilot job framework, the grid or in the cloud. Bartender will take care of moving the input and output files to the right place. To pick where an application should be run you can choose from a list of existing Python functions or supply your own.  Bartender can be used as the computational backend for a web application, the web application should guide visitors into the submission and show the results.",https://github.com/i-VRESSE/bartender,i-VRESSE/bartender
4ca59bd6-9a39-4bfa-99ce-07f927c5a80a,web-enabled-awareness-research-network-warn,Web-enabled Awareness Research Network (WARN),,https://data.oceannetworks.ca/home,
4d381f8e-6ee1-409b-b0c3-7070cb09fa04,4tu-vertical-conflict-resolution-in-layered-airspace-with-reinforcement-learning-using-the-bluesky-open-air-traffic-simulator,Vertical Conflict Resolution in Layered Airspace with Reinforcement Learning using the BlueSky Open Air Traffic Simulator,Simulation environment build in the BlueSky Open Air Traffic simulator (see reference) for testing the relationship between DRL model efficacy and traffic density during training. Spefically focused on conflict resolution during vertical manoeuvres in a layered airspace.       Before running or going through the code make sure to read the ReadMe file.       The software is also available on github: https://github.com/jangroter/TrafficDensityImpactRL,,
4dd0851c-7f14-4a19-81fe-8f7abf53d15e,4tu-endogenous-macrodynamics-in-algorithmic-recourse,endogenous-macrodynamics-in-algorithmic-recourse,"Code and research results for SaTML 2023 research paper. Originally released here: https://github.com/pat-alt/endogenous-macrodynamics-in-algorithmic-recourse.     The research results include:     Folders with images that went into a) the body of the paper or b) the online companion.Folders with results (.jls; .csv) for different experiments: a) synthetic data; b) real-world data; and, c) mitigation strategies for both categories of datasets (see paper for details on experiments). Results for all categories are further grouped by dataset.For each dataset, results include: a) ""experiment.jls"" files that can be loaded into a Julia session: the loaded Julia objects are structs that contain all settings characterizing a specific experiment. b) ""output.csv"" files that contain the final experimental outputs: estimated counterfactual evaluation metrics groups by model and counterfactual explainer.",https://data.4tu.nl/v3/datasets/f96f4a4c-3b04-4fe2-a4b9-0ffd666a7e89.git,
4e41cc13-49a8-4635-adbf-d26cae90182c,era5cli,era5cli,"- download meteorological data in GRIB/NetCDF, including ERA5 data from the preliminary back extension, and ERA5-Land data. - list and retrieve information on available variables and pressure levels - select multiple variables for several months and years - split outputs by years, producing a separate file for every year instead of merging them in one file - download multiple files at once - extract data for a sub-region of the globe",https://github.com/ewatercycle/era5cli,ewatercycle/era5cli
4e46a112-ff5a-48d3-93b2-03f4b9b4a56c,4tu-source-code-for-assessing-the-validity-of-a-calcifying-oral-biofilm-model-as-a-suitable-proxy-for-dental-calculus,Source code for: Assessing the Validity of a Calcifying Oral Biofilm Model as a Suitable Proxy for Dental Calculus,"This repository contains the source code to the publication ""Assessing the Validity of a Calcifying Oral Biofilm Model as a Suitable Proxy for Dental Calculus"".     Bartholdy, B. P., Velsko, I. M., Fagernas, Z., Henry, A. G., &amp; Gur-Arieh, S. (2023). Assessing the validity of a calcifying oral biofilm model as a suitable proxy for dental calculus (p. 2023.05.23.541904). bioRxiv. https://doi.org/10.1101/2023.05.23.541904     More information can be found in the top-level README.md file.     The development version can be found on GitHub (https://github.com/bbartholdy/byoc-valid).",,
4e5bda22-580b-4fd6-b3cd-3037fce21885,4tu-hcomp2022-archie,HCOMP2022_ARCHIE,"This repo contains all code and data associated with the paper ""It Is Like Finding a Polar Bear in the Savannah! Concept-level AI Explanations with Analogical Inference from Commonsense Knowledge."" The dataset mainly contains generated analogies and expert evaluations from five colleagues in TU Delft. Our experimental results indicate that the proposed qualitative dimensions can positively contribute to the perceived helpfulness of analogy-based explanations.",https://data.4tu.nl/v3/datasets/d6208c6f-50b0-42a9-84f5-0da1d39bc805.git,
4f076d31-3763-43ed-a959-3ca9ff9e2c6a,ggir,GGIR,"* GGIR is an R-package to process and analysis multi-day data collected with wearable raw data accelerometers for physical activity and sleep research. * GGIR uses this information to describe the data per day of measurement or per measurement, including estimates of physical activity, inactivity, and sleep. As part of the pipeline GGIR performs automatic signal calibration, detection of sustained abnormally high values, detection of sensor non-wear and calculation of average magnitude acceleration based on a variety of metrics. * GGIR is the only open source licensed software that provides a full pipeline for both physical activity and sleep analyses, with a high freedom for the user to configure the analyses to their needs. * The package has been used for domain science in 70+ publications, and is supported by 8 methodological publications.  The package has been developed and tested for binary data from GENEActiv and GENEA devices, .csv-export data from Actigraph devices, and .cwa and .wav-format data from Axivity. These devices are currently widely used in research on human daily physical activity.  A list of publications using GGIR can be found here: https://github.com/wadpac/GGIR/wiki/Publication-list  The package vignette which gives a general introduction can be found here: https://cran.r-project.org/web/packages/GGIR/vignettes/GGIR.html.",https://github.com/wadpac/GGIR,wadpac/GGIR
4f117bd1-d917-42d5-b579-c89038acb7ca,proteus,PROTEUS,PROTEUS is a Python framework that simulates the coupled evolution of the atmospheres and interiors of rocky planets.,https://github.com/FormingWorlds/PROTEUS,FormingWorlds/PROTEUS
4f8a0d23-5361-4360-8f4d-a95c0a82c62c,caselawanalytics,caselawnet,"* Search tool for legal experts or students, interested in in network analysis * Search publicly available Dutch judgments on keywords and download the result as a network * The exported network can be visualized in the [Case Law App](https://www.research-software.nl/software/case-law-app) * Provides both a graphical user interface and a python package  The law is a text that describes what a person or legal entity can and cannot do. However, the law is somewhat open to interpretation. This interpretation is done by judges whenever a case is brought to court. Over time, the outcome of individual cases build what is called _case law_.  Because consistency is crucial for the fair application of the law, cases often reference other cases: if the reasoning behind a ruling in one case also applies to another case, then the ruling should be the same. To warrant consistency it is thus paramount that any relevant rulings from previous cases are identified. Conventionally, both the justice department and the defense depend on legal experts to make this identification when preparing a case. However, court rulings are often difficult to understand, there is only limited time available, and even experts are not aware of all cases that may be relevant.  To help mitigate this situation, the Netherlands eScience Center worked together with Maastricht University to develop an [interactive visualization](https://www.research-software.nl/software/case-law-app) that assists the legal community at large (prosecutors, judges, lawyers, legal aids, but also researchers and students) in analyzing case law.   To analyze a specific topic or theme in the law, a collection of law cases first need to be represented as a network or _graph_.  Each node in the graph represents a case, while the edges represent references to other cases. The caselawnet tool is basically a search machine, that retrieves the cases related to a search term, as well as the citations between those cases. The network can be downloaded and directly used in the visualization.",https://github.com/caselawanalytics/CaseLawAnalytics,caselawanalytics/CaseLawAnalytics
4faaa5d9-e197-4db9-8122-8562d50b07f2,4tu-numerical-models-and-datasets-concerning-the-publication-electrocatalytic-reaction-driven-flow,"Numerical models and datasets concerning the publication:""Electrocatalytic reaction-driven flow""","These are the relevant numerical models mph fiels used for simulating the velocity flow in the publication  ""Electrocatalytic reaction-driven flow"" Two major forms of studies were performed, namely effects of varying Daa and varying Dac These can  be found in the folder named Daa_Dac_parametric study.  In the second folder are enclosed the models used for validating experimental velocities at different hydrogen peroxide concentrations.",,
4faab8b6-2892-4258-8b3c-b97ced315c86,qmcblip,QMCBlip,"* user friendly * interface to ML Force FLARE, FLARE++, sGDML * interface to QMC packages such as CHAMP and QMCTorch    Driving molecular dynamic simulation with quantum monte carlo calculations of the atomic forces is computationally expensive. QMCBlip allows to train machine learning force fields on the fly to reduce the computational requirements of such calculations. QMCBlip was developed during the internship of Emiel Slootman in collaboration with Claudia Filippi from University of Twente.",https://github.com/NLESC-JCER/QMCblip,NLESC-JCER/QMCblip
4fdacca8-d77a-4856-969d-fe529a9c7652,argopy,argopy,,https://github.com/euroargodev/argopy,euroargodev/argopy
50143cb8-88d2-4097-981f-c9c30d822637,4tu-supplementary-code-to-the-paper-flexible-enterprise-optimization-with-constraint-programming,Supplementary code to the paper: Flexible Enterprise Optimization With Constraint Programming,"This repository contains experimental data for the experiments performed in ""Flexible Enterprise Optimization With Constraint Programming"". The experiments are based on how enterprise models can be ""solved"" through CP. Due to page limit reasons, not all experiments were discussed in the paper. The experiments in this repository can be divided into three categories. 1. Petri-nets. Here, petri net models based on enterprise models are solved through MiniZinc.2. Netlogo simulation + Neural network. Here, neural networks are trained on NetLogo simulation models. Then, these neural networks are embedded into MiniZinc, and used to find solutions to it in a multi objective sense.3. Other experiments. Here, a simple supply chain of a pizza restaurant, as well as a hospital case (FHCC) that was based on a DEMO model, are formulated as a CP model. These experiments are not dicussed in the thesis.",,
506939b9-7eb3-40e3-a5ad-8511be18efdb,icdgems,ICD_GEMs.jl,"## Overview   ### ICD  The ICD provides a common language for the classification of diseases, injuries and causes of death, and for the standardised reporting and monitoring of health conditions. It is designed to map health conditions to corresponding generic categories together with specific variations, assigning to these a designated code, up to six characters long. These data form the basis of comparison and sharing between health providers, regions and countries, and over periods.  In addition to this essential core function, the ICD can also inform a wide range of related activities. It is used for health insurance reimbursement; in national health programme management; by data collection specialists and researchers; for tracking progress in global health; and to determine the allocation of health resources. Patient quality and safety documentation is also heavily informed by the ICD.  The ICD system is designed to promote international comparability in the collection, processing, classification, and presentation of health statistics, and health information in general. Currently, 117 countries report causes of death to WHO. Seventy per cent of the worldâ€™s health resources are allocated based on ICD data. Current uses include cancer registration, pharmacovigilance, and more than 20,000 scientific articles cite ICD-10.  For further details about the ICD, please consider to read the [References](#References).  ### GEMs  The General Equivalence Mappings (GEMs) are the product of a coordinated effort spanning several years and involving the National Center for Health Statistics (NCHS), the Centers for Medicare and Medicaid Services (CMS), the American Health Information Management Association (AHIMA), the American Hospital Association, and 3M Health Information Systems providing a temporary mechanism to link ICD-9 codes to ICD-10 and vice versa.   According to the CMS:   > The purpose of the GEMs is to create a useful, practical, code to code translation reference dictionary for both code sets, and to offer acceptable translation alternatives wherever possible. For each code set, it endeavors to answer this question: Taking the complete meaning of a code (defined as: all correctly coded conditions or procedures that would be classified to a code based on the code title, all associated tabular instructional notes, and all index references that refer to a code) as a single unit, what are the most appropriate translation(s) to the other code set?  For further details on how the GEMs work, please consider to read the [References](#References).  ## Installation  Press `]` in the Julia REPL and then  ```julia pkg> add ICD_GEMs ```  ## Tutorial   Let us showcase the features of the package.  First, we import the necessary packages:  ```julia using ICD_GEMs ```  The GEMs converting ICD-10 codes into ICD-9 and viceversa have already been downloaded from [here](https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2018/Dxgem_2018.zip) and exported both as `DataFrame` s from [DataFrames.jl](https://github.com/JuliaData/DataFrames.jl) and `OrderedDict`s from [DataStructures.jl](https://github.com/JuliaCollections/DataStructures.jl):  - `GEM_I10_I9_dataframe`:  GEM from ICD-10 to ICD-9 represented as a dataframe; - `GEM_I10_I9_dictionary`:  GEM from ICD-10 to ICD-9 represented as a dictionary; - `GEM_I9_I10_dataframe`:  GEM from ICD-9 to ICD-10 represented as a dataframe; - `GEM_I9_I10_dictionary`:  GEM from ICD-10 to ICD-9 represented as a dictionary.  These are all `GEM` structs consisting of two fields:  - `data`: The actual GEM, which may either be a dataframe or a dictionary; - `direction`: A string, either `""I10_I9""` or `""I9_I10""`.  The package can be used with custom GEMs (or ""applied mappings"") as long as they are wrapped inside a `GEM` struct where the `data` field is either a dataframe or a dictionary with the exact same format as the exported ones above. If some new GEMs are released by the [CDC](https://www.cdc.gov/nchs/icd/Comprehensive-Listing-of-ICD-10-CM-Files.htm), the functions that load CDC-formatted .txt files are exported:  ```julia path      = ""path/to/txt"" direction = ""I10_I9"" # If the GEM translates from ICD-10 to ICD-9, otherwise ""I9_I10"" gem       = get_GEM_dataframe_from_cdc_gem_txt(path, direction) # Or get_GEM_dictionary_from_cdc_gem_txt(path, direction) ```  Finally, let us show how to translate ICD-10 codes into ICD-9 for, as an example, neoplasms:  ```julia ICD_10_neoplasms = ""C00-D48"" # This is equivalent as explicitly specifiying all codes from C00.XX to D48.XX ICD_9_neoplasm   = execute_applied_mapping(GEM_I10_I9_dictionary, [""C00-D48""])   ```  ```nothing 932-element Vector{String}:  ""1400""  ""1401""  ""1409""  ""1403""  ""1404""  ""1405""  ""1406""  ""1408""  ""1410""  â‹®  ""23877""  ""2380""  ""2381""  ""2354""  ""2382""  ""2383""  ""2388""  ""2389"" ```  And back:  ```julia ICD_10_neoplasm_back   = execute_applied_mapping(GEM_I9_I10_dictionary, ICD_9_neoplasm) ```  ```nothing 1186-element Vector{String}:  ""C000""  ""C001""  ""C002""  ""C003""  ""C004""  ""C005""  ""C006""  ""C008""  ""C01""  â‹®  ""D480""  ""D481""  ""D483""  ""D484""  ""D485""  ""D4860""  ""D487""  ""D489"" ```  Are these the same as all the codes we started from, namely all ICD-10 codes from the first starting with `C00` to the last that starts with `D48` ? These would be:  ```julia get_ICD_code_range(""C00-D48"", ""ICD-10"") # Specify a code range and the revision it belongs to ```  ```nothing 1622-element Vector{String}:  ""C000""  ""C001""  ""C002""  ""C003""  ""C004""  ""C005""  ""C006""  ""C008""  ""C009""  â‹®  ""D483""  ""D484""  ""D485""  ""D4860""  ""D4861""  ""D4862""  ""D487""  ""D489"" ```  No, they are more! Why? Because the mapping between the two revisions is not injective nor surjective (see the [official documentation](https://github.com/JuliaHealth/ICD_GEMs.jl/tree/main/official_gem_documentation)).   More complex translations, involving both single codes and ranges with arbitrary precision on both ends, can be performed:  ```julia execute_applied_mapping(GEM_I10_I9_dictionary, [""I60-I661"", ""I670"", ""I672-I679""]) ```  **Note**: All codes must be specified and are returned by the translation utilities without punctuation (no dot before the decimal digits).  ## How to Contribute  If you wish to change or add some functionality, please file an [issue](https://github.com/JuliaHealth/ICD_GEMs.jl/issues).   ## How to Cite   If you use this package in your work, please cite this repository using the metadata in [`CITATION.bib`](https://github.com/JuliaHealth/ICD_GEMs.jl/blob/main/CITATION.bib).  ## Announcements   - [Twitter](https://twitter.com/In_Phy_T/status/1529444377281671168?s=20&t=O7f9qRLdsyY8WM3TEdCWLg) - [Discourse](https://discourse.julialang.org/t/ann-icd-gems-jl-a-package-to-translate-between-icd-9-and-icd-10-codes/81679?u=pietromonticone) - [Forem](https://forem.julialang.org/inphyt/ann-icdgemsjl-a-package-to-translate-between-icd-9-and-icd-10-codes-17am)  ## References   - Butler (2007), [ICD-10 General Equivalence Mappings: Bridging the Translation Gap from ICD-9](https://library.ahima.org/doc?oid=74265#.Ynre9i8RoiM), *Journal of AHIMA* - CMS (2018), [2018 ICD-10 CM and GEMs](https://www.cms.gov/Medicare/Coding/ICD10/2018-ICD-10-CM-and-GEMs) - NCHS-CDC (2018), [Diagnosis Code Set General Equivalence Mappings: ICD-10-CM to ICD-9-CM and ICD-9-CM to ICD-10-CM](https://ftp.cdc.gov/pub/health_statistics/nchs/publications/ICD10CM/2018/Dxgem_guide_2018.pdf)  - NCHS-CDC (2022), [Comprehensive Listing ICD-10-CM Files](https://www.cdc.gov/nchs/icd/Comprehensive-Listing-of-ICD-10-CM-Files.htm) (download General Equivalence Mappings [here](https://ftp.cdc.gov/pub/Health_Statistics/NCHS/Publications/ICD10CM/2018/Dxgem_2018.zip)) - WHO (2022), [ICD-11 Implementation or Transition Guide](https://icd.who.int/docs/ICD-11%20Implementation%20or%20Transition%20Guide_v105.pdf)",https://github.com/JuliaHealth/ICD_GEMs.jl,JuliaHealth/ICD_GEMs.jl
50d03fd3-3c53-481e-8590-57e72590c1e3,orangehackathon,Orange Hackathon,* adds modules for text processing (emails) to the Orange3 visual data mining tool * adds modules for data visualization to the Orange3 visual data mining tool * new modules can be used in combination with existing Orange3 modules  Orange3  is  a free data mining toolkit offering a large collection of  modules and a user-friendly graphical interface for building arbitrary processing   pipelines. We extended Orange3 with custom-built modules for analyzing therapeutic email conversations in the project What Works When for Whom.,https://github.com/e-mental-health/orange-hackathon,e-mental-health/orange-hackathon
50e26b52-6130-4c45-bc27-fcb362169d0a,4tu-aeolis-v2-1-0,AeoLiS v2.1.0,"AeoLiS is a process-based model for simulating aeolian sediment transport in situations where supply-limiting factors are important, like in coastal environments. Supply-limitations currently supported are soil moisture contents, sediment sorting and armouring, bed slope effects, air humidity and roughness elements.     The maintenance and development is done by the AEOLIS developer team: Current members are: Bart van Westen at Deltares, Nick Cohn at U.S. Army Engineer Research and Development Center (ERDC), Sierd de Vries (founder) at Delft University of Technology, Christa van IJzendoorn at Delft University of Technology, Caroline Hallin at Delft University of Technology, Glenn Strypsteen at Katholieke Universiteit Leuven and Janelle Skaden at U.S. Army Engineer Research and Development Center (ERDC).  Previous members are: Bas Hoonhout (founder), Tom Pak, Pieter Rauwoens and Lisa Meijer     During a paper revision, a change was made to the transport.py file. This change is not included in the original AeoLiS v2.1.0 version (.tar.gz-file). Therefore, we have added a separate transport.py file that contains a, newly implemented, grain-size dependent Bagnold equation. This change will be included in future versions of AeoLis.",,
50e87f26-8247-4c09-87c6-29d496bca795,unsat3d,UNSAT3D,,https://github.com/UNSAT3D/unsat,UNSAT3D/unsat
51247fea-11a0-497e-b658-bd47d9391618,tilbot,Tilbot,,https://github.com/tilbotio/tilbot-main,tilbotio/tilbot-main
5160aba3-06de-4b3b-b41e-87131ee960d7,yatiml,yatiml,"- Lets you describe a document format/object model by defining Python classes - Checks YAML input against your format/model - Applies transformations that make the YAML file easier to read and write - Constructs Python objects from YAML in your format - Saves Python objects to YAML in your format  YAML-based file formats can be very handy, as YAML is easy to write by humans, and parsing support for it is widely available. Just read your YAML file into a document structure (a tree of nested dicts and lists), and manipulate that in your code.  While this works fine for simple file formats, it does not scale very well to more complex file formats such as the Common Workflow Language (CWL) or the Multiscale Computing Language (yMCL). Manual error-checking is lots of work and error-prone, defaults are not set automatically (which is especially tricky if you have multiple nested optional objects), and the file format often ends up somewhat underspecified.  Furthermore, a small collection of nested dicts and lists may work fine, but for more complex file formats, this becomes unwieldy and a set of objects is a better choice. Although it is not often used this way, YAML is actually a fully fledged object-to-text serialisation protocol. The Python yaml and ruamel.yaml libraries will actually construct objects for you, but the class names need to be put in the YAML file for that to work, which makes those files harder to read and write for humans.  YAtiML is a helper library that helps address these issues. With YAtiML, you have easy-to-read YAML for the user, and easy-to-use objects for the programmer, with validation and automatic type recognition in between.",https://github.com/yatiml/yatiml,yatiml/yatiml
51dcb49e-39e4-43ac-978c-17cfbb453f68,4tu-script-companion-to-a-note-on-the-modelling-of-lubrication-forces-in-unresolved-simulations,Script companion to: A note on the modelling of lubrication forces in unresolved simulations,"Script companion to:  A note on the modelling of lubrication forces in unresolved simulations Tim M.J. Nijssen, Marcel Ottens, Johan T. Padding Powder Technology, 2022  contact: t.m.j.nijssen@tudelft.nl (Tim M.J. Nijssen) Tested with: MATLAB R2021b and GNU Octave 6.4.0 Delft University of Technology (NL), 09-03-2022",,
51f218f5-4c71-4c72-a926-97ecd1ad0202,mpet,MPET,,https://github.com/TRI-AMDD/mpet,TRI-AMDD/mpet
5265d94d-5369-4213-a6d4-21b37071ecdc,paradigma,paradigma,"| Badges | | |:----:|----| | **Packages and Releases** | [![Latest release](https://img.shields.io/github/release/biomarkersparkinson/paradigma.svg)](https://github.com/biomarkersparkinson/paradigma/releases/latest) [![PyPI](https://img.shields.io/pypi/v/paradigma.svg)](https://pypi.python.org/pypi/paradigma/)  [![Static Badge](https://img.shields.io/badge/RSD-paradigma-lib)](https://research-software-directory.org/software/paradigma) | | **DOI** | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13838393.svg)](https://doi.org/10.5281/zenodo.13838393) | | **Build Status** | [![](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/) [![Build and test](https://github.com/biomarkersParkinson/paradigma/actions/workflows/build-and-test.yml/badge.svg)](https://github.com/biomarkersParkinson/paradigma/actions/workflows/build-and-test.yml) [![pages-build-deployment](https://github.com/biomarkersParkinson/paradigma/actions/workflows/pages/pages-build-deployment/badge.svg)](https://github.com/biomarkersParkinson/paradigma/actions/workflows/pages/pages-build-deployment) | | **License** |  [![GitHub license](https://img.shields.io/github/license/biomarkersParkinson/paradigma)](https://github.com/biomarkersparkinson/paradigma/blob/main/LICENSE) | <!-- | **Fairness** |  [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/8083/badge)](https://www.bestpractices.dev/projects/8083) | -->   Paradigma is a Python package (see [documentation](https://biomarkersparkinson.github.io/paradigma/)) for processing wearable sensor data to identify digital biomarkers for Parkinsonâ€™s disease. It simplifies the analysis of gait and photoplethysmography (PPG) data using a unified Time Series Data Format (TSDF), helping researchers extract key PD biomarkers. Future updates will add tremor detection and more feature extraction tools.   ## Installation  The package is available in PyPi and requires [Python 3.10](https://www.python.org/downloads/) or higher. It can be installed using:  ```bash pip install paradigma ```",https://github.com/biomarkersParkinson/paradigma,biomarkersParkinson/paradigma
528c0634-9dc2-49ab-bb5b-26a373118cf0,tulipaclustering,TulipaClustering.jl,,https://github.com/TulipaEnergy/TulipaClustering.jl,TulipaEnergy/TulipaClustering.jl
534ad575-5167-490a-bf59-2fb277f03757,otree-waiting-room,oTree Waiting Room,,https://github.com/obeliss-nlesc/otree-waiting-room,obeliss-nlesc/otree-waiting-room
539109bf-8bf9-431e-a68e-b8beb174df35,4tu-bluesky-software-underlying-the-publication-distributed-conflict-resolution-at-high-traffic-densities-with-reinforcement-learning,Bluesky software: underlying the publication â€œDistributed Conflict Resolution at High Traffic Densities with Reinforcement Learningâ€,"Bluesky scenarios and code files used in the work ""Distributed Conflict Resolution at High Traffic Densities with Reinforcement Learning"". The scenario files can be used with the Bluesky simulator tool implementation found at https://github.com/TUDelft-CNS-ATM/bluesky.",,
53bff0a8-c803-4e39-ae0a-459cf7c4ce06,4tu-inductive-3d-printer-xy-calibration-gui,Inductive 3D printer XY calibration GUI,"This program can be used to calibration the x and y offset of a  multi-material 3D printer using LDC1101EVM evaluation module, to which a  coil is mounted with the axis perpendicular to the PCB.   The evaluation module should be placed on the bed and the printer and  the module should be connected using usb cables to your computer. Then  when running this program it will move each nozzle over the coil and  determine the offset between the nozzles. This works because the nozzle will cause a decrease in the inductance of the coil as is described in ""XY calibration of tool offsets in a multi-material fused deposition 3D printer""",,
53c3582d-2a80-4fff-86f1-29907349291b,4tu-code-to-test-the-implementation-of-linear-bi-level-parent-child-mpc, Code to test the implementation of linear Bi-level Parent-Child MPC,"Implementation, simulation, and testing the performance of Bi-level Parent-Child MPC while controlling a linear system. More details can be found in the referenced paper, which will be added after publication.",https://data.4tu.nl/v3/datasets/1f721cf5-1db9-4565-919b-8fa0f526f5bd.git,
53db015e-c5d7-437a-a66d-28ac716c0ecc,4tu-software-supporting-analysis-of-natural-variation-in-photosynthesis-in-a-panel-of-brassicaceae-species,Software supporting: Analysis of natural variation in photosynthesis in a panel of Brassicaceae species,"This software package contains all scripts employed to analyze data from an high- and low-throughput investigation of natural variation in photosynthetic light-use efficiency (LUE), and a number of traits potentially correlated to it, in a panel of ten Brassicaceae species. In this study, I performed an analysis of photosynthetic efficiency at high irradiance in ten species that reflect key evolutionary events within the Brassicaceae family: Arabidopsis thaliana, Brassica oleracea, Brassica nigra, Brassica rapa, Brassica tournefortii, Erucastrum littoreum, Hirschfeldia incana, Sinapis alba, Sisymbrium irio, and Zahora ait-atta. I made use of high-throughput phenotyping techniques to measure photosynthetic efficiency, and integrated these measurements with other image-based parameters, such as the Excess Green Index (ExGI) and the Normalized Difference Vegetation Index (NDVI), as well as a range of anatomical and biochemical characteristics that potentially influence photosynthetic efficiency. I then explored the resulting multivariate dataset using various statistical methods to identify trends across species and investigated if more species within the Brassicaceae family show high-photosynthetic LUE at high irradiance. Furthermore, I assessed the alignment of these trends with the evolutionary history of the Brassicaceae family. This study delivers a detailed description of inter-specific variation in photosynthetic parameters for the Brassicaceae family, completed by a selection of anatomical and biochemical characteristics that may play a role in supporting high photosynthetic LUE under high irradiance. The gained insights will be important in developing strategies to enhance the photosynthetic LUE at high irradiance of crop species.",https://data.4tu.nl/v3/datasets/38581506-079f-4470-a149-dc02a09d5b45.git,
53e283e7-ad54-4626-b7c5-d0de7fad3ac6,streetview-segmentation,Streetview Segmentation,"Streetview Segmentation script performs semantic segmentation on images, using models from Facebook's collection of Mask2Former models. Output consists of a CSV-file detailing, for each input image, the number of pixels in each semantic class in that image. Optionally, it can save a copy of each input image overlaid with the semantic segmentation. If the input images are 360Â° photo's, the script provides the possibility of tranforming them by projecting them onto a cube, resulting in six images per input image.  This script is specifically designed to run on a computer without a GPU. Some of the underlying libraries require the presence of CUDA-drivers to run, even if the actual device is absent. As it can be problematic to install such drivers on a computer without an actual GPU, the program is packaged as Docker-container, based on an official NVIDIA-image, which comes with pre-installed drivers. Building and running the container requires the presence of Docker engine. Note that the container is based on a Linux image (Ubuntu 18.04); running it on a Windows computer may require extra configuration.",https://github.com/UtrechtUniversity/streetview-segmentation,UtrechtUniversity/streetview-segmentation
54113ae7-6ecc-430f-aa9f-1886f376ba36,4tu-dales-v4-2-modified-for-ammonia-plume-dispersion-and-blending-distance-estimations,DALES v4.2 modified for ammonia plume dispersion and blending-distance estimations,"A modified version of the DALES model version 4.2 for the purpose of studying ammonia plume dispersion in a polluted atmosphere. With the data in this repository, the fine-scale simulation framework can be applied to individual locations to study the representativity of (potential new) measurement sites under the local conditions, using the concept of blending-distance.     This repository contains the modified DALES code, instruction, MATLAB processing scripts and example input files of the reference experiment are added to the repository. The modifications with respect to DALES v4.2 are described in the publication.",,
54209972-aa10-4bc6-b11d-55031a24f593,4tu-rooster-ros-package-for-robot-fleet-management,ROOSTER: ROS package for robot fleet management,"Robot Optimization, Scheduling, Task Execution and Routing (Ro.O.S.T.E.R.) is a ROS (Robot Operating System) based open source project to develop a heterogeneous fleet management solution with task allocation, scheduling and autonomous navigation capabilities.  This software has been developed as part of the work at the 'Center of Design for Advanced Manufacturing' lab of TU Delft on the 'Collaborating and coupled AGV swarms with extended environment recognition' project funded by EIT Manufacturing.  Detailed documentation including architectural overview, installation instructions, license information and source code API documentation can be found here.",,
54353825-1d58-4289-b7fe-54e82dce8307,4tu-swinging-kite,swinging-kite,"This Python code is published as part of the scientific paper Swinging Motion of a Kite with Suspended Control Unit Flying Turning Manoeuvres published in the Wind Energy Science journal. Along with the code, the flight data for the specific pumping cycles used in this study are included.    The paper assesses the accuracy of a two-point kite model in resolving this swinging motion using two different approaches: approximating the motion as a transition through steady-rotation states and solving the motion dynamically. The kite is modelled with two rigidly linked point masses representing the control unit and wing, which conveniently extend a discretised tether model. The tether-kite motion is solved by prescribing the trajectory of the wing point mass to replicate a figure-of-eight manoeuvre from the flight data of an existing prototype. The computed pitch and roll of the kite are compared against the attitude measurements of two sensors mounted to the wing.",https://data.4tu.nl/v3/datasets/5bdc36c2-6e55-483b-9b1f-ac523b544ca5.git,
54b2652b-1341-4efc-b63f-39be897bffbe,rucio-deployment-for-km3net,Rucio deployment for KM3NeT,"[Rucio](https://rucio.cern.ch/) is a scientific data management solution. It is most useful for research groups that have a lot of data in large-ish files and want to process that at multiple High Performance Computing sites. It provides a REST API as well as a Python and command line client. To run a Rucio server and associated services, Rucio can be deployed on a Kubernetes cluster. This repository is used for the [KM3NeT](https://www.km3net.org/) deployment and can be used as an example by others interested in running their own Rucio service.",https://git.km3net.de/rucio/rucio-deployment,
54f57a08-ee4a-4b09-8b35-6f317ba3c9c0,4tu-det2d-read,det2d-read,"About 200 coronary angiogram procedures were recorded from a distance in the Reinier de Graaf hospital, Delft, NL.  Human pose data were extracted from these videos and saved.  As the dataset is large, a tool was needed to read the pose data for usage.  This Python package reads poses from .det2d.json files, and converts between detection and tracklet formats.  For large datasets, it offers functions to read pose data one file at a time such that the entire dataset need not be loaded into memory.  It uses multiprocessing to buffer poses in memory for faster access.",https://data.4tu.nl/v3/datasets/2e7f333d-f828-4c78-a47d-26ae7d162e08.git,
5503cab5-2a7f-430e-a004-ffff66f788ea,4tu-chi2025-plan-then-execute-llmagent,CHI2025_Plan-then-Execute_LLMAgent,"This repo contains all code, data, and user interfaces associated with paper ""Plan-Then-Execute: An Empirical Study of User Trust and Team Performance When Using LLM Agents As A Daily Assistant."" In our study, we analyzed different extents of user involvement in the planning and execution stages of LLM agents. Our data is evaluated based on action sequences. We also recorded how users interact with LLM agents and provided an interface built upon Flask.",https://data.4tu.nl/v3/datasets/5ff47fde-d960-4caf-806f-214d9e491276.git,
552e49b0-603f-43e5-a7b0-0dd86705e6a8,zeeschuimer,Zeeschuimer,"# ðŸ´â€â˜ ï¸ Zeeschuimer  [![DOI: 10.5281/zenodo.4742622](https://zenodo.org/badge/DOI/10.5281/zenodo.6826877.svg)](https://doi.org/10.5281/zenodo.6826877) [![License: MPL 2.0](https://img.shields.io/badge/license-MPL--2.0-informational)](https://github.com/digitalmethodsinitiative/4cat/blob/master/LICENSE)  <p align=""center""><img alt=""A screenshot of Zeeschuimer's status window"" src=""images/example_screenshot.png""></p>  Zeeschuimer is a browser extension that monitors internet traffic while you are browsing a social media site, and  collects data about the items you see in a platform's web interface for later systematic analysis. Its target audience is researchers who wish to systematically study content on social media platforms that resist conventional scraping or  API-based data collection.  You can, for example, browse TikTok and later export a list of all posts you saw in the order you saw them in. Data can  be exported as a JSON file or exported to a [4CAT](https://github.com/digitalmethodsinitiative/4cat) instance for  analysis and storage. Zeeschuimer is primarily intended as a companion to 4CAT, but you can also integrate its output into your own analysis pipeline.  Currently, it supports the following platforms: * [TikTok](https://www.tiktok.com) (posts and comments) * [Instagram](https://www.instagram.com) (posts only) * [X/Twitter](https://www.x.com) * [LinkedIn](https://www.linkedin.com) * [9gag](https://9gag.com) * [Imgur](https://imgur.com) * [Douyin](https://douyin.com) * [Gab](https://gab.com)  Platform support requires regular maintenance to keep up with changes to the platforms. If something does not work, we welcome issues and pull requests. See 'Limitations' below for some known limitations to data capture.  The extension does not interfere with your normal browsing and never uploads data automatically, only when you explicitly ask it to do so. It uses the [WebRequest](https://developer.mozilla.org/en-US/docs/Mozilla/Add-ons/WebExtensions/API/webRequest) browser API to locally collect and parse the data search engines are sending to your browser as you use it.  ## Installation Zeeschuimer is in active development. .xpi files that you can use to install it in your browser are available on the  [releases](https://github.com/digitalmethodsinitiative/zeeschuimer/releases) page. These are signed and can be installed  in any Firefox-based browser. If you want to run the latest development version instead, you can [do so from the Firefox debugging console](https://www.youtube.com/watch?v=J7el77F1ckg) after cloning the repository locally.  ## How to use A [guide to using Zeeschuimer and 4CAT](https://tinyurl.com/nmrw-zeeschuimer-tiktok) is available. Basic instructions  are as follows:   Install the browser extension in a Firefox browser. A button with the Zeeschuimer logo (a 'Z') will appear in the  browser toolbar. Click it to open the Zeeschuimer interface. Enable capturing for the sites you want to capture from.  Next, simply browse a supported platform's site. You will see the amount of items detected per platform increase as you  browse. When you have the items you need, you can export the data as an [ndjson](https://ndjson.org) file, or upload it to a 4CAT instance where a 4CAT dataset will be created from the uploaded items. You can then run 4CAT's analytical  processors on the data.  To upload to 4CAT, copy the URL of the website of the 4CAT instance to the ""4CAT instance"" field at the top of  Zeeschuimer's interface. You can then use the ""to 4CAT"" button to create a new 4CAT dataset from the captured data.  After uploading, Zeeschuimer will show you a link and the ten most recently uploaded datasets are shown at the bottom of the interface.  Don't forget to reset the data as needed. For example, if you want to create a dataset for a given TikTok hashtag, first reset the TikTok data in Zeeschuimer, _then_ go to the hashtag's ""Explore"" page on TikTok, and then upload the dataset when you've scrolled down enough to be satisfied with the amount of items.  If you find yourself scrolling a lot to collect data, consider using another browser extension to do it for you, for  example [FoxScroller](https://addons.mozilla.org/en-US/firefox/addon/foxscroller/).  ## Limitations  Due to the technical limitations, it may not be possible to collect all items from all 'views' for each supported  platform. The following limitations are known:  * *Instagram* items that cannot be captured:   * Stories   * Posts from the 'Tagged' and 'Reels' tabs on a profile page   * Posts from the 'Saved' overview of bookmarked posts   * Posts from the 'For You' feed on the 'Explore' page (the 'Not personalized' feed _does_ work)   * 'Suggested for you' and 'Sponsored' posts on the front page feed * *TikTok* items that cannot be captured:   * Live streams  Note that these are *known* limitations; data capture may break or change based on platform changes. Always  cross-reference captured data with what you are seeing in your browser.  ## Credits & license Zeeschuimer was developed by Stijn Peeters for the [Digital Methods Initiative](https://digitalmethods.net) and is  licensed under the Mozilla Public License, 2.0. Refer to the LICENSE file for more information.  Graphics based on [an image generated by Dall-E](https://labs.openai.com/s/oWvGAHC0pxwWV3bNLfENu7AV), for the prompt  'detail of a 1914 metaphysical painting by giorgio de chirico depicting a buccaneer pensively looking out over the open  sea'. Interface icons by [Font Awesome](https://fontawesome.com/license/free).  [Open Sans](https://fonts.google.com/specimen/Open+Sans) and [Lobster](https://fonts.google.com/specimen/Lobster) fonts  from Google Fonts.  Development is supported by the Dutch [PDI-SSH](https://pdi-ssh.nl/en/) foundation through the [CAT4SMR  project](https://cat4smr.humanities.uva.nl/).",https://github.com/digitalmethodsinitiative/zeeschuimer,digitalmethodsinitiative/zeeschuimer
553d5466-edbe-4a69-a885-051c0889cd90,nplinker-webapp,NPLinker Web Application,,https://github.com/NPLinker/nplinker-webapp,NPLinker/nplinker-webapp
5545c589-03db-4292-a82b-e3bb5ce5b3bc,siga-py,SIGA.py,* An easy-to-use command-line tool for bioinformaticians * Transforms genome annotations in GFF files into RDF graphs using a config file * Supports different GFF versions and RDF serializations (formats) * Used in combination with RDF stores or Linked Data platforms,https://github.com/candYgene/siga,candYgene/siga
555179ce-615a-4618-8125-956d5b03fc37,4tu-application-of-sddtmpc-to-control-a-unicycle,Application of SDDTMPC to control a unicycle,"The code of this repository is used to implement and test a new algorithm called SDD-TMPC. It is a control algorithm that sacrifices a bit of optimality (much less than tube MPC) but returns robust solutions. In this repository, SDD-TMPC was used to control a unicycle. The results were compared to other controller from literature.",https://data.4tu.nl/v3/datasets/049b582f-0ed5-404f-ab57-e3bc933a28e9.git,
559d946a-95a0-491f-be71-dade48909515,4tu-code-and-secondary-data-of-analysis-on-pesticide-transport-during-rainfall-runoff-events,Code and secondary data of analysis on pesticide transport during rainfall runoff events,"This repository contains the code and secondary data used for analysis and calculations on runoff, erosion and pesticide transport in particulate and dissolved phase. The analysis is done on an observational dataset in which rainfall-runoff events where monitored in a small agricultural catchment in Limburg, The Netherlands.",,
55c5897a-5cf2-4e1b-858c-4928151905a0,4tu-code-for-project-approximate-edge-ai-components-for-energy-aware-adaptive-framework,Code for project: Approximate Edge AI Components for energy-aware adaptive framework,"The project contains a collection of code for energy-aware training and inference of CNN/DNN and transformer models, and further supports deployment on edge devices. The models used are primarily within the scope of perception applications. The project provides a comprehensive end-to-end framework for developing, optimizing, and deploying neural networks across CNN/DNN and transformer architectures. It implements approximate algorithms for model quantization and compression to reduce computational overhead while maintaining performance accuracy. The codebase also includes bias mitigation techniques with sensitivity and selectivity metrics analysis to ensure fair and robust model behavior across diverse datasets. Graph optimization modules enable efficient model structure refinement and computational graph streamlining for enhanced inference performance. The system supports comprehensive bias analysis tools that evaluate model fairness and provide detailed sensitivity reports for ethical AI deployment. Advanced quantization techniques include both post-training and quantization-aware training methods with support for various bit-width configurations. The compression pipeline includes pruning, knowledge distillation, and weight sharing strategies to minimize model size without significant accuracy loss. The code also includes a model partition and resource allocation mechanism using an integrated LAP-DTR (Layer-Adaptive Partitioning with Dynamic Task Redistribution) component, enabling energy-aware distributed inference for Vision Transformers across heterogeneous edge device networks with adaptive partitioning strategies and fault-tolerant execution. Edge deployment capabilities include optimized inference engines, real-time performance monitoring, and dynamic resource management for constrained computing environments. Overall, the codebase is specifically designed for perception applications in autonomous driving, computer vision tasks, and distributed edge computing scenarios including vehicular networks and IoT systems.",https://data.4tu.nl/v3/datasets/97dde772-abd4-4552-b41c-746b8686f137.git,
55d52d39-d922-409b-bf64-ea885bc8790c,netcdf2littler,NetCDF2LittleR,* Provides an easy way to convert netCDF files into the LittleR file format  NetCDF2LittleR is an application to convert NetCDF files to the Little-R format. The Little-R format is the accepted input format for the WRF data assimilation system (WRFDA) preprocessor (obsproc). Currently only the conversion of synoptical weather stations are supported by this application.,https://github.com/ERA-URBAN/netcdf2littler,ERA-URBAN/netcdf2littler
564d7113-b318-4ba7-9b5c-974c13a36b5c,speckcn2,speckcn2,,https://github.com/MALES-project/SpeckleCn2Profiler,MALES-project/SpeckleCn2Profiler
56852895-d3c8-4628-906b-303647c08a63,xraytiff2h5,XrayTIFF2h5 ,,https://github.com/UNSAT3D/XrayTIFF2h5,UNSAT3D/XrayTIFF2h5
56a027e9-3668-4e3d-8cdb-80c6c5e6d1ab,evidence,evidence,"- Provides AI/machine-learning support for close-reading-based research - Intuitive example based search throughout large corpora  - browser-based usage / User interface  - concept based search using abstract doc2vec representations - context based search using word frequency/TF-IDF represenations - automated processing of user-supplied corpora  ## Machine-supported research in humanities While research in the humanities has been able to leverage the digitization of text corpora and the development of computer based text analysis tools to its benefit, the interface current systems provide the user with is incompatible with the proven method of scholarly close reading of texts which is key in many research scenarios pursuing complex research questions.  What this boils down to, is the fact that it is often restrictive and difficult, if not impossible, to formulate adequate selection criteria, in particular for more complex or abstract concepts, in the framework of a keyword based search which is the standard entry point to digitized text collections.  ## Querying by example - close reading with tailored suggestions `evidence` provides an alternative, intuitive entry point into collections by leveraging the doc2vec framework. Using doc2vec `evidence` learns abstract representations of the theme and content of the elements of the user's corpus. Then, instead of trying to translate the scientific query into keywords, after compiling a set of relevant elements as starting points, i.e. examples of the concept the user is interested in, the user can query the corpus based on these examples of their concept of interest. Specifically, `evidence` retrieves elements with similar abstract representations and presents them to the user, using the users feedback to refine its retrieval. Furthermore, this concept-based query mode is complemented by the ability to perform additional retrieval using `more-like-this` context based retrieval function provided by `elasticsearch`. Together, this enables a user to combine the power of a close-reading approach with that of a large digitized corpus, selecting elements from the entire corpus which are likely to be of interest, but leaving the decision up to the user as to what evidence they deem useful.",https://github.com/ADAH-EviDENce/evidence,ADAH-EviDENce/evidence
56b134bb-d634-4347-9a6f-8a6ae98a94d6,snvec,snvec,,https://github.com/rezeebe/snvec,rezeebe/snvec
56b5c24c-91b5-4747-8b52-47be9f4fc681,haddock3,haddock3,"# A brief introduction to HADDOCK3  HADDOCK3 is the next generation integrative modelling software in the long-lasting HADDOCK project. It represents a complete rethinking and rewriting of the HADDOCK2.X series, implementing a new way to interact with HADDOCK and offering new features to users who can now define custom workflows.  In the previous HADDOCK2.x versions, users had access to a highly parameterisable yet rigid simulation pipeline composed of three steps: `ridig-body docking (it0)`, `semi-flexible refinement (it1)`, and `final refinement (itw)`.  ![HADDOCK 2.x workflow](https://github.com/haddocking/haddock3/raw/main/docs/figs/HADDOCK2-stages.png)  In HADDOCK3, users have the freedom to configure docking workflows into functional pipelines by combining the different HADDOCK3 modules, thus adapting the workflows to their projects. HADDOCK3 has therefore developed to truthfully work like a puzzle of many pieces (simulation modules) that users can combine freely. To this end, the â€œoldâ€ HADDOCK machinery has been modularised, and several new modules added, including third-party software. As a result, the modularisation achieved in HADDOCK3 allows users to duplicate steps within one workflow (e.g., to repeat the `it1` stage of the HADDOCK2.x rigid workflow).  Note that, for simplification purposes, at this time, not all functionalities of HADDOCK2.x have been ported to HADDOCK3, which does not (yet) support NMR RDC, PCS and diffusion anisotropy restraints, cryo-EM restraints and coarse-graining. Any type of information that can be converted into ambiguous interaction restraints can, however, be used in HADDOCK3, which also supports the *ab initio* docking modes of HADDOCK.  ![HADDOCK3 workflow](https://github.com/haddocking/haddock3/raw/main/docs/figs/HADDOCK3-workflow-scheme.png)  To keep HADDOCK3's modules organised, we catalogued them into several categories. But, there are no constraints on piping modules of different categories.  The main module's categories are ""topology"", ""sampling"", ""refinement"", ""scoring"", and ""analysis"". There is no limit to how many modules can belong to a category. Modules are added as developed, and new categories will be created if/when needed. You can access the HADDOCK3 documentation page for the list of all categories and modules. Below is a summary of the available modules:  * **Topology modules**     * `topoaa`: *generates the all-atom topologies for the CNS engine.* * **Sampling modules**     * `rigidbody`: *Rigid body energy minimisation with CNS (`it0` in haddock2.x).*     * `lightdock`: *Third-party Glow-worm Swarm Optimisation (GSO) docking software.*     * `gdock`: *Third-party genetic algorithm-based docking software.* * **Model refinement modules**     * `flexref`: *Semi-flexible refinement using a simulated annealing protocol through molecular dynamics simulations in torsion angle space (`it1` in haddock2.x).*     * `emref`: *Refinement by energy minimisation (`itw` EM only in haddock2.4).*     * `mdref`: *Refinement by a short molecular dynamics simulation in explicit solvent (`itw` in haddock2.X).* * **Scoring modules**     * `emscoring`: *scoring of a complex performing a short EM (builds the topology and all missing atoms).*     * `mdscoring`: *scoring of a complex performing a short MD in explicit solvent + EM (builds the topology and all missing atoms).* * **Analysis modules**     * `alascan`: *Performs alanine scanning on the models generated in the previous step.*     * `caprieval`: *Calculates CAPRI metrics (i-RMDS, l-RMSD, Fnat, DockQ) with respect to the top scoring model or reference structure if provided.*     * `contactmap`: *Calculates the contact maps for the models generated in the previous step.*     * `clustfcc`: *Clusters models based on the fraction of common contacts (FCC)*     * `clustrmsd`: *Clusters models based on pairwise RMSD matrix calculated with the `rmsdmatrix` module.*     * `ilrmsdmatrix`: *Calculates the pairwise iLRMSD matrix between all the models generated in the previous step.*     * `rmsdmatrix`: *Calculates the pairwise RMSD matrix between all the models generated in the previous step.*     * `seletop`: *Selects the top N models from the previous step.*     * `seletopclusts`: *Selects top N clusters from the previous step.*  The HADDOCK3 workflows are defined in simple configuration text files, similar to the TOML format but with extra features. Here is an example of a configuration file that would reproduce the HADDOCK2.x rigid workflow:  ```toml run_dir = ""run1-protein-protein"" ncores = 40 mode = ""local""  # molecules to be docked molecules =  [     ""data/e2aP_1F3G.pdb"",     ""data/hpr_ensemble.pdb""     ]  [topoaa]  [rigidbody] ambig_fname = ""data/e2a-hpr_air.tbl""  [seletop]  [flexref] ambig_fname = ""data/e2a-hpr_air.tbl""  [emref] ambig_fname = ""data/e2a-hpr_air.tbl""  [clustfcc]  [caprieval] ```  In this example only very few parameters are defined as most correspond to the default ones defined for each module. The sampling in this case would be 1000 rigidbody models and 200 for the refinement stages. Clustering is based on the fraction of common contacts and the final clusters are analysed using the best model generated as a reference (the `caprieval` module).  Detailed explanations on how to configure a workflow through the configuration files can be found <a href=""https://github.com/haddocking/haddock3/blob/main/docs/tutorials/user_config.rst"">here</a>.  Having the configuration file ready, users can run HADDOCK3 with a single command-line input:  ```bash haddock3 config-file.cfg ```  You can find examples of HADDOCK3 workflows for the different biological systems in the `examples` [subfolder][examples] of the HADDOCK3 folder.  HADDOCK3 will start running (different execution modes are available using either local resources (threads) or in ""batch"" mode using a batch queuing system) - for details, see [here][queue]. See examples in the `examples` [subfolder][examples] for configuration files ending in `-full.cfg`.  Finally, HADDOCK3 has several advanced features that allow users additional flexibility in creating, restarting and extending runs, for example. Please read about our advanced features [here][advanced].  Please continue exploring our documentation pages for more information and explanations on HADDOCK3. If you wish to go beyond that, you might wish to see our lectures on YouTube:  * [Introducing HADDOCK3: Enabling modular integrative modelling pipelines](https://www.youtube.com/watch?v=V7uwFbVDKFE) * [Modular code for a modular software: developing HADDOCK3](https://www.youtube.com/watch?v=5Uk1EvzCOIg)  [queue]: https://github.com/haddocking/haddock3/blob/87e7c81ab6827d331d0c00bb9fa1b1d742344ef6/src/haddock/modules/defaults.yaml#L26-L40 [examples]: https://github.com/haddocking/haddock3/tree/main/examples [advanced]: https://github.com/haddocking/haddock3/tree/main/docs/tutorials",https://github.com/haddocking/haddock3,haddocking/haddock3
57717dcb-aa60-40f0-ac5c-5bc80ad9e500,pantools, PanTools,PanTools currently provides these functionalities:  - Construction of a panproteome - Adding new genomes to the pangenome  - Adding structural/functional annotations to the genomes - Detecting homology groups based on similarity of proteins - Optimization of homology grouping using BUSCO - Read mapping - Gene classification - Phylogenetic methods,https://git.wur.nl/bioinformatics/pantools,
577b60f7-c7c4-4bca-a19d-d6888a6a0fe4,2025-jarigsma-001,Landscape and Land Cover Generation,"This model is an algorithm implemented in NetLogo.   Landcover generation is user-configurable, supporting both stochastic dispersion (Figure 1) and seed-initiatied contiguous expansion (Figure 2) to accomodate varied modeling requirements.  ![random cover](https://github.com/user-attachments/assets/4a4b6e5d-bf31-48ca-a863-35050aa884b1) (Figure 1)  ![contiguous cover](https://github.com/user-attachments/assets/8ce52dc3-8821-4773-a3e1-4596ae8b57ff) (Figure 2)    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |min-patch-xcor, min-patch-ycor, max-patch-xcor, max-patch-ycor|integers|Represents the numeric boundaries of the NetLogo world in terms of patch coordinates (i.e., the lower-left and upper-right corners of the grid). | |patch-pixel-size|integer|Controls the visual resolution of the NetLogo view.  A larger patch-pixel-size makes each patch look bigger on screen but does not change the underlying model resolution or patch count. | |max-settlements|integer|Maximum number of settlements to be placed on the landscape. | |max-households-per-settlement|integer|Maximum number of households that can be assigned to each individual settlement. | |landscape-types|list of lists (integer, string, float, string)|List of landscape types defined as nested lists. Each list contains  an ID (integer), landscape name (string), proportion of total land (float between 0â€“1), and color (string or integer). Landscape types should be ordered from central to outermost. | |valid-landscape-types|list of strings|List of landscape type names where settlements are allowed to be placed. These must match the names defined in `landscape-types`. | |land-covers|list of lists (integer, string, string, float, number, number)|List of land cover definitions. Each cover is a nested list containing an ID (integer), cover name (string), a target landscape type (string),  proportion of that cover on the landscape type (float between 0â€“1),  minimum biomass volume (number, mÂ³/ha), and maximum biomass volume (number, mÂ³/ha). Use ""undefined"" if the target landscape type is not explicitly defined. | |biomass-density|number|Density of biomass used in calculations (e.g. wood = 439 kg/mÂ³). This is used to convert volume-based biomass availability to mass. | ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |landscape-type|string|The name of the landscape category assigned to a patch.| |landscape-id|integer|A numerical identifier for the type of landscape a patch belongs to.| |use|string|The functional use of a patch based on landcover classification.| |biomass|number|The total amount of biomass assigned to a patch based on its landcover.| |no-households|integer|Number of households in a settlement.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2025-Jarigsma-001,
57973492-14f8-46b7-8060-15643ebf9005,wntr-quantum,wntr-quantum,,https://github.com/quantumapplicationlab/wntr-quantum,quantumapplicationlab/wntr-quantum
58052779-cba2-4784-81dc-1afdbaa7f8b1,4tu-netlogo-model-agent-based-simulation-of-short-term-peer-to-peer-rentals-evidence-from-the-amsterdam-housing-market,"NetLogo model: ""Agent-Based Simulation of Short-Term Peer-to-Peer Rentals: Evidence from the Amsterdam Housing Market""","NetLogo model corresponding to the E&P B article ""Agent-Based Simulation of Short-Term Peer-to-Peer Rentals: Evidence from the Amsterdam Housing Market""",,
5807438f-2f15-4ce9-b654-98e0d10ab779,4tu-dataset-for-nanowire-solar-cell-above-the-radiative-limit,Dataset for Nanowire solar cell above the radiative limit,"A lossless solar cell operating at the Shockley-Queisser limit generates an open circuit voltage (Voc) equal to the radiative limit. At Voc, the highly directional beam of photons from the sun is absorbed and subsequently externally re-emitted into a 4Ï€ solid angle, providing a large photon entropy loss. A solar cell can beat the Shockley-Queisser limit and approach the 46.7% ultimate limit by decreasing the output solid angle of the light emission at open circuit conditions.  Here, we present a design for an InP single nanowire solar cell capable to operate 159 mV above the radiative limit. We first optimize the spontaneous emission factor (b-factor in the dataset) into a guided mode of the nanowire towards 68%. We subsequently launch a guided mode at the bottom straight part of the tapered nanowire yielding a photon escape probability of 81% for a tapering angle of Î¸=1.2 degrees                                                   and a top facet with a radius of 83 nm (transmission part of the dataset). When assuming homogeneous light emission along the nanowire, an outcoupling efficiency of 42% of the emitted light is obtained. The final optimization is the reduction of the emission cone towards 0.011 sr by focusing the guided mode with an external lens (lens part of the dataset).",,
584b9999-15a4-4752-8a9d-e3894793300b,4tu-code-accompanying-the-paper-on-aircraft-path-planning-in-continuous-environments-with-deep-reinforcement-learning,Code accompanying the paper on aircraft path planning in continuous environments with deep reinforcement learning,"This code is used for generating the results shown in the paper on aircraft path planning in continuous environments with deep reinforcement learning.    When the paper is published it will be referenced here.    The code is structured in 3 folders, all using the same population data, obtained from&nbsp;Eurostats. The discrete_environment folder contains all of the code related to the discretization, Dijkstra solutions and postprocessing of the Dijkstra output. The continuous_environment folder contains a fork of the&nbsp;BlueSky Open Air Traffic Simulator&nbsp;repository, with all of the plugins related to training the Deep Reinforcement Learning algorithm, and evaluating of the paths in the continuous environment. Finally the policy_plotter folder contains all of the tools for generating the visuals presented in the paper.    Before running or going through the code make sure to read the ReadMe file.    The software is also available on github: https://github.com/jangroter//PathplanningDRL",,
584bfd38-715d-4a43-9792-fdc23716b2f1,4tu-scintillation-comsol,SCINTILLATION_COMSOL,"This software provides a framework to perform 2D shape optimization of scintillation crystals parameterized through splines using FEM (finite-element-methods) solutions of transient and steady of electromagnetic waves with COMSOL and MATLAB to increase light collection. The geometry is based on the MTD-BTL (Minimum Ionizing Particle Detector - Barrel TIming Layer, https://cds.cern.ch/record/2667167/files/CMS-TDR-020.pdf).",https://data.4tu.nl/v3/datasets/2a150cdb-d1a6-4440-9b6e-f4ff89690b2f.git,
58bab2f3-7211-460c-b4ae-ecde564d785e,discuit,Discuit,"## About Discuit  Discuit is a **D**ynamic **i**tem **s**et **c**lustering **UI** **t**ool (with the UI part yet to come)  Discuit can split datasets (e.g. words defined by several variables) into subsets that are as comparable as possible.  The package takes a csv file as input and generates a defined number of matched sets for a given number of continuous and categorical variables. One of the categorical variables can be selected to be split absolutely even across sets. Discuit generates the following output: - csv file ammended with the set membership for each item - txt file that reports on the outcomes of statistics tests  In the future, this will be integrated in an GUI.   The project setup is documented in [project_setup.md](project_setup.md).  ## Installation  Discuit is available on PyPI!  ```bash pip install discuit ```  Or install the latest development version directly from GitHub:  ```console git clone git@github.com:doerte/discuit-project.git cd discuit-project python3 -m pip install . ```  <!-- ## Documentation  Include a link to your project's full documentation here. -->   ## Using Discuit In the terminal, run Discuit with the following command: python3 run_discuit.py ""name of input file"" [number of desired sets] --columns l/c/n/a/d --runs [desired number of runs]  Example: python3 discuit/run_discuit.py example/input.csv 2 --columns l a n c n d --runs 3  This will run Discuit with the [provided testfile](example/input.csv) and create 2 subsets. The columns in the file are identified as ""label"", ""categorical"", ""numerical"", ""absolute"", ""numerical"" and ""disregard"" (in that order). The program will run 3 times (and create 3 output files).  ### Required input The input file needs to be a .csv file with a first line containing headings followed by rows that represent the different items. Each column specifies one variable.   When launching the script, please specify per column what kind a data the script should expect:  - (l)abel: just a label, will not be taken into consideration, could be the itemname or itemnumber. This can only be assigned once.  - (n)umerical: a numerical variable, such as frequency or AoA,  - (c)ategorical: a categorical variable, such as ""transitivity"" or ""accuracy"",  - (a)bsolute: this needs to be perfectly divided between sets. This can only be assigned once.  - (d)isregard: a column that does not need to be taken into account for the split, but contains other information you have in the same file.  The package will try maximally 20 times to come-up with a good split. If it doesn't it will give up and output it's last try. You can always run it again. Often it will succeed eventually. If not, consider dropping variables.  If you run the script without specifying --columns, you will be asked what you want per column. If you don't specify the desired number of runs, it will generate 1 output file.  ### Missing data If you choose an ""absolute split variable"", this variable cannot have missing data. The program will exit if it does.  For categorical variables, a dummy category is created that holds the items with missing data. For numerical variables, missing data is replaced with the average of this variable. If you prefer a different approach, please prepare your input file in a way that does not include missing data.  ## Contributing  If you want to contribute to the development of Discuit, have a look at the [contribution guidelines](CONTRIBUTING.md).  ## Credits  This package was created with [Cookiecutter](https://github.com/audreyr/cookiecutter) and the [NLeSC/python-template](https://github.com/NLeSC/python-template).",https://github.com/doerte/discuit-project,doerte/discuit-project
58c32bb5-a7ef-4b65-8271-2d969b3affb3,4tu-software-underlying-the-article-modelling-the-cross-sectional-dynamics-of-tidal-sandbanks-in-sediment-scarce-conditions,Software underlying the article: Modelling the cross-sectional dynamics of tidal sandbanks in sediment-scarce conditions,"Our idealised morphodynamic numerical model simulates the temporal evolution of tidal sandbanks in sediment-scarce seas. The model solves the tide-topography interactions that drive sandbank evolution. The initial topography is a small sandbank in a shallow sea&nbsp;that is unaffected by coastal boundaries and where the sand supply is limited by the presence of a non-erodible layer. The growth and migration of this sandbank are simulated until a morphodynamic equilibrium is reached. The inputs are topographic, tidal, and numerical parameters, which are fully described in the README file. The outputs are the evolution of the bank topography over time and whether and equilibrium bank shape is reached. The topography of the last time step is the equilibrium profile (if one is reached). Importantly, our model is idealised, which means that it is meant to understand the dynamics of sandbank equilibria rather than predicting how a specific sandbank in a complex field setting will evolve. Furthermore, our model simulates the evolution of the one-dimensional cross-sections. It does not simulate variations in the along-bank direction. A full description of our methodology can be found in our journal paper (doi: 10.1029/2023JF007308).&nbsp;",,
58d20664-8139-4599-818a-2504cee1b3dd,4tu-code-and-data-accompanying-the-paper-entitled-seismic-fault-slip-behaviour-predicted-from-internal-microphysical-processes,"Code and data accompanying the paper entitled ""Seismic fault slip behaviour predicted from internal microphysical processes""","Improving the basis for earthquake hazard assessemt relies in part on structural observation of natural fault zones, on laboratory experiments, and on theoretical developments. While quantitative models that reproduce the laboratory data on frictional slip on faults is the minimum necessary to simulate their mechanical behavior in nature, convincing models must ultimately also account for all key fault zone observations. Taking a fault in carbonate rock as an example, and using fundamental data on the microscale processes that lead to fault friction, this work takes a first step towards simulating the dynamic evolution and self-organization of internal fault zone (micro)structure, processes and properties during a seismic slip event. The results show that the model captures many key observations on seismic fault behaviour, paving the way for still further improvements in developing a physics-based understanding of earthquake rupture in future. The present dataset includes the raw codes of four cases that are implemented in the finite element model (FEM) package Comsol (version 3.5a), and the numerical data produced by the codes (Results I to IV).",,
58d8ac8b-b62f-45fb-bc15-8d271c166e69,gammalearn,gammalearn,,https://gitlab.in2p3.fr/gammalearn/gammalearn,
595f2954-b36d-4c65-8e26-171739f4742a,4tu-openavem-open-aviation-emissions,openAVEM (Open Aviation Emissions),"openAVEM is a Python package that takes a list of flights as input and calculates fuel burn and atmospheric emissions: oxides of nitrogen (NOx),  hydrocarbons (HC), CO, non-volatile particulate matter (nvPM). Landing  and takeoff (LTO) is modeled with a time-in-mode approach, while the  non-LTO portions of flight (climb, cruise, descent) are modeled using  EUROCONTROL's BADA aircraft performance model (which needs to be acquired separately).",,
59654d9f-d401-4193-86f9-c6e125efb5f9,4tu-numpy-based-salt-flux-decomposition,NumPy-based Salt Flux Decomposition,"NumPy-based salt flux decomposition. Processing of three-dimensional hydrodynamic model data to decompose the salt flux in four components: salt flux related to (1) net flow, (2) tidal oscillation, (3) estuarine circulation, and (4) time-dependent shear. The decomposition is based on three variables: (1) flow velocity, (2) salinity, and (3) flow-normal cross-sectional area.",https://data.4tu.nl/v3/datasets/770aa2c0-f891-4b65-8925-5f474b937c22.git,
59695e32-b278-4d2c-90d1-489a60276e62,particledajl,ParticleDA.jl,,https://github.com/Team-RADDISH/ParticleDA.jl,Team-RADDISH/ParticleDA.jl
59a30aae-48fc-4d9d-a7cb-1faf75ff59b6,ufemism2,UFEMISM2.0,,https://github.com/IMAU-paleo/UFEMISM2.0/,IMAU-paleo/UFEMISM2.0
59e37175-3da9-4016-ad31-d189413863ec,4tu-code-belonging-to-the-publication-closed-loop-model-predictive-wind-farm-flow-control-under-time-varying-inflow-using-floridyn,Code belonging to the publication: Closed-loop model-predictive wind farm flow control under time-varying inflow using FLORIDyn,"FLORIDyn framework  The Flow Redirection and Induction Dynamics Model (FLORIDyn) is a dynamic wake model designed for model-based wind farm flow control.  The code is written for Matlab; for Python, see the FLORIDyn implementation in OFF (https://github.com/TUDelft-DataDrivenControl/OFF).     Model features  - Simulate wind farms dynamically at a low computational cost  - Estimate the power generated, added turbulence, and wake-induced losses.  - Apply heterogeneous and time-varying wind speeds and directions  - Test different modeling approaches     Control features  - Three families of yaw angle trajectory derivation methods with each different implementations  - Different cost functions (e.g., max energy, power, shifted energy)  - Switch flow field predictions on / off or choose a transition between full and no knowledge.     State estimation features  - Ensemble Kalman Filter flow field state estimation based on turbine measurements  - Correction of Wind speed, direction, and ambient turbulence intensity  - Different ways to calculate the Kalman Gain Matrix     Closed-loop application examples  -&nbsp;Demo files to apply closed-loop model-based wind farm flow control based on FLORIDyn in tandem with the LES SOWFA (https://github.com/TUDelft-DataDrivenControl/SOWFA)     Getting started  This code is folder-based. This means that a folder is referenced in the main.m and the code draws all information from that folder. Look under â€žSimulationsâ€œ to find example cases. Each case consists of data (e.g., wind speeds), parameters for FLORIS and FLORIDyn, a setup.mlx, and information about the turbine placement in turbineArrayProperties.m. The closed-loop-cases also have a clc_settings.mlx file and the Ensemble Kalman Filter settings.     To change settings, investigate setup.mlx, the file contains all major settings and explanations. The same holds true for the clc_stettings.mlx and EnKF_settings.m  To set up your own case, copy past an existing one and replace what is needed. If you select data input sources that donâ€™t have a related .csv file, the code will generate one for you in the fitting format.     Requirements  The code is based on MATLAB and tested in R2023a. The code uses the optimization toolbox, as well as the parallelization toolbox by MathWorks.     References  FLORIDyn - A dynamic and flexible framework for real-time wind farm control, M. Becker, D. Allaerts, J.W. van Wingerden, 2022, doi: 10.1088/1742-6596/2265/3/032103  Used FLORIS model:  Experimental and theoretical study of wind turbine wakes in yawed conditions, M. Bastankhah, F. PortÃ©-Agel, 2020, doi: 10.1017/jfm.2016.595  Additional references for smaller subcomponents can be found in the code or in the related publications.",https://data.4tu.nl/v3/datasets/89623e1a-5960-4b68-956a-7c664e3a5a76.git,
5a2baf7c-d196-420f-947d-0be611777082,4tu-data-and-code-underlying-chapter-4-of-the-phd-thesis-advanced-magnetocaloric-regenerators-for-heat-pump-applications,"Data and code underlying Chapter 4 of the PhD thesis ""Advanced Magnetocaloric Regenerators for Heat Pump Applications""",This dataset contains the results of numerical simulations performed to estimate the seasonal coefficient of performance (SCOP) of a magnetocaloric heat pump for the built environment. The SCOP is estimated based on the heating demand of a well-insulated house in the Netherlands. These results were published in the International Journal of Refrigeration 164 (2024) 38-48. The simulations were performed using a one-dimensional numerical model of an AMR implemented in Python.,https://data.4tu.nl/v3/datasets/042da9ba-6fe8-414f-9c40-1a152e831867.git,
5a56ea14-6ebd-4297-82a5-bbe5c2ce049f,pdbtbx-ts,pdbtbx-ts,* Provides a strict parser for Protein Data Bank files in JavaScript * Powered by pdbtbx rust library,https://github.com/i-VRESSE/pdbtbx-ts,i-VRESSE/pdbtbx-ts
5ac8ba2b-94f3-4ffd-be5a-a581a05b566e,4tu-code-used-to-generate-the-data-underlying-the-publication-a-monolithic-finite-element-formulation-for-the-hydroelastic-analysis-of-very-large-floating-structures,Code used to generate the data underlying the publication: A monolithic Finite Element formulation for the hydroelastic analysis of Very Large Floating Structures,"This repository contains all the scripts and underlying Finite Element code that have been used to generate the data shown in the manuscript: Â A monolithic Finite Element formulation for the hydroelastic analysis of Very Large Floating Structures by Oriol ColomÃ©s, Francesc Verdugo and Ido Akkerman",,
5b0d7dbd-c5ae-4b4c-9a66-d8e7ad69798b,eigencuda,Eigencuda,"* Simple interface to copy matrices to GPU * Automatic conversion to Eigen matrix type   Taking advantage of accelerators like GPU is fundamental in modern HPC applications. This library offers a simple interface to perform matrix-matrix multiplication in GPU for the C++ Eigen library. This library uses the same notation and operators that Eigen, automatically handling the GPU memory management.",https://github.com/NLESC-JCER/EigenCuda,NLESC-JCER/EigenCuda
5b95b1a5-914a-4f2e-b02f-4f1141d75d01,4tu-fracsim2d-a-new-methodology-for-generating-geological-constrained-2d-discrete-fracture-networks,FracSim2D: A new methodology for generating geological constrained 2D discrete fracture networks,"This folder contains the python code (FracSim2D) and data presented in chapter 6 of the Thesis: Natural Fracture Network Characterization: Numerical Modelling, Outcrop Analysis and Subsurface data (ISBN 978-94-6366-320-5) written by Q.D. Boersma in 2020. The thesis can be found on https://repository.tudelft.nl/. An installation and user guide can aslo be found within this download.   FracSim2D is an open source python code which is able to simulate 2D Discrete Fracture Networks (DFN) based on the fracture characteristics commonly observed on outcrops, including hierarchy, topology, spacing along with different length - and orientation distributions. The code is exemplified using natural fracture data acquired from four different pavement outcrops, which show distinctly different fracture network geometries. The results show that by implementing controls such as: 1) different fracture levels, 2) hierarchical behaviour, 3) topological rules, and 4) a strict fracture spacing relationship, this algorithm is able to simulate the network geometries which show a close resemblance to their outcrop counterparts, especially with respect to simulations which only account for the statistical observations.  The code has primarily been developed by Nico Hardebol and some results have been presented in: Multiscale fracture network characterization and impact on flow: A case study on the Latemar carbonate platform.",,
5bba04e3-1c81-4a89-9a04-725b03630e00,2021-romanowska-001,Pedestrian random walk in NetLogo (ch2.1),"![Interface screenshot](netlogo_implementation/documentation/ch2.1_pedestrian%20interface.png)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |(initial) xcor, ycor|float (absolute position) or integer (patch coordinate)|initial position of the agent (turtle) as x and y coordinates.| |walk (name tag of the method to select)|string|this input is an intermediate reference to the procedures that perform specific variations of random walk. Therefore, it can be bypassed as an input. Note - there are some values hard-coded inside each procedure that could be used as parameters or variable inputs.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |(current) xcor, ycor|float (absolute position) or integer (patch coordinate)|Current position of the agent (turtle) as x and y coordinates.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2021-Romanowska-001,
5bd3b0a1-e741-4499-8792-6f234d3b3ebf,4tu-aircraftductedfanoptimisation,AircraftDuctedFanOptimisation,"This dataset contains the codebase developed for the Unified Ducted fan Code (UDC) and its implementation into a ducted fan optimisation framework using the Unified Non-dominated Sorting Genetic Algorithm III (U-NSGA-III), as part of the MSc thesis titled ""A Framework for Medium-Fidelity Ducted Fan Design Optimisation"" by T.S. Vermeulen at Delft University of Technology, Faculty of Aerospace Engineering.     Note that the MTFLOW software used as the foundation for this codebase is not included, as it is licensed. Free academic licenses are available from the Technology Licensing Office at MIT.      Please refer to the README of the repository for details on installation and the required dependencies.",https://data.4tu.nl/v3/datasets/c4e01eaa-706d-4562-8a25-8734746d5fad.git,
5bdb045e-55ad-461a-a770-e8329b78d27f,qmflows,QMflows,"* Automation of quantum chemistry simulations  for  scientists *  Input generation, molecular manipulation, tasks execution and data extraction for  quantum chemistry simulations * Interoperable between different quantum chemistry software     This library tackles the construction and efficient execution of computational chemistry workflows. This allows researchers to use massively parallel compute environments in an easy manner and focus on interpretation of scientific data rather than on tedious job submission procedures and manual data processing.",https://github.com/SCM-NV/qmflows,SCM-NV/qmflows
5caa3a21-9301-47bf-aa2f-342c7a685d85,mibiscreen,mibiscreen,A Python package for prediction using coupled simulations and analysis in Microbiome based Remediation. Developed as part of the MiBiRem toolbox for Bioremediation in the Mibirem project.,https://github.com/MiBiPreT/mibiscreen,MiBiPreT/mibiscreen
5cacd3c4-9fe8-460b-a54e-deede66f22b7,tulipaenergymodel,TulipaEnergyModel,"This package provides an optimization model for the electricity market and its coupling with other energy sectors (e.g., hydrogen, heat, natural gas, etc.). The main objective is to determine the optimal investment and operation decisions for different types of assets (e.g., producers, consumers, conversions, storages, and transports).",https://github.com/TulipaEnergy/TulipaEnergyModel.jl,TulipaEnergy/TulipaEnergyModel.jl
5cf0e0d1-8433-4216-8b51-7ba21fd70d21,4tu-annesi-an-open-source-artificial-neural-network-for-estuarine-salt-intrusion,ANNESI: An open-source artificial neural network for estuarine salt intrusion,An open-source artificial neural network for estuarine salt intrusion trained with process-based model simulations using the state-of-the-art hydrodynamic modelling software Delft3D Flexible Mesh.,,
5d1bf3fe-869e-4828-8440-21cba524575d,4tu-code-and-test-data-set-underlying-the-publication-characterizing-single-molecule-dynamics-of-viral-rna-dependent-rna-polymerases-with-a-multiplexed-magnetic-tweezers,Code and test data set underlying the publication: Characterizing single-molecule dynamics of viral RNA-dependent RNA polymerases with a multiplexed magnetic tweezers,"Code and test data set for the dwell time analysis of enterovirus RNA-dependent RNA polymerases using multiplexed magnetic tweezers, to investigate snap-back synthesis and recombination.           This script constructs a dwell time distribution of tanslocating polymerases, optimized for data acquired with Magnetic or Optical Tweezers. The main function is called . All other files are different function upon which depends; they should be in the same directory.       Additionally, the workshop plans for our non-commercial flow-cell holder are attached.",,
5da98210-b3b1-4077-8c9b-44bac001c0cf,svnecr,svnecR,,https://github.com/japhir/snvecR,japhir/snvecR
5e0e8573-a1bc-4ec1-857e-dbdee3a35e8b,4tu-g-code-generation-python-script,G-code generation Python script,"Python script intended for use in app framework ""Streamlit"".   Can be used to generate g-code for the ""Hyrel Engine HR"" 3D printer, which in the associated (to be published) work, is used to deposit liquid crystal based inks.",,
5e368530-df7b-4c71-be5c-64599cadef9c,texcavator,Texcavator,"* Create word cloud and time line visualizations of large text corpora * Download search results for offline processing * Based on Elasticsearch * Used for educational purposes  This project is no longer maintained and has been succeeded by I-Analyzer, which is actively developed: https://github.com/UUDigitalHumanitieslab/I-analyzer.",https://github.com/UUDigitalHumanitieslab/texcavator,UUDigitalHumanitieslab/texcavator
5e78537c-1eaf-499a-b9fd-be8e79ac3dfa,pychelator,PyChelator,"# PyChelator v1.1: a Python-based Colab and web application for metal chelator calculations  ## Description: Pychelator is a helpful tool for researchers, scientists, and professionals working with metal-chelator interactions who need to obtain critical data related to these complexes. PyChelator Colab offers the Python version and extensive new features of the well-established <a href=""https://owamoosa.com/maxchelator/"" target=""_blank"">Maxchelator</a> developed by Chris Patton and colleagues [1]. The JavaScript version is served in Github Pages <a href=""https://amrutelab.github.io/PyChelator/"" target=""_blank"">PyChelator Web</a>.  ## Features: - Calculation of Metal Chelator Complexes: PyChelator performs complex calculations involving chelators and metal ions, taking into account essential environmental parameters like pH, temperature, and ionic strength (the left panel).  - Selection of constants: Easily select the constants to be used from the dropdown menu located in the left panel. Available stability constants are sourced from the National Institute of Standards and Technology â€˜NISTâ€™ [2], â€˜SPECSâ€™ by Fabiato [3], â€˜Chelatorâ€™ by Schoenmakers et al. [4], and â€˜Calciumâ€™ by FÃ¶hr et al. [5]. Users can also manually input the constants. The download feature allows users to obtain a local JSON file containing the entries, facilitating the subsequent use by uploading the constants. Equipped with input fields for metal and chelator names, a versatile use of the calculator is allowed for a broader range of applications.  - Selection of input units: Convenient entry of chelator and metal concentrations in units ranging from Molar (M) to nano Molar range (M, mM, Î¼M, and nM) (the left panel).  - Ionic strength: Ionic strength as equivalence of ions can be calculated inside PyChelator  - Purity of chelators: Percent purity of chelators can be entered in the top panel  - Structured Output: The output are structured for selectability. An extra option for the logarithmic transformation of free metal concentration (-log10[free]) is incorporated. The subsequent calculations are appended to the middle panel and can be conveniently downloaded as a single Excel file.  - Arbitrary precision arithmetic: The built-in Python Decimal module in Colab offers user-defined precision in the decimal place calculations  ## Flexibility and Customization: Using the Python-based Colab, users can utilize further analysis and customization.  Users are encouraged to modify the Python script according to their unique requirements, enabling them to integrate PyChelator with their existing Python-based workflows and projects.  ## Compatibility: PyChelator is designed to work with modern web browsers and supports JavaScript-based web applications. It is versatile and can be integrated into various web projects related to metal-chelator interactions.  ## Citation: Spahiu, E., Kastrati, E. & Amrute-Nayak, M. PyChelator: a Python-based Colab and web application for metal chelator calculations. BMC Bioinformatics 25, 239 (2024). <a href=""https://doi.org/10.1186/s12859-024-05858-8"" target=""_blank"">https://doi.org/10.1186/s12859-024-05858-8</a>    ## Disclaimer: While PyChelator aims to provide accurate and useful data, users are advised to validate the results obtained from the code with experimental data and consult the experts for critical applications.    ## References:      1. Bers DM, Patton CW, Nuccitelli R. A practical guide to the preparation of Ca(2+) buffers. Methods Cell Biol. 2010;99:1â€“26.      2. Smith RM, Martell AE. NIST Critically Selected Stability Constants of Metal Complexes Database Version 8.0. 2004.      3. Bers DM, Patton CW, Nuccitelli R. A practical guide to the preparation of Ca(2+) buffers. Methods Cell Biol. 2010;99:1â€“26.      4. Fabiato A. [31] Computer programs for calculating total from specified free or free from specified total ionic concentrations in aqueous solutions containing multiple metals and ligands. In: Methods in Enzymology. Academic Press; 1988. p. 378â€“417.      5. Schoenmakers TJM, Visser GJ, Flik G, Theuvenet APR. CHELATOR: an improved method for computing metal ion concentrations in physiological solutions. 1992.      6. FÃ¶hr KJ, Warchol W, Gratzl M. Calculation and control of free divalent cations in solutions used for membrane fusion studies. In: Methods in Enzymology. Academic Press; 1993. p. 149â€“57.",https://github.com/AmruteLab/PyChelator,AmruteLab/PyChelator
5e8dfa10-2aeb-418b-8090-fe7e76777a97,4tu-software-related-to-impact-of-optical-aberrations-on-axial-position-determination-by-photometry,"Software related to ""Impact of optical aberrations on axial position determination by photometry""","Software distributionThis software is distributed as accompanying software for the article Impact of optical aberrations on axial position determination by photometry by Rasmus Ã˜. Thorsen, Christiaan N. Hulleman, Mathias Hammer, David GrÃ¼nwald, Sjoerd Stallinga and Bernd Rieger. This distribution contains Matlab software to run the algorithms described in the article. MatlabThe provided scripts use Matlab (https://mathworks.com) and have been tested working in version R2017b. The scripts uses help functions from the DIPimage Toolbox and must be installed to make full use of the functions. DIPimage is a freely available image processing toolbox for Matlab (http://www.diplib.org). In the directory matlabfun all relevant Matlab functions are included. There are three examples showcasing different computations: example1.m, shows the estimated photon count for a measured bead by a fully-fledged vectorial or simplified Gaussian PSF model compared to TRABI, example2.m, computes a vectorial through-focus PSF and the corresponding photometric ratio (Gaussian fit over TRABI value) as a function of the axial position, example3.m, computes the photometric ratio over six bead measurements. These examples depict part of the data shown in the figure from the article. We hope that these examples are instructive enough to allow the interested user to apply our code. If you have any troubles please to not hesitate to contact us at the email address given below. Data availabilityAdditional data is available for download at: https://doi.org/10.4121/uuid:557b6445-5d40-402a-b214-93d7c6415195. Terms of use Copyright (C) 2018 This program is free software: you can redistribute it and/or modify it under the terms of the GNU General Public License as published by the Free Software Foundation, either version 3 of the License, or (at your option) any later version. This program is distributed in the hope that it will be useful, but WITHOUT ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the GNU General Public License for more details (https://www.gnu.org). Quantitative Imaging GroupFaculty of Applied SciencesDelft University of TechnologyLorentzweg 1, 2628 CJ DelftThe Netherlandscontact: Bernd Rieger, b.rieger@tudelft.nl",,
5ed8a384-095f-449e-b57f-86ec13028c75,root,ROOT,"ROOT is a unified software package for the storage, processing, and analysis of scientific data: from its acquisition to the final visualization in form of highly customizable, publication-ready plots. It is reliable, performant and well supported, easy to use and obtain, and strives to maximize the quantity and impact of scientific results obtained per unit cost, both of human effort and computing resources.  ROOT provides a very efficient storage system for data models, that demonstrated to scale at the Large Hadron Collider experiments: Exabytes of scientific data are written in columnar ROOT format. ROOT comes with histogramming capabilities in an arbitrary number of dimensions, curve fitting, statistical modelling, minimization, to allow the easy setup of a data analysis system that can query and process the data interactively or in batch mode, as well as a general parallel processing framework, RDataFrame, that can considerably speed up an analysis, taking full advantage of multi-core and distributed systems.  ROOT is performance critical software written in C++ and enables rapid prototyping powered by a unique C++ compliant interpreter called Cling. Cling also enables performant C++ type introspection which is a building block of automatic interoperability with Python. Thanks to PyROOT, leveraging the cppyy technology, ROOT offers efficient, on-demand C++/Python interoperability in a uniform cross-language execution environment.  ROOT fully embraces open-source, it's made with passion by its community, for the benefit of its community.",https://github.com/root-project/root,root-project/root
5eff6299-9fac-43b9-b137-7cf3c9017b10,frbcat-web,frbcat-web,* Provides a web frontend to the Fast Radio Burst (FRB) Catalogue. * Enables to quickly browse and filter the catalogue of FRBs. * No need to write your own interface to the database. * The [Fast Radio Bursts (FRBs) catalog](http://www.frbcat.org) is used by many scientists.  frbcat-web provides the web frontend to the FRB Catalogue.,https://github.com/AA-ALERT/frbcat-web,AA-ALERT/frbcat-web
5f29654d-2b18-411a-8fbb-1fa1d877fcef,qmctorch,QMCTorch,"* Easily use and implement new neural network ansatz * Use ADF or pySCF as SCF backend * Use Horovod to deploy on GPU clusters  In QMCTorch the trial wave function is calculated by  a small, physically motivated network. Starting from the electronic positions, R, the first layer computes the values of all atomic orbitals for all electrons. From there a linear map computes the values of all relevant molecular orbitals. A Slater Pooling mask is then applied to compute all Slater determinants that are finally combined by a fully connected layer. The Jastrow factors are computed in parallel  and   combined with the CI expansion to obtain the value of the wave function",https://github.com/NLESC-JCER/QMCTorch,NLESC-JCER/QMCTorch
5fbb8826-b710-47d4-894d-eb72c358ce2a,compass-school,Compass-school,* An agent-based model for primary-school choice * empirically realistic -- used to simulate the city of Amsterdam * can model strongly non-linear effects in for instance segregation,https://github.com/ODISSEI-School-Choice/school-choice,ODISSEI-School-Choice/school-choice
606cddde-dade-4425-b1b8-b327dc44d779,byteparsing,byteparsing,- Read binary OpenFOAM files - Uses memory mapping for efficient access - Parser/combinator library for writing mixed ascii/binary parsers  The main aim of this package is to read and alter OpenFOAM binary files. These files are a mixed form of a ASCII based C syntax with binary blobs of data intermixed. Byteparsing uses memory mapping for efficient access with Numpy. Byteparsing builds on a parser-combinator stack that can be adapted for other file types as well. An example is included for PLY (3D polygon data) files.,https://github.com/parallelwindfarms/byteparsing,parallelwindfarms/byteparsing
614356f0-4e26-4a44-b4c3-0fd56183c8ec,slurm-mongo,slurm-mongo,,https://github.com/neelravi/slurm-mongo,neelravi/slurm-mongo
627c77e4-225d-45c1-97e1-89ef85e1aeee,c3s-magic-wps,c3s-magic-wps,"* Provides a Web Processing Service for climate data analysis using ESMValTool. * The software in this WPS powers the processes behind the C3S-MAGIC portal. * The processes can be run on any compute resource with access to CP4CDS CMIP data.  Web Processing Service for Climate Data Analysis in the MAGIC project. The software in this WPS powers the processes behind the C3S-MAGIC portal. The processes can be run on any compute resource with access to CP4CDS CMIP data, and are typically run on CP4CDS infrastructure.",https://github.com/c3s-magic/c3s-magic-wps,c3s-magic/c3s-magic-wps
62f8e0c3-1980-443c-a9b6-4580b237449a,geodisy,Geodisy,"# What is Geodisy?  **Find research data visually, spatially and quickly** Research data can be hard to find, and even harder if you're researching a specific place. Geodisy changes that, giving you a window into the world of research data with map-based tools familiar to everyone. Search by place name, or by drawing a box. The world of research data is yours to discover. Geodisy is the software that will let you do just that. Who will use Geodisy?  **People** Anyone looking for research data who wants to use a map or a box to find data. Researchers, students, journalists, and anyone else with an interest in data from university research will benefit from Geodisy's search tools.  **Institutions** The first users of Geodisy will be FRDR, Canada's Federated Research Data Repository, for the quick and easy discovery of Canadian Research Data. When released, anyone with the available infrastructure will be able to plug in Geodisy and make a compatible repository more discoverable. Why use Geodisy?  Data, and in particular research data, has always been difficult to find. Keywords can be hit and miss, and text based descriptions don't show you where your place of interest lies. Geodisy changes that, showing you where, not just what.  If you're a running a research data repository based on Dataverse, Geodisy will take your repository's data, search for geospatial metadata and files, and copy them to a new system which allows for visual searching. Your original data and search methods are untouched; you have the benefit of both. How does Geodisy work?  Geodisy is a separate server software component that examines the metadata, such as the study records for research data and any associated data. If Geodisy finds spatial data, metadata and data are harvested, then normalized to have the same geospatial metadata standard. Afterwards, both data and metadata are injected into a geospatial data server and a viewer/search component.",https://github.com/ubc-library/geodisy,ubc-library/geodisy
62fdccb8-e223-433d-9f1a-49af70bb9caf,4tu-implementation-of-approximate-sdd-tmpc-state-dependent-dynamic-tube-model-predictive-control-for-a-nonlinear-problem,Implementation of approximate SDD-TMPC (state-dependent dynamic tube model predictive control) for a nonlinear problem,"The code in this repository is used to implement and test an approximation of a new algorithm called SDD-TMPC. It is a control algorithm that sacrifices some optimality (much less than tube MPC) but provides robust solutions (more about this can be found in [1]). The original controller presented in [1] proved to work and showed good performance, but solving the optimality problem took too long to implement it for a real robot. To solve this problem, we used a spiking neural network to approximate SDD-TMPC.  &nbsp;  First, we generated data using Matlab's main script. It is a function that takes a seed as input and randomly samples the state of the robot in an environment as it works. The environment is a circular environment with 1 meter radius and a wall around it. Then it tries to solve the optimization problem, i.e. find an optimal input such that a cost function is minimized, a robot does not collide with the wall, and the terminal constraints are satisfied. Once the optimization problem is solved, a new file is created, and the sampled data is saved with the additional data.  &nbsp;  It is possible that no solution was found. In such a case, a vector full of zeros will be saved. It is worth noting that it takes a lot of time to generate each data sample. According to our research, it is important to generate the data as close to the global optimum as possible, because outliers make training the network much more difficult. Therefore, we decided to spend a lot of time generating each data sample.  &nbsp;  &nbsp;  All generated data were saved in two files ""final_data.csv"" and ""final_data.xlsx"". The former contains the state (only 2 columns, there is no saved direction because it is assumed that the coordinate frame is rotated so that the direction is always 0), optimal input (2 columns - linear and angular velocity), nominal input (2 columns - linear and angular velocity) and nominal error (3 columns - 2-dimensional position and direction). The nominal velocity and nominal error can be used to calculate the optimal input (see [2] for details), i.e. to check if there is no problem with the data. The file ""final_data.xlsx"" also contains an additional column with seeds to make the data reproducible.  &nbsp;  The next step is to use ""network_traning.ipynb"" to train the networks. After running this file, the files ""linear_final.pt"" and ""angular_final.pt"" are created. These files contain the trained spiking neural networks that can be used instead of SDD-TMPC. The ""network_tester.ipynb"" file is used to compare the behavior between SDD-TMPC and its approximation. After execution, figures are generated.  &nbsp;  Finally, to test the behavior of the system, we created a ROS2 package to control a wheeled robot.  &nbsp;",https://data.4tu.nl/v3/datasets/7cd3aa1b-4cf0-46c8-87a6-17190998dcd6.git,
630119b1-517b-4ce7-b08e-973958e54c15,4tu-longterm-datasets,longterm_datasets,"Many real-world applications, from sport analysis to surveillance, benefit from automatic long-term action recognition. In the current deep learning paradigm for automatic action recognition, it is imperative that models are trained and tested on datasets and tasks that evaluate if such models actually learn and reason over long-term information. In this work, we propose a method to evaluate how suitable a video dataset is to evaluate models for long-term action recognition. To this end, we define a long-term action as excluding all the videos that can be correctly recognized using solely short-term information. We test this definition on existing long-term classification tasks on three popular real-world datasets, namely Breakfast, CrossTask and LVU, to determine if these datasets are truly evaluating long-term recognition. Our method involves conducting user studies, where we ask humans to annotate videos from these datasets. Our study reveals that these datasets can be effectively solved using shortcuts based on short-term information. In this repository, we provide the code and data. The code includes the HTML files for the user studies and the data analysis. The data includes the input to the user studies (e.g., video urls) and the responses collected on Amazon Mechanical Turk.",https://data.4tu.nl/v3/datasets/6bcbfb60-7eb0-4380-9690-26e6f3b2f174.git,
635839f1-58a0-42eb-b490-8ff81fdcd12b,structure-from-motion,Structure from Motion,- pipeline to construct 3D point clouds from a set of 2D images,https://github.com/NLeSC/structure-from-motion,NLeSC/structure-from-motion
63aace5a-9d63-4001-9583-36b2c1b9abdb,scholia,Scholia,"[Scholia](https://scholia.toolforge.org/) is a service that creates visual scholarly profiles for topics, people,  organizations, species, chemicals, etc using bibliographic and other information in [Wikidata](https://wikidata.org/).  # Finding literature  Scholia has topic pages and lists the scientific literature about this topic. This [tutorial](https://laurendupuis.github.io/Scholia_tutorial/) explains how it can be used.",https://github.com/WDscholia/scholia,WDscholia/scholia
63e1d652-6cb7-45be-8fe7-56914beceb5e,4tu-code-to-incorporate-entrapped-air-pocket-dynamics-into-swmm-airswmm2-0,Code to incorporate entrapped air pocket dynamics into SWMM (AirSWMM2.0),"This code has the implementation of a novel methodology to locate and quantify entrapped air pockets created during pipe-filling events often found in intermittent water supply systems. A new numerical model capable of simulating the air pocket creation, dragging and entrainment has been proposed, AirSWMM, was implemented as an extension of the Stormwater Management Model (SWMM) with stochasticity of air pocket formation reproduced by simulations with different air entrainment rates. The obtained numerical results show that the proposed model, even though based on a single-phase one-dimensional flow, can accurately locate and approximately quantify the entrapped air pocket volumes.",https://data.4tu.nl/v3/datasets/bb8757f0-2cb5-4822-8c2c-ec889e2a231b.git,
63f3139d-5b7e-46e5-bf74-890cf1e6ab81,otree-development-environment,oTree Development Environment,,https://github.com/obeliss-nlesc/otree-docker,obeliss-nlesc/otree-docker
640df270-8ad7-4a5f-843c-9e2a0225e670,4tu-code-for-flowprint-semi-supervised-mobile-app-fingerprinting-on-encrypted-network-traffic, Code for FlowPrint: Semi-Supervised Mobile-App Fingerprinting on Encrypted Network Traffic,"This repository contains the code for FlowPrint by the authors of the NDSS FlowPrint paper [PDF]. Please cite FlowPrint when using it in academic publications. This repository provides a stable artefact version of FlowPrint. For the most up-to-date version that receives updates, please see the Github repository.",https://data.4tu.nl/v3/datasets/53eca195-a7dc-4c53-9477-5afaa7b1a957.git,
644a42f4-841f-4fbb-89af-1e5991e1cfe4,4tu-code-for-ion-sensing-based-on-frequency-dependent-physico-chemical-processes-at-electrode-electrolyte-interfaces,Code for: Ion Sensing based on Frequency-Dependent Physico-Chemical Processes at Electrode/Electrolyte Interfaces,"This repository contains all MATLAB and Python codes developed for the studyIon Sensing based on Frequency-Dependent Physico-Chemical Processes at Electrode/Electrolyte Interfaces, including:  Fundamental impedance model â€“ MATLAB implementation of the continuum-based model describing frequency-dependent physico-chemical processes at electrode/electrolyte interfaces.Machine learning workflow â€“ Python scripts for data preprocessing, model training, cross-validation, and prediction of ion composition/concentration from EIS data.",,
648a7647-5413-4072-a4cc-fdb9bd25995d,4tu-mathematica-files-for-the-paper-on-the-existence-of-derivations-as-square-roots-of-generators-of-state-symmetric-quantum-markov-semigroups,Mathematica files for the paper 'On the existence of derivations as square roots of generators of state-symmetric quantum Markov semigroups',The mathematica files used to analyse the existence of derivations for the paper 'On the existence of derivations as square roots of generators of state-symmetric quantum Markov semigroups'. Each file corresponds to one example in the paper.,,
64c448e5-ea8a-4d75-91ba-b7c00c8e1df6,4tu-quality-diversity-sparse-data,quality_diversity_sparse_data,"This repository is part of the Ph.D. thesis of&nbsp;Isabelle M. van Schilt, Delft University of Technology.    This repository is used to calibrate the underlying structure and parameters of a stylized supply chain simulation model of counterfeit Personal Protective Equipment (PPE) using the quality diversity algorithm. For this, we use the&nbsp;pyribs&nbsp;library for the quality diversity algorithm, and&nbsp;pydsol-core&nbsp;and&nbsp;pydsol-model&nbsp;for the discrete event simulation model. The calibration is done with sparse data, which is generated by degrading the ground truth data on noise, bias, and missing values. We define the structure of a supply chain simulation model as a key value of a dictionary (sorted on graph density), which is a set of possible supply chain models. The integer is, thus, a decision variable of the calibration, next to other parameters in the simulation model.    To use this repository, we need a simulation model developed in&nbsp;pydsol-core&nbsp;and&nbsp;pydsol-model&nbsp;. Additionally, we need a dictionary with various different simulation structures as input, as well as the ground truth data. For this project, we use the repository&nbsp;complex_stylized_supply_chain_model_generator&nbsp;as simulation model.",https://data.4tu.nl/v3/datasets/72b71d3f-a404-46c3-9c73-60f9b677db62.git,
64e7dcf7-62d3-4162-8cad-8dbfce15cbd7,4tu-snapwf,SNAPWF,"We introduce an automated Python workflow, SNAPWF, using SNAP-ESA that enables efficient PSI and SBAS InSAR interferometric time series stacks generation using flexible network graphs. The new workflow has been implemented on a dedicated geospatial computing platform, enabling efficient performance over large areas",https://data.4tu.nl/v3/datasets/d4574db9-d365-4f9f-856f-9e609fa2e540.git,
656a4440-0784-4e70-9cef-daa8b155cfb7,4tu-gnss-precise-point-positioning-ppp-matlab-toolbox,GNSS Precise Point Positioning (PPP) Matlab Toolbox,"GNSS-PPP Matlab toolbox with functions and example scripts to read data from NRCan CSRS-PPP output files and combine the single day solutions into a single multi-day solution with statistical testing. Applications range from computing multiday station coordinates, with statistical testing, up to quality control for a multi-week GNSS campaign with automatic outlier detection and powerful graphics.      The most recent version can be found on github (https://github.com/hvandermarel/gnss-ppp-matlab-toolbox ).",https://data.4tu.nl/v3/datasets/1d455ba6-780c-4782-b0c9-01c4be8114a8.git,
658b0723-1fa2-4c3e-957e-37f112c72266,platalea,Platalea,* Provides dataloaders and models for researchers in speech acquistion * Trains models and contains a number of experiments used in publications that can be reproduced and modified,https://github.com/spokenlanguage/platalea,spokenlanguage/platalea
669146c0-d744-499b-8bc6-6074524873b4,4tu-data-and-model-of-scale-down-simulator-for-industrial-syngas-fermentation,Data and Model of Scale Down Simulator for Industrial Syngas Fermentation,"The data and MATLAB code are provided for the development of the scale-down simulator of industrial-scale syngas fermentation.&nbsp;  The CFD-derived residence time probablity distributions are required for the development and testing the results of the scale-down simulator. For use, they should be in the mentioned folder in the MATLAB file.&nbsp;",,
67790c78-8a2d-45a2-aaa5-79c9088bbfe0,4tu-purcell-filter-simulation-scripts,Purcell filter simulation scripts,"Script to numerically simulate the photon levels in readout-resonators and Purcell-filters of transmon devices. From this, the SNR and thus fidelity of the readout process as a function of time can be determined.",,
67c7547b-030b-4ea1-a027-0521149b60b3,4tu-tillage-controlled-runoff-pattern-model-tcrp-v1-1-a-gis-model-to-adjust-runoff-patterns-for-tillage-induced-roughness-in-agricultural-catchments,Tillage-controlled Runoff Pattern model (TCRP) v1.1; a GIS model to adjust runoff patterns for tillage induced roughness in agricultural catchments,"The TCRP model can be used to create a runoff pattern, taking into account the effects of tillage-induced roughness. The TCRP model has as main aim to adjust  runoff patterns based on topography for tillage induced roughness. The resulting output of TCRP are runoff pattern raster maps, which can be further used as input  for hydrological modelling of surface runoff processes. The methodology to create the runoff pattern includes four steps. First, a topographically controlled runoff pattern is created using the standard single-flow, steepest descent algorithm. Next, a tillage-determined runoff pattern is created, i.e. a runoff pattern based on the assumption that water will always flow in the tillage direction. Then, decision rules are applied to determine for each cell whether the water will flow in tillage direction or in topographic direction. Finally, a flow direction map can be generated by combining the topographically determined runoff pattern with the tillage-controlled runoff pattern. This repository is an updated version of the original code, adjusted to run smoothly with the current version of PCRaster (v4.4.0)",,
689a50db-c3ba-477f-89eb-2d3d9a01a297,4tu-sbsl,SBSL,"This repository consists of code to reproduce the results of the paper ""Overcoming Selection Bias in Synthetic Lethality Prediction"".     The code to reproduce paper results is shared at: https://github.com/joanagoncalveslab/SBSL     No experimental data was collected as part of this project, this project only utilised previously publicly available datasets. Details on accessing this data is fully described in Section 2.1 of the published article.",,
689c89b2-0f63-49aa-a1be-72b46a84ec60,workflomics,Workflomics,"# Workflomics: A Workflow Benchmarking Web Platform   | Badges                   |                                                                                                                              | |:------------------------:|------------------------------------------------------------------------------------------------------------------------------| | **Packages and Releases** |  [![Latest release](https://img.shields.io/github/release/workflomics/workflomics-frontend.svg)](https://github.com/sanctuuary/APE/releases/latest) [![Static Badge](https://img.shields.io/badge/RSD-Workflomics-workflomics)](https://research-software-directory.org/software/workflomics) | | **Build Status** | [![Build Frontend](https://github.com/Workflomics/workflomics-frontend/actions/workflows/build-frontend.yaml/badge.svg)](https://github.com/Workflomics/workflomics-frontend/actions/workflows/build-frontend.yaml) | | **Documentation Status** | [![Documentation Status](https://readthedocs.org/projects/workflomics/badge/?version=latest)](https://workflomics.readthedocs.io/en/latest/?badge=latest) | | **DOI**                  | [![DOI](https://zenodo.org/badge/594054560.svg)](https://zenodo.org/doi/10.5281/zenodo.10047136)                             | | **License**              | ![GitHub](https://img.shields.io/github/license/workflomics/workflomics-frontend)                                           |   The **Workflomics** web platform aims to address the challenge faced by life science researchers who work with increasingly large and complex datasets and struggle to create optimal workflows for their data analysis problems.   The platform facilitates a ""Great Bake Off"" of computational workflows in bioinformatics by integrating bioinformatics tools and metadata with technologies for automated workflow exploration and benchmarking. This enables a systematic and rigorous approach to the development of cutting-edge workflows, specifically in the field of proteomics, to increase scientific quality, robustness, reproducibility, FAIRness, and maintainability.  The platform currently focuses on the proteomics domain. We aim to extend the platform to additional domains, e.g., metabolomics, genomics.  ### Visit the live demo: http://145.38.190.48/ Note: The website does not currently use HTTPS.   ## Architecture  The Workflomics web interface is part of a larger infrastructure that includes a Postgres database, a Postgrest API, a RESTful APE service, etc. The architecture is presented in the figure below:  ![architecture_workflomics](https://github.com/Workflomics/.github/blob/main/draw.io/architecture_simplified.png?raw=true)",https://github.com/workflomics/workflomics-frontend,workflomics/workflomics-frontend
68af674d-8532-4ad8-9231-d62f3856f15a,4tu-lap-orbit-maintenance,lap_orbit_maintenance,"This thesis investigates the feasibility of the usage of space-based Laser Ablation Propulsion (LAP) to perform orbit maintenance on CubeSat in Low Earth Orbit (LEO). The thesis is motivated by the growing need for efficient, compact propulsion systems for small satellites and explores LAP as a method that eliminates the need for onboard fuel by generating momentum via directed laser beams from orbital laser stations.  A detailed simulation framework is developed to model the physical environment in LEO, including atmospheric drag, solar radiation pressure, and third-body gravitational effects. The propagation of high-powered lasers through space is modeled considering diffraction, absorption, and current limits of aiming precision. The propulsion interaction between laser and CubeSat is simulated through a dynamic model based on material properties and laser beam parameters.  Power generation and energy storage are analyzed in the context of space-based constraints, comparing solar arrays, battery technologies, and their degradation over time. A financial model evaluates system-level trade-offs, including station cost, maintenance intervals, and replacement economics. Optimizations are performed to understand how propulsion efficiency, energy storage capacity, and environmental variability affect system performance and scalability.  Simulation results show that LAP is viable under current technological constraints, provided stations are deployed in sufficient numbers and at the right altitudes. The system can service a meaningful number of CubeSats while remaining competitive with the cost of satellite replacement. Technological limitations such as aiming accuracy and beam control are identified as key challenges, with recommendations for future work including the expansion of in-orbit LAP to networks, and further  applications.",https://data.4tu.nl/v3/datasets/784443cf-d65e-43d8-8cfc-0614e274c7cc.git,
68d4ffb6-ea0a-492c-b06a-f611abf773ee,ersilia-model-hub,Ersilia Model Hub,,https://github.com/ersilia-os/ersilia,ersilia-os/ersilia
68d81683-f4fd-439f-9941-fd19e3f5f963,4tu-gaussian-floridyn-matlab-implementation-belonging-to-the-paper-the-revised-floridyn-model-implementation-of-heterogeneous-flow-and-the-gaussian-wake,"Gaussian FLORIDyn, Matlab implementation belonging to the paper: The revised FLORIDyn model: Implementation of heterogeneous flow and the Gaussian wake","Dynamic implementation of the Gaussian FLORIS model     Software used to generate the results pubished in the paper The revised FLORIDyn model: Implementation of heterogeneous flow and the Gaussian wake byÂ Marcus Becker, Bastian Ritter, Bart Doekemeijer, Daan van der Hoek, Ulrich Konigorski, Dries Allaerts, and Jan-Willem van Wingerden.       In order to run the software use the main.m function or the FLORIDyn_App.mlapp application for an easy start. The GitHub page features an explanation of the code, along with the comments in the code. The theory is described in the referencede paper.",,
6921d621-5271-44e1-9905-8b349d8cf4ad,4tu-leeuwis-et-al-2024-castor-wheel-supplementary-information,leeuwis-et-al-2024-castor-wheel-supplementary-information,"Supplementary code and videos for publication ""A slanted castor wheel enables pushing manual wheelchairs from the side to improve social interaction"".  This repository contains the supplementary S2 files and supplementary videos S3-5.     S2 File. Experimental data and the MATLAB code for calculations of castor wheel.CastorModel  Contains Matlab code for the simulation of the castor wheel and the generation of figures comparing simulations to experimental results. This code relies on the functions folder. To run the code and generate the results figure presented in the paper, run the main.m file.     Data  Contains experimental data of the castor wheel on the treadmill, organized by collection date.     Functions  Contains functions used by CastorModel code.     Rolling resistance  Contains Matlab code to compute the rolling resistance of the castor wheel on the treadmill. The value of the computed rolling resistance is used in the CastorModel.     S3 Video. Video of the control wheelchair on the treadmill.The video contains the control conditions, where the wheelchair was pulled using a cable while rolling on a treadmill.     S4 Video. Video of the test wheelchair on the treadmill.The video contains the test condition, where the wheelchair with a slanted castor wheel was pulled using a cable attached to the push bar while rolling on a treadmill.     S5 Video. Video of the wheelchair during outdoor use with an occupant.The video demonstrates how a modified wheelchair could be used outdoors on an uneven path, allowing the occupant and caregiver to walk side by side.",https://data.4tu.nl/v3/datasets/5642d7b0-1803-41a6-881f-aa0b866795b0.git,
695ccbc8-cca3-476d-aa50-087b63e11601,cwltool,cwltool,"[![Linux Status](https://github.com/common-workflow-language/cwltool/actions/workflows/ci-tests.yml/badge.svg?branch=main)](https://github.com/common-workflow-language/cwltool/actions/workflows/ci-tests.yml) [![Coverage Status](https://img.shields.io/codecov/c/github/common-workflow-language/cwltool.svg)](https://codecov.io/gh/common-workflow-language/cwltool) [![Documentation Status](https://readthedocs.org/projects/cwltool/badge/?version=latest)](https://cwltool.readthedocs.io/en/latest/?badge=latest)  PyPI: [![PyPI Version](https://badge.fury.io/py/cwltool.svg)](https://badge.fury.io/py/cwltool) [![PyPI Downloads Month](https://pepy.tech/badge/cwltool/month)](https://pepy.tech/project/cwltool) [![Total PyPI Downloads](https://static.pepy.tech/personalized-badge/cwltool?period=total&units=international_system&left_color=black&right_color=orange&left_text=Total%20PyPI%20Downloads)](https://pepy.tech/project/cwltool)  Conda: [![Conda Version](https://anaconda.org/conda-forge/cwltool/badges/version.svg)](https://anaconda.org/conda-forge/cwltool) [![Conda Installs](https://anaconda.org/conda-forge/cwltool/badges/downloads.svg)](https://anaconda.org/conda-forge/cwltool)  Debian: [![Debian Testing package](https://badges.debian.net/badges/debian/testing/cwltool/version.svg)](https://packages.debian.org/testing/cwltool) [![Debian Stable package](https://badges.debian.net/badges/debian/stable/cwltool/version.svg)](https://packages.debian.org/stable/cwltool)  Quay.io (Docker): [![Quay.io Container](https://quay.io/repository/commonwl/cwltool/status)](https://quay.io/repository/commonwl/cwltool)  This is the reference implementation of the [Common Workflow Language open standards](https://www.commonwl.org/). It is intended to be feature complete and provide comprehensive validation of CWL files as well as provide other tools related to working with CWL.  cwltool is written and tested for [Python](https://www.python.org/) 3.x {x = 6, 7, 8, 9, 10, 11}  The reference implementation consists of two packages. The `cwltool` package is the primary Python module containing the reference implementation in the `cwltool` module and console executable by the same name.  The `cwlref-runner` package is optional and provides an additional entry point under the alias `cwl-runner`, which is the implementation-agnostic name for the default CWL interpreter installed on a host.  `cwltool` is provided by the CWL project, [a member project of Software Freedom Conservancy](https://sfconservancy.org/news/2018/apr/11/cwl-new-member-project/) and our [many contributors](https://github.com/common-workflow-language/cwltool/graphs/contributors).",https://github.com/common-workflow-language/cwltool,common-workflow-language/cwltool
6a182546-7ef7-45c7-8ff5-e9b415427872,4tu-machine-learning-based-classification-model-corresponding-to-the-paper-machine-learning-to-improve-orientation-estimation-in-sports-situations-challenging-for-inertial-sensor-use,Machine learning-based classification model corresponding to the paper 'Machine learning to improve orientation estimation in sports situations challenging for inertial sensor use',"Machine learning model as used in 'Machine learning to improve orientation estimation in sports situations challenging for inertial sensor use'. The model to run the Extended Madgwick filter is based on a random forest algorithm and can be used as explained in Figure 3 in the paper. Explanation for use:The model can be loaded and executed in Python using the following code- RFmodel = pickle.load(open([filename_RFmodel], 'rb'))- y = RFmodel.predict_proba(X)[:,1]- if y > 0.5     EFcorrect = 1 else     EFcorrect = 0 X are (normalized) input variablesy are probabilities for EFcorrect (see paper)",,
6a1a4a65-a49b-4dfb-b843-3041e2e6be8d,abinit,Abinit,"## What is ABINIT ? **ABINIT** is a software suite to calculate the optical, mechanical, vibrational, and other observable properties of materials. Starting from the quantum equations of **density functional theory**, you can build up to advanced applications with **perturbation theories** based on DFT, and **many-body Green's functions** (GW and DMFT).   **ABINIT** can calculate molecules, nanostructures and solids with any chemical composition, and comes with **several complete and robust tables of atomic potentials**. **On-line tutorials** are available for the main features of the code, and several schools and workshops are organized each year. -",https://github.com/abinit/abinit.git,abinit/abinit
6a379975-0bfd-4a7d-8ba1-c7f4b7598ce8,4tu-code-accompanying-the-paper-validating-human-driver-models-for-interaction-aware-automated-vehicle-controllers-a-human-approach,"Code accompanying the paper ""Validating human driver models for interaction-aware automated vehicle controllers: A human approachâ€","This python package contains scripts needed to train IRL Driver models on HighD datasets. This code is accompanying the paper ""Validating human driver models for interaction-aware automated vehicle controllers: A human factors approach - Siebinga, Zgonnikov & Abbink 2021"" and should be used in combination with TraViA, a program for traffic data visualization and annotation. A preprint of this paper can be found on arxiv: https://arxiv.org/abs/2109.13077",,
6a487ccd-207f-4b45-9eb3-93b4e8ec6e33,ibridges,iBridges,"**A tool to make it easier for scientists and scientific programmers to work with the data stored in iRODS and Yoda systems.**   iBridges works on all operating systems: Windows, Mac and Linux Prerequisites: You will still need a python compiler on your system and some basic knowledge how to install python packages, e.g. through Anaconda   **For Whom?** Scientific programmers and scientists who find the icommands and native APIs to iRODS/Yoda a bit too daunting, cumbersome or simply do not have time to learn them.   **The iBridges API box**   iBridges https://github.com/UtrechtUniversity/iBridges A python API for scientific programmers working in python to integrate their data handling into compute workflows A command line tool for moving data between local computers and Yoda/iRODS Tutorials guiding through how to work with data in Yoda/iRODS Some features: Safe data transfers from and to your computer, data synchronisation, tagging data with own search terms, searching for data   **iBridges and Yoda** iBridges is meant for being used when you are actively working with your data. It does not replace the Yoda web portal. That mean for annotating your data with the Yoda metadata and to move your data to the Vault you will need to log in to the web portal.   **iBridges and the icommands and the python API for iRODS** You do not have to decide between iBridges and the icommands or the python API of iRODS. Simply combine the functions of iBridges with the icommands or the iRODS python API, no need to login again with the other tools or create new connections to Yoda/iRODS.",https://github.com/UtrechtUniversity/iBridges,UtrechtUniversity/iBridges
6a7bfc7e-ba28-4b6a-8d17-898f63c03c32,4tu-script-that-produces-the-table-and-figures-for-pitfalls-of-statistical-methods-in-traffic-psychology,Script that produces the table and figures for: Pitfalls of Statistical Methods in Traffic Psychology,"Script that produces the table and figures for De Winter, J. C. F., & Dodou, D. (2021). Pitfalls of Statistical Methods in Traffic Psychology. Encyclopedia of Transportation.",,
6ae34e47-e942-4c49-b2ab-392d35f16cd8,forecasting-the-grid-emission-factor-for-the-netherlands,Forecasting the grid emission factor for the Netherlands,,https://github.com/ESI-FAR/emissionfactor-nl,ESI-FAR/emissionfactor-nl
6aebcaea-529c-41c3-8e28-1ecac34ebc65,frbcatdb,frbcatdb,* Provides a relational database interface to Virtual Observation Event (VOEvent) data. * Stores VOEvents into a relational database. * Makes VOEvent data more easily accessible for analysis. * Succesfully used as a backend for the [Fast Radio Bursts (FRBs) catalog](http://www.frbcat.org).  The frbcatdb is a database to store a catalog of Fast Radio Bursts (FRBs). The DB is intended to contain old FRB events as well as new FRBs detected by the AA-ALERT FRB detection pipeline from Apertif observations and also possible follow-up observations or others FRBs detected by other telescopes. The frbcatdb is attached to the VOEvent backbone and uses this infrastructure as its source.,https://github.com/AA-ALERT/frbcatdb,AA-ALERT/frbcatdb
6b0e0712-9632-480b-a3f2-5d02ec94c41c,timp,TIMP,,https://github.com/glotaran/TIMP,glotaran/TIMP
6b23733a-70e9-4beb-a8cf-e9acc8c7d9ea,glotaran,Glotaran,,https://github.com/glotaran/glotaran-legacy/,glotaran/glotaran-legacy
6b69c563-de9f-4587-9895-43e78668ad07,sliceraigt,SlicerIGT,"Image-guided therapy (IGT) stands forÂ medical procedures performed inside patients, where operators rely on computer-generated imagesÂ rather than direct sight of the target organs.Â We are supporting IGT research by maintaining a software kit for rapid development of IGT applications. This software kit is SlicerIGT.  # Main features **Builds on a dynamic platform** SlicerIGT is an extension of 3D Slicer, a free, open source software for visualization and image analysis. SlicerIGT can be installed from the 3D Slicer Extension Manager on Windows, Mac, and Linux to use all the advanced features of 3D Slicer for real-time navigation.  **Development without coding** Modules of SlicerIGT are designed so you can configure a procedure-specific application without programming, or with minimal scripting. We have configured SlicerIGT to support many procedures including brain surgery, urology, regional anesthesia, and moreâ€¦  **SlicerIGT is open** Our development and research work is public, including source code, data, experiment protocols, manuals, etc. SlicerIGT is distributed under the BSD-styleÂ Slicer licenseÂ allowing academic and commercial use without any restrictions.  # Service **SlicerAIGT:** Deep learning software modules for image-guided medical procedures. Includes tools for data collection, training, and deployment from 3D Slicer. [Source code](https://github.com/SlicerIGT/aigt)",https://github.com/SlicerIGT/SlicerIGT,SlicerIGT/SlicerIGT
6b7b9693-f690-4890-894a-05752bbc5159,h2-scenario-explorer,H2 scenario explorer,,https://github.com/ESI-FAR/h2-scenario-explorer/,ESI-FAR/h2-scenario-explorer
6b7f9325-0050-469b-8e05-85fb57455c28,4tu-specification-translator-tool-to-translate-specifications-for-deductive-verifiers,Specification Translator: Tool to Translate Specifications for Deductive Verifiers,"About the Specification Translator  The Specification Translator is a tool that has been implemented as part of our research titled ""Join Forces! Reusing Contracts for Deductive Verifiers through Automatic Translation"". This tool will translate specifications in verified Java programs from one specification language into another. It supports the tools Krakatoa, OpenJML and VerCors.     The tool takes an annotated Java program and a target tool as input. It will then generate an annotated Java program where the annotations have been translated.     What's included in this artifact?  This artifact contains the following things:  - specification-translator.zip: A directory containing the Specification Translator tool as well as the examples used for the evaluation.  - SpecTranslatorArtifact.ova: A Virtual Machine with the Specification Translator tool, as well as the OpenJML and VerCors verifiers. It also contains the examples used for the evaluation and a script to reproduce the evaluation.     You can use the Virtual Machine to reproduce the evaluation including verification after translation.  If you just want to use the Specification Translator or have a look at the input/output files of the evaluation, then the zip file is sufficient.     License information  The Specification Translator tool is shared under the CC-BY 4.0 license.  The verifiers in the VM, as well as the examples and case studies used for the evaluation, are not licensed under the CC-BY 4.0 but under their original licenses which have been included.",https://data.4tu.nl/v3/datasets/9650e706-6335-4fdd-b669-1a72ac1fe23d.git,
6b8f98f1-5057-48cd-9862-02e20ea72e37,qalcore,qalcore,"Dedicated library of the Quantum Application Lab, containing generic quantum computing routines developed by the partners of the lab.",https://github.com/QuantumApplicationLab/qalcore,QuantumApplicationLab/qalcore
6bbc1cce-dc64-4626-9835-1407067a5cdc,4tu-unetge-software-and-it-user-manual,UNetGE software and it User Manual,"UNetGE is developed to segment grains from large -area images using U-Net technique (a fully convolutional network). It can extract grains in a superbly quick and highly efficient manner (e.g., about 2 mins for 600~700 crystals)",,
6bbce158-d9e7-48ce-9a2b-c03d93d024e4,tulipaio,TulipaIO.jl,,https://github.com/TulipaEnergy/TulipaIO.jl/,TulipaEnergy/TulipaIO.jl
6bfc69ac-5af5-4ea0-b26a-abc2aed6f506,laserfarm,Laserfarm,"* Provides a framework to run point cloud data analysis pipelines, e.g. for macroecological research using the laserchicken library  * Easily configurable modular pipelines  * Cross-platform distributed computing using Dask  * Support for local and remote storage, incl. webdav protocol   * Used in multiple eScience and Horizon Europe projects     Laserfarm (Laserchicken Framework for Applications in Research in Macro-ecology) provides a FOSS wrapper to Laserchicken supporting the use of massive LiDAR point cloud data sets for macro-ecology, from data preparation to scheduling and execution of distributed processing across a cluster of compute nodes.",https://github.com/eEcoLiDAR/Laserfarm,eEcoLiDAR/Laserfarm
6c5e8ad3-f28f-4a4a-8770-efedfd9cdb86,obiba-opal,OBiBa Opal,"# What is Opal?  Opal is OBiBa's core data managment application. This server application provides all the necessary tools to import, transform and describe data. Subjectâ€™s identifiers can also be managed at data import and export time.  ## Analysis Thanks to its integration with R , complex statistical analysis and reports can be performed. The implementation of the DataSHIELD process allows advanced statistical data analysis across multiple studies without sharing and disclosing any individual-level data.  ## Integration Being integrated with Onyx and Mica, studies using Opal can seamlessly and securely import data collected with Onyx. They can also create web data portals with Mica that query Opal databases to obtain real-time aggregated reports on subject's data.  Secured REST web services are also available allowing to automate server management (Python command line tools) or to access to data (from R or any tools that are web-capable).  # Features ## Data Warehouse Here are some of the main features of the Opalâ€™s data warehouse technologies:  * Store data on an unlimited number of variables, * Support MongoDB , Mysql , MariaDB and PostgreSQL as database software backend, * Customized variable dictionaries, * Import data from CSV, SPSS, SAS, Stata files and from SQL databases, * Export data to CSV, SPSS, SAS, Stata files and to SQL databases, * Incremental data importation, * Connect directly to multiple data source software such as SQL databases and LimeSurvey , * Store data about any type of ""entity"", such as subject, sample, geographic area, etc., * Store data of any type (e.g., texts, numbers, geo-localisation, images, videos, etc.), * Import and store genotype data as VCF files (Variant Call format ), * Advanced indexing functionality using ElasticSearch , * SQL API for selecting, filtering, grouping, joining table's data.  ## Resources  Resources are datasets or computation units which location is described by a URL and access is protected by credentials. When assigned to a R/DataSHIELD server session, remote big/complex datasets or high performance computers are made accessible to data analysts. Opal provides an interface for managing the access to the resources and assigning them to a R/DataSHIELD server session, in integration with the resourcer R package. When using resources, the Opal installation is very light-weight as no database and no import process is required: the data are accessed where they are originaly located, from the R server.  ## Views and Derived Variables  Opal provides the software infrastructure to create virtual tables called ""views"" of derived variables that can be persisted on disk or exported into files. Main features are: * Comprehensive JavaScript library of util functions commonly used to derive new variables (e.g. unit conversion) See Magma Javascript API . * User-friendly interfaces to recode variables without programming, * Instant summary statistics computation of the new derived variables.  ## Privacy, Confidentiality and Security Opal provides a state-of-the-art software infrastructure for data encryption, participant identifiers management and user authentication/authorization. Main features are: * Public Key Infrastructure (PKI) allowing Opal to manage public-private key pairs for encrypting and decrypting data, * Authentication using either certificates, username/password or token mechanisms, * Integration with any OpenID Connect providers, * Advanced participant identifiers manager enabling multiple identifiers per participant, * Distinct and highly secure database for storing participant identifiers, * Granular permission management down to the variable level, * REST web services using HTTPS protocol.  ## Opal File System Studies's operations involve file management and exchanges. Opal comes with its own file system to facilitate these processes. Main features are: * Centralized and file management, * SFTP access.  ## Genotypes Genotyping data can be stored in Opal as VCF files (Variant Call format ). This functionality is available as a plugin . Main features are: * Support of VCF and BCF formats, * Basic statistics, * Sample-participant mapping, * Extraction of VCF files combined with phenotypes criteria.  ## R Interface Opal includes a module enabling data statistical analysis using R. Main features are: * R server monitoring from Opal, * Secured data access from R, * Opal R package (opalr ), * DataSHIELD R packages, * Import R dataset into Opal, * Export Opal dataset into R, * Opal files management from R, * R server workspaces can be saved and restored.  ## SQL API Opal's tables can be queried with SQL: * Execute SQL from the web interface and download SQL output, * Execute SQL from the R client opalr R package, * Execute SQL from Python client sql command.  ## Reporting Opal leverages R advanced graphic and statistical capabilities by allowing the design of reports in R Markdown format. Main features are: * Scheduled Execution (with email notifications), * Advanced statistical analysis, * Advanced graphics, * Secured data access, * RStudio IDE can be used for designing reports.  Indexing Opal automatically indexes data imported in its embedded search engine (ElasticSearch ). This allows very fast retrieval and complex querying of the data. Main features are: * Real-time data dictionary search capability, * Real-time data faceted search capability, * Contingency tables.  ## Web Services (API) Opal is built on REST web services: everything is accessible through an URL. Any client that can make an HTTPs request can be a client to an Opal server. Main features are: * The resources can be obtained in JSON or binary form (Protobuf), * Client authentication can be done by providing username/password credentials or a token or by establishing a Two-way SSL authentication , * Clients are already available in Javascript, R, Python and Php.",https://github.com/obiba/opal,obiba/opal
6ced1fcd-eb53-40a7-b66f-02cfc7efb55c,4tu-code-underlying-the-publication-a-generalized-partitioning-strategy-for-distributed-control,Code underlying the publication: A Generalized Partitioning Strategy for Distributed Control,"The partitioning problem is a key problem for distributed control techniques. The problem consists in the definition of the subnetworks of a dynamical system that can be considered as individual control agents in the distributed control approach. Despite its relevance and the different approaches proposed in the literature, no generalized technique to perform the partitioning of a network of dynamical systems is present yet. In this article, we introduce a general approach to partitioning for distributed control. This approach is composed by an algorithmic part selecting elementary subnetworks, and by an integer program, which aggregates the elementary components according to a global index. We empirically evaluated our approach on a distributed predictive control problem in the context of power systems, obtaining promising performances in terms of reduction of computation speed and resource cost, while retaining a good level of performance.",https://data.4tu.nl/v3/datasets/502016d0-e852-4176-8a7e-0e4d8b350440.git,
6cf2c870-2c4d-44a4-b6bf-4429b60b98a4,sagecal,SAGECal,"Sagecal supports all source models including points, Gaussians and Shapelets. Distributed calibration using MPI and consensus optimization is enabled. Tools to build/restore sky models are included. Works on a wide range of platforms, from personal computers to supercomputer clusters.  * Sagecal provides calibration solutions for radio-interferometric data * Sagecal calibrates large sets of visibility data * Sagecal provides a solution for large datasets which need to be calibrated across tens of GPU-accelerated compute nodes for many directions on the sky * Works for datasets with thousands of channels * GPU accelerated * Distributed calibration across many compute nodes * Provides calibration solutions for hundreds of directions on the sky simultaneously",https://github.com/nlesc-dirac/sagecal,nlesc-dirac/sagecal
6d388f07-8117-4b98-83c9-5a13c2475dc6,ewatercycle-marrmot,ewatercycle-marrmot,"Generate MARRMoT forcing and run the MARRMoT hydrological model M01 and M14 in a container using the eWaterCycle package.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/ewatercycle-marrmot/,eWaterCycle/ewatercycle-marrmot
6d53cf42-c7f0-4e5f-adbd-9cf628b147e5,mass-spec-studio,Mass Spec Studio,"The **Mass Spec Studio** (www.msstudio.ca) is an architecture that supports the development of mass spec data processing apps in a Windows enviroment. It was created over the last 12 years to reduce the burden of new software development for a wide range of valuable applications. The Studio allows for component reuse in every new development project, with new extensions continually adding to the Studioâ€™s appeal as a general-purpose tool builder. It exceeds the capabilities of the generic workflow generators, particularly in areas of visualization, processing speed, and ease-of-use.   * For **users**, it's a fully customizable, user-friendly desktop application for analyzing any kind of mass spec data.  * For **developers**, it's an intuitive framework designed to provide a pluggable architecture for rapid development of new analysis packages and specific processing tools.",,
6d64afec-4751-444a-8fc9-419261dc6a25,4tu-matlab-codes-for-probabilistic-field-approach-for-motorway-driving-risk-assessment,Matlab codes for Probabilistic Field Approach for Motorway Driving Risk Assessment,"This package contains scripts used in the article: `Mullakkal-Babu, Freddy A., et al. ""Probabilistic field approach for motorway driving risk assessment."" Transportation research part C: emerging technologies 118 (2020): 102716.` All the folders contain a `README.md` file with steps to use and detailed description of the folder contents. The generic description of the folder contents are as follows: ## Figures1-2+ This folder contains scripts used to create `Figure.1` and `Figure.2`. + Please follow the README file in the folder. ## singleStepPDRF+ This folder contains scripts for estimate single-step PDRF for a given trajectory pair. Here the trajectory pair is specified in .xls format+ The scripts were used to create `Figure.5`, `Figure.6` and `Figure.7`+ Please follow the README file in the folder. ## multiStepPDRF+ This folder contains scripts for estimate multi-step PDRF+ The scripts were used to create `Figure.3` and `Figure.12`. + Please follow the README file in the folder. ## SimulationBasedValidationStudy+ This folder contains scripts for estimate single-step PDRF for a given trajectory pair involved in a cut-in event.     These scipts can be used to generate variations of cut-in encounters and study the risk of these encounters as expressed by singleStepPDRF+ The scripts were used to create `Figure.8` and `Figure.9`.",,
6d857dc7-15de-44e5-b9af-ae1cd84c4ba4,storyteller,StoryTeller,* Visual Analytics for text data with rich connections * Able to show connections between entities and storylines * Interconnected graphs and visualizations with many filtering options * Successfully used in multiple Humanities projects,https://github.com/NLeSC/UncertaintyVisualization,NLeSC/UncertaintyVisualization
6dd3211e-8383-4a8b-816b-39aa49aae333,chimp-classifier,chimp-classifier,"The aim of this software is to classify Chimpanze vocalizations in audio recordings from the tropical rainforests of Africa. The software can be used for processing raw audio data, extracting features, and apply and compare Support Vector Machines and Deep learning methods for classification. The pipeline is reusable for other settings and species or vocalization types as long as a certain amount of labeled data has been collected. The best performing models will be available here for general usage.",https://github.com/UtrechtUniversity/animal-sounds,UtrechtUniversity/animal-sounds
6dd5fbe8-7915-4849-b695-c6668ebff063,leakybucket,leakybucket,"This is a template for creating a new hydrological model in Python, using the Basic Model Interface (BMI).  The goal is to plug this model into eWaterCycle, and as such forcing data and configuration file handling will be performed using eWaterCycle.  With the leakybucket template you can: - Help you write a new hydrological model in a BMI. - Learn how to put this model in a (Docker) container with grpc4bmi, so other people can use it.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/leakybucket-bmi,eWaterCycle/leakybucket-bmi
6e5be2d5-62a9-467a-bf57-a37d2f565e1b,4tu-bachelor-thesis,bachelor-thesis,"This repository contains all the code related to the Bachelor thesis ""Creating Robust Train Unit Shunting Plans using Probabilistic Programming"" for the Bachelor of Computer Science and Engineering at Delft University Of Technology. The research investigates the use of probabilistic programming in increasing robustness of train shunting plans. The dataset includes the code which is used to model and to apply probabilistic inference, as well as all configuration files required to reproduce the experiments. Additional information about the contents and the setup of the project can be found in the README file.",https://data.4tu.nl/v3/datasets/5a7f760d-9bd9-4925-92c3-28dd56ee3318.git,
6e60b314-87c2-4a31-b92d-646994ec969b,biomero,BIOMERO,,https://github.com/NL-BioImaging/biomero,NL-BioImaging/biomero
6e8e22db-04ab-4a1b-aa29-40fa744ff7ab,mzrecal,mzRecal,"mzRecal recalibrates mass spectrometry (MS1) data in mzML format, using peptide identifications in mzIdentML.  * mzRecal uses calibration functions based on the physics of the mass analyzer (FTICR, Orbitrap, TOF) * Consuming and producing data in open standard formats ([mzML](https://www.psidev.info/mzML), [mzIdentML](https://www.psidev.info/mzidentml)) * Simple to plug into a data processing pipeline * Recalibrated data generally results in a higher number of high-confidence identifications  ![picture of recalibration result](https://github.com/524D/mzrecal/raw/master/ppmerr.png)",https://github.com/524D/mzrecal,524D/mzrecal
6e9301da-e898-48ee-a1a9-021e346fd5b9,4tu-morality-is-non-binary-building-a-pluralist-moral-sentence-embedding-space-using-contrastive-learning-code,Morality is Non-Binary: Building a Pluralist Moral Sentence Embedding Space using Contrastive Learning - code,"We train embedding spaces with the MFTC corpus, to see how an embedding space can learn the distribution of pluralist morality. We compare off-the-shelf, unsupervised, and supervised approaches, showing that a supervised approach is necessary. Here, you can find the code used to train and evaluate the resulting embedding spaces.",https://data.4tu.nl/v3/datasets/296d19ac-d818-472d-a55d-568695e83d2b.git,
6e9ca303-14c0-4568-8906-eb0dc3c81ce3,finding-journalists,Lysander,"* the software helps in finding unknown Twitter users related to a known group of users. * target audience for this software: researchers * discovering previously unknown Twitter users related to a known group of users * suggesting Twitter users related to a known group of users * useful results for finding political journalists based on group of politicians  For analysis of online behaviour of groups, the first task is to find out who the members of the group are. Usually we know a few key people but not the whole group. We can find other people of the group by checking which people are related to the key people. On Twitter we can use the asymmetrical follower relation to expand groups: if a person follows many members of a group and many members of the group follow this person, the person is likely to be a member of the group. This software package uses this idea for suggesting extra members for groups on Twitter.",https://github.com/online-behaviour/find-journalists,online-behaviour/find-journalists
6eb13892-7374-492a-8b40-484a66b27878,4tu-code-complementing-the-paper-feasibility-analysis-of-a-self-reinforcing-electroadhesive-rotational-clutch,"Code complementing the paper ""Feasibility Analysis of a Self-Reinforcing Electroadhesive Rotational Clutch""","The code complementing the paper ""Feasibility Analysis of a Self-Reinforcing Electroadhesive Rotational Clutch"" presented on AIM 2021. This code determines the least sensitive configuration for a lightweight self-reinforcing electroadhesive rotational clutch while taking into account practical design and production limitations.",,
6f28e522-9e22-48d6-b381-22a81fc9c7e2,4tu-codebase-underlying-the-bsc-thesis-type-checking-modules-and-imports-using-scope-graphs,Codebase Underlying the BSc Thesis: Type-Checking Modules and Imports using Scope Graphs,"The objective of this research was to determine whether a stratified type-checking approach using scope graphs could type-check the proof-of-concept language LM. Scope graphs provide a way to type-check real-world programming languages and their constructs. Previous implementations that type-check LM, a language with relative, unordered, and glob imports, do not halt. This dataset contains Haskell code for constructing and type-checking a scope graph of an LM program. It is based on the Phased Haskell library. With this implementation, the all test cases halt and the majority exhibit the correct behaviour with only one false-negative.     This implementation uses a five-step approach:  Constructing a module hierarchy.Constructing a scope graph consisting of scopes and module sinks.Iteratively resolving imports and placing import edges in the scope graph.Adding all declarations of all modules to the scope graph.Type-checking the bodies of all declarations with respect to the scope graph.   Many test cases bundled with this data set are based on those for LMR (which is very similar to LM) and can be found here. On top of that. more test cases were derived and included. All test cases are denoted as annotated terms.     This data set is linked to a Bachelor's thesis completed at the EEMCS faculty at the TU Delft. A link will be added after publication.",https://data.4tu.nl/v3/datasets/2ef92305-5f55-417a-9da4-b0e50f0d265a.git,
6f2ca636-1ae3-4702-9c1f-b18faedd3464,galerkintoolkitjl,GalerkinToolkit.jl,"# What  `GalerkinToolkit` is a high-performance finite element (FE) toolbox fully implemented in the Julia programming language. It provides modular building blocks to easily implement custom finite element codes to solve partial differential equations (PDEs), using a variety of numerical schemes, and across diverse computing environments. `GalerkinToolkit` integrates seamlessly with the broader Julia ecosystem. It supports external solvers such as `PartitionedSolvers.jl`, `PetscCall.jl`, `LinearSolve.jl`, `NonLinearSolve.jl`, and `DifferentialEquations.jl` to handle the algebraic systems resulting from PDE discretizations. For visualization, the toolkit provides plotting recipes for Makie.jl and utilities for exporting results in VTK format using `WriteVTK.jl`.  # Why  GalerkinToolkit is definitively not the first FEM software project out there, but it has some unique features. This includes a unified API with high- and low-level abstractions, a deep integration with the Julia package ecosystem, a new form compiler, and a redesign of the core ideas behind Gridap.",https://github.com/GalerkinToolkit/GalerkinToolkit.jl,GalerkinToolkit/GalerkinToolkit.jl
6f5ba5d1-0232-4abb-b657-cc33d12a98b0,ricgraph,Ricgraph - Research in context graph,"## What is Ricgraph?  Ricgraph, also known as Research in context graph, enables the exploration of researchers, teams, their results, collaborations, skills, projects, and the relations between these items.  Ricgraph can store many types of items into a single graph.  These items can be obtained from various systems and from multiple organizations. Ricgraph facilitates reasoning about these items because it infers new relations between items, relations that are not present in any of the separate source systems.  Ricgraph is flexible and extensible, and can be adapted to new application areas.  Throughout this text, we illustrate how Ricgraph works by applying it to the application area research information.   ### Motivation  Ricgraph, also known as Research in context graph, is software that is about relations between items. These items can be collected from various source systems and from multiple organizations. We explain how Ricgraph works by applying it to the application area *research information*. We show the insights that can be obtained by combining information from various source systems, insight arising from new relations that are not present in each separate source system.  *Research information* is about anything related to research: research results, the persons in a research team, their collaborations, their skills, projects in which they have participated, as well as the relations between these entities.  Examples of *research results* are publications, data sets, and software.  Example use cases from the application area research information are: * As a journalist, I want to find researchers with a certain skill and their publications, so that I can interview them for a newspaper article. * As a librarian, I want to enrich my local research information system with research results that are in other systems but not in ours, so that we have a more complete view of research at our university. * As a researcher, I want to find researchers from other universities that have co-authored publications written by the co-authors of my own publications, so that I can read their publications to find out if we share common research interests.  These use cases use different types of information (called *items*): researchers, skills, publications, etc. Most often, these types of information are not stored in one system, so the use cases may be difficult or time-consuming to answer. However, by using Ricgraph, these use cases (and many others) are easy to answer.  Although this text illustrates Ricgraph in the application area research information, the principle â€œrelations between items from various source systemsâ€ is general, so Ricgraph can be used in other application areas.  ### Main contributions of Ricgraph  * Ricgraph can store many types of items in a single graph. * Ricgraph harvests multiple source systems into a single graph. * Ricgraph Explorer is the exploration tool for Ricgraph. * Ricgraph facilitates reasoning about items because it infers new relations between items. * Ricgraph can be tailored for an application area.  ### Read more about Ricgraph  For a gentle introduction in Ricgraph, read the reference publication: Rik D.T. Janssen (2024). Ricgraph: A flexible and extensible graph to explore research in context from various systems. *SoftwareX*, 26(101736). https://doi.org/10.1016/j.softx.2024.101736.  Extensive documentation can be found on [https://docs.ricgraph.eu](https://docs.ricgraph.eu), publications, videos and source code can be found in the [GitHub repository](https://github.com/UtrechtUniversity/ricgraph). The website for Ricgraph can be found at [www.ricgraph.eu](https://www.ricgraph.eu).",https://github.com/UtrechtUniversity/ricgraph,UtrechtUniversity/ricgraph
6f954b44-2daf-4cdf-9d87-a7b27a54ea37,spec2vec,spec2vec,"* Allows to learn abstract mass spectra representations from large mass spectral data sets (unsupervised learning). * Computes mass spectra similarities that show a high correlation with actual molecular similarity.   Spec2vec is a novel spectral similarity score inspired by a natural language processing algorithm -- Word2Vec. Where Word2Vec learns relationships between words in sentences, spec2vec does so for mass fragments and neutral losses in MS/MS spectra. The spectral similarity score is based on spectral embeddings learnt from the fragmental relationships within a large set of spectral data.",https://github.com/iomega/spec2vec,iomega/spec2vec
6fbe1740-9d5d-45fc-b2a8-65fe79a6f26d,qumia-train-scripts,qumia-train-scripts,"[![DOI](https://zenodo.org/badge/704001389.svg)](https://doi.org/10.5281/zenodo.14199793)  These are the scripts used to train two models to automatically assess muscle ultrasound images on the presence of neuromuscular diseases. The first model is used to predict the Heckmatt score based on the input image and, optionally, the patient BMI and age. The second model distinguishes healthy from non-healthy, based on the (predicted) Heckmatt score for each muscle.",https://github.com/QUMIA/train-scripts,QUMIA/train-scripts
6fcf352d-24d5-46bd-b91a-72e2d04efcdd,smartplay,SmartPlay,,https://github.com/ctwhome/SmartPlay,ctwhome/SmartPlay
700dd51c-faa5-4dd2-83ab-b2587411e02e,analizo,Analizo,,https://github.com/analizo/analizo,analizo/analizo
7063578d-91bc-4103-8b83-8fd8a5ebf89f,islandora,Islandora,"# What is Islandora?   Islandora is an extensible, modular, open source digital repository ecosystem focused on collaborative authorship, management, display, and preservation of digital content at scale. Islandora adheres to widely adopted best practices and open standards. â€ Islandora is a community-developed project, rather than a singular entityâ€™s vision of what a digital repository system should be. As such, functionality is created by individuals, institutions, service providers and consortia and shared back with the community.Â Â The community pulls together in Community Sprints that complete tasks of universal interest and build consensus on issues of architecture and feature development. Islandora is maintained by the community and through targeted service and contractor arrangements funded by community membership in the Foundation. â€  Core functions of Islandora allow you to: * Create, manage, and display collections or other compound configurations of any type of file: Images, documents (including PDFs), described audio/video, books/newspapers (and other paged content), and arbitrary file formats (binary files). * Display and view any file supported in modern browser technology, with integrations for the International Image Interoperability Framework (IIIF) for images, and native integration with Drupalâ€™s rich ecosystem of modules for digital exhibit creation. * Transcode source files for preservation, display, and full text indexing via an optional suite of microservices that enable the system to scale and manage complex media workflows, as well as to move and transform content.Â  * Preserve content through integration with common standards for persistent identifiers (such as DOI, ARK and Handle), fixity checking, bag-it compliant bags (Archival Information Packages) creation, PRONOM registry linking, reporting for content health. content versioning support, and the creation of robust administrative metadata.Â  * Create, serialize, export, and expose metadata in any format, with out-of-the box support for a robust MODS use case. In Islandoraâ€™s current iteration, we have replaced Islandora Legacy (Islandora 7s) form builder with a tighter integration with Drupal's extensive content management system, allowing for better, and more user-friendly manipulation of metadata entry forms and both the transformation and publication of metadata.Â  * Utilize world-class, highly configurable research index with faceting, simple search, faceted search, browse functions, and the ability to expose metadata through both OAI-PMH and Sparql endpoints. * Secure content through Drupalâ€™s authentication layer, which facilitates the authoring of roles and permissions at multiple layers. Content may be kept dark and exposed to various target groups, and unique workflows can be authored to suit your institution, including the ability to set embargos (often used for institutional repositories).Â   Islandora â€œcoreâ€ is comprised of the following Open Source technologies, installation of which has been automated through the ISLE-DC, ISLE-BUILDKIT (Docker), and ISLANDORA-PLAYBOOK (Ansible) projects: * Drupal * Apache Solr and ActiveMQ * Cantaloupe, a IIIF-compliant media server * Mirador, a IIIF-compliant media viewer * Matomo * ImageMagick * FFmpeg * Tesseract OCR * FITS * Fedora (optional)  Out of the box, Islandora includes content model support for:  * Collections, the basic containers into which other Repository Items may be organized * A variety of Images, from simple JPEGs to archival TIFFsÂ  * PDF-based â€œDigital Documentsâ€ * Audio files in a variety of formats * Video files in a variety of formats * Books, comprised of Pages which may be either Images or PDFs * Newspapers (periodical series of Newspaper Issues, which are comprised of Pages which may be either Images or PDFs) * Binary Objects, which allow arbitrary attachments for which there are often not web viewers * Compound Objects, which are a special type of Collection which presents like a Digital Exhibition and may also be used as a secondary organization mechanism  Through extension of these content models, Islandora allows for modeling, storage and display of a wide variety of specialized content, including Theses and Dissertations, Journal Articles, High-quality archival media, and Research Data.  Please review our full documentation for additional details about Islandora.  # Migration and Adoption There are a variety of mechanisms for exporting content from Islandora 7 and other repository systems and importing it into the current release of Islandora, with Islandora Workbench being the most widely used and best documented. The Islandora Roadmap emphasizes the creation of migration documentation and support in Q2, 2022.  #What's next? The Islandora Leadership Group maintains a roadmap of anticipated additions to the Islandora ecosystem, both from projects known to be ongoing at community institutions as well as the result of community sprints. This document may be viewed here.  #How do I get involved? Just getting started in Islandora? Join one of our communication channels, review our community wiki (especially the onboarding checklist). We look forward to meeting you!",https://github.com/Islandora/islandora,Islandora/islandora
70ced43d-6f86-4519-a542-4c0fd2d90cbf,4tu-propeller-airspeed-sensor,propeller_airspeed_sensor,"This repository contains MATLAB scripts to derive and validate an airspeed model for fixed-wing Unmanned Aerial Vehicles (UAVs) using solely propeller power and rotational speed data. The model can be used to replace Pitot-tube-based airspeed sensors, or contribute to redundancy in airspeed estimation. It does not require knowledge of the vehicleâ€™s dynamic model and is computationally lightweight. It leverages power and rotational speed feedback, which is readily available from modern Electronic Speed Controllers (ESCs), thereby enabling seamless integration with existing systems and off-the-shelf components.",https://data.4tu.nl/v3/datasets/37eb50b9-265a-472a-9488-ed6719d5c318.git,
71a07e5f-0f66-4ab9-970d-a5031a3b7370,rcrisp-shiny-app,rcrisp Shiny app,,https://github.com/CityRiverSpaces/rcrisp-app,CityRiverSpaces/rcrisp-app
72375c29-6601-446b-9a1d-698eaf587c1e,4tu-shipp,shipp,"This software implements methods to study and design hybrid power plants, i.e. power plants combining one or more renewable energy production with energy storage systems. The design methodology is based on integrated sizing optimization of storage systems, where the storage size and dispatch are design simultaneously.",https://data.4tu.nl/v3/datasets/6e381f9b-387d-4165-83cd-b9d0f5f48279.git,
72bf8b0c-3154-4766-bedf-8418e5f37501,fairworkflows,fairworkflows,"* Facilitates the construction of RDF descriptions of a variety of scientific 'workflows' * Allows easy publication of the resultant RDF by means of nanopublications  The focus is on description of workflows consisting of manual and computational steps using semantic technology, such as the ontology described in the publication:  Celebi, R., Moreira, J. R., Hassan, A. A., Ayyar, S., Ridder, L., Kuhn, T., & Dumontier, M. (2019). Towards FAIR protocols and workflows: The OpenPREDICT case study. arXiv:1911.09531.  The goals of the project are:  1. To facilitate the construction of RDF descriptions of a variety of scientific 'workflows', in the most general sense. This includes experimental procedures, ipython notebooks, computational analysis of results, etc. 2. To allow validation and publication of the resultant RDF (for example, by means of nanopublications). 3. Re-use of previously published steps, in new workflows. 4. FAIR data flow from end-to-end.  We seek to provide an easy-to-use python interface for achieving the above.",https://github.com/fair-workflows/fairworkflows,fair-workflows/fairworkflows
72dae8f6-9cc9-4b62-9e85-cbd21e85d5a3,4tu-simulation-software-and-dataset-reaction-moments-matter-when-designing-lower-extremity-robots-for-tripping-recovery,Simulation software and dataset: Reaction moments matter when designing lower-extremity robots for tripping recovery,Files required to generate SCONE simulations and scripts for reaction moments analysis for balance recovery after tripping while walking,,
732377aa-db44-4e68-87fe-708fc23499be,openms,OpenMS,"It comes with a vast variety of pre-built and ready-to-use tools for proteomics and metabolomics data analysis (TOPPTools) as well as powerful 1D, 2D and 3D visualization (TOPPView).  OpenMS offers analyses for various quantitation protocols, including label-free quantitation, SILAC, iTRAQ, TMT, SRM, SWATH, etc.  It provides built-in algorithms for de-novo identification and database search, as well as adapters to other state-of-the art tools like X!Tandem, Mascot, Comet, etc. It supports easy integration of OpenMS built tools into workflow engines like KNIME, Galaxy, WS-Pgrade, and TOPPAS via the TOPPtools concept and a unified parameter handling via a 'common tool description' (CTD) scheme.  With pyOpenMS, OpenMS offers Python bindings to a large part of the OpenMS API to enable rapid algorithm development. OpenMS supports the Proteomics Standard Initiative (PSI) formats for MS data. The main contributors of OpenMS are currently the Eberhard-Karls-UniversitÃ¤t in TÃ¼bingen, the Freie UniversitÃ¤t Berlin, and the ETH ZÃ¼rich.",https://github.com/OpenMS/OpenMS,OpenMS/OpenMS
736e4351-6598-4159-a02f-d14f9da3beaf,4tu-code-underlying-fluid-structure-interaction-model-to-predict-bending-vibrations-of-flood-gates,Code underlying Fluid-structure interaction model to predict bending vibrations of flood gates,"The fluid-structure model developed in the thesis ""wave-induced vibrations of flood gates"". This model uses a semi-analytical mode matching method to predict the bending vibrations of flood gates immersed in fluid.",,
738390fb-14c7-4252-a6af-8c742d528662,laserchicken,laserchicken,"* Find neighboring points in your point cloud and describe them as feature values * Filter points spatially or by attribute * Normalize height for every point  Laserchicken is a user-extendable, cross-platform Python tool for extracting statistical properties (features in machine learning jargon) of flexibly defined subsets of point cloud data. Laserchicken loads a point-cloud from a LAS or LAZ or PLY file. After this, it can filter points by various criteria, and it can normalize the height. Laserchicken can load another point cloud, which contains targets. For every target point, Laserchicken computes its neighbors. Based on the list of neighbors, Laserchicken extracts features that effectively describe the neighborhood of each target point.",https://github.com/eEcoLiDAR/laserchicken,eEcoLiDAR/laserchicken
7391f6f8-8291-4cd8-8df9-96af9ae1e509,splithalfr,splithalfr,,https://github.com/tpronk/splithalfr,tpronk/splithalfr
73e888a7-fca5-4ebb-b816-108af520732c,birch,BIRCH,"# BIRCH  * is a comprehensive desktop bioinformatics system integrating many of the commonly-used bioinformatics programs * is a framework of tools, files, and documentation for organizing and managing a bioinformatics core facility * can be customized by seamlessly merging 3rd party applications into BIRCH",https://www.bioinformatics.org/wiki/BIRCH,
743dc2fe-97d1-473e-a92f-74af8d47531d,doe2vec,DoE2Vec,,https://github.com/Basvanstein/doe2vec,Basvanstein/doe2vec
746605aa-f4b9-401f-a8f6-c9a561c052cb,ewatercycle-lisflood,ewatercycle-lisflood,"Generate LISFLOOD forcing and run the LISFLOOD hydrological model in a container using the eWaterCycle package.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/ewatercycle-lisflood,eWaterCycle/ewatercycle-lisflood
7467297c-e9e0-476b-94a5-7c7812632b8f,4tu-source-code-for-the-msc-thesis-multi-leader-adaptive-cruise-control-systems-considering-sensor-measurement-uncertainties-based-on-deep-reinforcement-learning,Source code for the MSc thesis: Multi-leader Adaptive Cruise Control Systems considering Sensor Measurement Uncertainties based on Deep Reinforcement Learning,"This repository contains programming codes for the training and simulation of Deep Reinforcement Learning-based Adaptive Cruise Control systems.       For more information, please see the README.txt file in the repository or the report of master thesis titled ""Multi-leader Adaptive Cruise Control Systems considering Sensor Measurement Uncertainties based on Deep Reinforcement Learning"", which can be found in the reference (link).",,
74c2bcf7-b060-48f1-a305-6952d105f74f,4tu-matlab-script-for-a-3d-marker-tracking-program,MATLAB script for a 3D marker tracking program,"A MATLAB script to track markers from multiple cameras, triangulate to 3D world coordinate using camera calibration, and determine object orientation by matching to known marker locations on a CAD model.",https://data.4tu.nl/v3/datasets/c582c3d6-6f64-4051-8f00-0535d957b300.git,
7534b34f-6b57-402a-bcb7-67facc765dc6,zarrvis,ZarrVis (Ruisdael),,https://github.com/NLeSC/zarrviz,NLeSC/zarrviz
754cd1bf-3b05-4480-af3f-95773576a21b,slurm-cli-api-proxy,SLURM CLI-API Proxy,"Application examples of this tool in the biomedical domain would include the Galaxy Pulsar server that forwards jobs to an HPC backend through SLURM. Another key use case for this tool is Arvados, an open-source platform designed for managing and processing large biomedical data. In general, anywhere where a setup assumes the SLURM CLI to be available on a host, the SLURM CLI-API Proxy could help to offload job processing to a more powerful system.",https://github.com/SLURM-CLI-API-Proxy/SLURM-CLI-API-Proxy-client,SLURM-CLI-API-Proxy/SLURM-CLI-API-Proxy-client
759ac609-887d-4a79-a65c-d74bd66acf5b,qc2,qc2,,https://github.com/qc2nl/qc2,qc2nl/qc2
75c2f035-c1df-45a7-90ef-175ffcd6a106,pbg-ld,pbg-ld,* provides an integrated genetic resource on potato and (wild) tomato species for plant researchers and breeders * enables queries for genomic regions or genes associated with traits of interest (QTLs) * biological concepts and  their relationships are described by (domain-specific) ontologies and controlled vocabularies,https://github.com/candYgene/pbg-ld,candYgene/pbg-ld
75fc5bd6-83c7-4eea-956b-69cd3fc44477,juri,JURI,,https://github.com/FZJ-JSC/JURI,FZJ-JSC/JURI
76d1d0af-693c-495d-a3a2-4cb8676263e6,4tu-code-for-appscanner-automatic-fingerprinting-of-smartphone-apps-from-encrypted-network-traffic,Code for Appscanner: Automatic fingerprinting of smartphone apps from encrypted network traffic,"This repository contains the code for for AppScanner that was implemented as part of the NDSS FlowPrint paper [PDF], it implements the Single Large Random Forest Classifier of AppScanner [PDF]. We ask people to cite both works when using the software for academic research papers.",https://data.4tu.nl/v3/datasets/6c117625-0bbf-4988-b13e-26bb4daa203c.git,
7700ad77-6676-4792-81f8-e23f64676ce8,knime-testflow,KNIME Testflow,"* KNIME node developers can run test workflows, inside KNIME or from the command line already, this software allows you to test a workflow from JUnit test * Allows you to write JUnit tests not only for unit tests, but also for integration test by running workflows inside a JUnit test * By running the test workflow inside a JUnit test, you get the JUnit benefits like automation and coverage * Used in all KNIME nodes made in the 3D-e-Chem project to test that the nodes are working as expected",https://github.com/3D-e-Chem/knime-testflow,3D-e-Chem/knime-testflow
771ae74d-7ee2-460e-93e7-af189d6751fc,4tu-matlab-script-and-comsol-models-of-the-article-an-efficient-multiscale-method-for-subwavelength-transient-analysis-of-acoustic-metamaterials,"MATLAB script and COMSOL models of the article ""An efficient multiscale method for subwavelength transient analysis of acoustic metamaterials""","A reduced-order homogenisation framework is proposed in the article ""An efficient multiscale method for subwavelength transient analysis of acoustic metamaterials"", providing a macro-scale enriched continuum model for locally resonant acoustic metamaterials operating in the subwavelength regime, for both time and frequency domain analyses. The homogenised continuum has a non-standard constitutive model, capturing a metamaterial behaviour such as negative effective bulk modulus, negative effective density, and Willis coupling. A suitable reduced space is constructed based on the unit cell response in a steady state regime and the local resonance regime.     - The effective continuum material properties are computed via the MATLAB script provided here.     -A frequency domain numerical example demonstrates the efficiency and suitability of the proposed framework. The macro-scale model is implemented via a COMSOL model provided here.     -The direct numerical simulations (COMSOL models) are also provided here.",https://data.4tu.nl/v3/datasets/609090f4-35ee-4bd8-b3d7-3a1e67db7867.git,
77201671-58bf-4d6e-9f3d-d509c0ff5b88,knime-kripodb,KNIME node for Kripo,"* For cheminformaticans who want to do structure-based protein binding site comparison and bioisosteric replacement for ligand design * It makes the Kripo Python library available in the KNIME workflow platform as workflow nodes. * Kripo encodes the interactions of protein and bound ligand also known as a pharmacophore into a fingerprint, the fingerprints can be compared to each other to find similar pharmacophores * The Kripo software is open source while most other similar software is commercial or requires registration",https://github.com/3D-e-Chem/knime-kripodb,3D-e-Chem/knime-kripodb
773d6e81-c57a-4a58-a9ea-444297021e70,user-defined-oceanographic-data-products,User-Defined Oceanographic Data Products,,https://data.oceannetworks.ca/home,
77603966-9172-499e-ba76-eb8851f23543,4tu-multistage-pinn,Multistage-PINN,"Physics Informed Neural Networks (PINNs) have been rarely applied to solve multiphysics systems due to the inherent challenges in optimizing their complex loss functions, which typically incorporate multiple physics-based terms. This study presents a multistage PINN approach designed to efficiently solve coupled multiphysics systems with strong interdependencies. The multistage PINN progressively increases the complexity of the physical system being modeled, enabling more effective capture of coupling between different physics. The computational merits of this approach are demonstrated through two illustrative applications: prediction of asphalt aging and modeling of lid-driven cavity flow. Quantitative and qualitative comparisons with standard PINN and adaptive weight PINN approaches demonstrate the enhanced precision and computational efficiency of the proposed algorithm. The multistage PINN achieves a reduction in training time of more than 90% compared to standard PINNs while maintaining better alignment with the finite element method (FEM) solutions. The improvement in computational efficiency, coupled with enhanced accuracy, positions the multistage PINN as a powerful tool for addressing complex multiphysics problems across various engineering disciplines. The methodâ€™s ability to handle interactions between multiple physical processes, such as diffusion, chemical reactions, and fluid dynamics, makes it suitable for simulating long-term material behavior and complex fluid system.",https://data.4tu.nl/v3/datasets/8cdb120a-f4cb-47eb-98b3-aac6fbea1d75.git,
7782731b-2fb4-49c0-a86a-fcf349862567,pacte,PACTE,"PACTE was originally developed by CRIM for a set of research groups, notably the Laboratoire dâ€™ingÃ©nierie cognitive et sÃ©mantique (LiNCS) and the Centre de recherche et d'expertise Jeunes en difficultÃ© (CCSMTL). The platform, having been developed for general use, is adaptable and can meet a broad range of needs of research communities outside the initial groups.  PACTE helps users annotate large text corpora through a web interface. To improve analysis and annotation of those corpora, bilingual linguistic, lexical and semantic analysis services are available for text annotation. The platform also includes an active learning service enabling semi-automatic annotation of text attributes by letting the user interact with a machine learning module that will identify the most significant data to annotate. This service will thus predict annotation of some text attributes that were not manually annotated. Visualization and research tools are also integrated to facilitate the researcher's work.",http://pacte.crim.ca/,
77a63091-1fa5-40f9-a2a9-d03dadb0f4ea,4tu-coverage-driven-slam-testing,Coverage-Driven SLAM Testing,"This dataset contains software to automatically generate high-coverage tests for SLAM algorithms by partitioning of the input space. It also contains some pre-generated test suites, experimental results comparing the performances of different generation methods, and replication information for the experiments. For details on how to adjust the test case generation as well as detailed replication instructions, refer to the enclosed README.md.",,
77b0e2a1-a33f-4985-872c-644a3f242729,fairdatapoint,FAIR Data Point,"* Makes your data assets structured and available through RESTful Web API * Provides  standardized descriptions (RDF-based metadata) using controlled vocabularies and ontologies * Enables client applications to retrieve, aggregate or filter (meta)data from distributed FDPs",https://github.com/NLeSC/fairdatapoint,NLeSC/fairdatapoint
78052659-30ee-4236-9af3-4355aceda0c5,4tu-data-underlying-the-phd-thesis-accelerating-programmer-friendly-intermittent-computing-chapter-6,Data underlying the PhD thesis: Accelerating Programmer-Friendly Intermittent Computing (Chapter 6),"This is the archive for Chapter 6 of the PhD thesis named ""Accelerating Programmer-Friendly Intermittent Computing"" by Vito Kortbeek.       Rationale   Intermittently operating embedded computing platforms powered by energy harvesting must frequently checkpoint their computation state. Using non-volatile main memory significantly reduces the checkpoint size but at the cost of increasing the checkpoint frequency to cover WAR dependencies. Additionally, non-volatile memory is significantly slower to access. Both of these challenges are addressed by the architecture proposed in Chapter 6, greatly increasing performance.       Archive Structure   This archive consists of the software to emulate the system (`icemu/plugins`), and the LLVM toolchain used to compile applications (`llvm`). The code can be built and run using the development docker container in the `docker` directory. To build all the benchmarks, execute the `run.sh` script within the `benchmarks` directory. Run the respective Jupyter Notebook in the `plotting` directory to analyze the results.",,
7848ccd6-1675-48fc-a35c-2aac67edaba0,pystemmusscope,PyStemmusScope,,https://github.com/EcoExtreML/STEMMUS_SCOPE_Processing,EcoExtreML/STEMMUS_SCOPE_Processing
7865d43f-858f-4e2b-9ed4-560601668720,physicalconstantsjl,PhysicalConstants.jl,,https://github.com/JuliaPhysics/PhysicalConstants.jl,JuliaPhysics/PhysicalConstants.jl
796f31cf-545a-484e-88c5-ee408a4fde2e,medical-image-processing-in-python,Medical Image Processing in Python,"Medical Image Processing in Python This lesson gives an introduction to some classic medical image processing problems with popular Python libraries for medical image processing e.g. SimpleITK, NiBabel and Pydicom.  Teaching this lesson? Do you want to teach any of this material? This material is open-source and freely available.  We would love to help you prepare to teach the lesson and receive feedback on how it could be further improved, based on your experience in the workshop.  You can notify us that you plan to teach this lesson by creating an issue in our repository.   Target Audience The main audience of this carpentry lesson is PhD students, post-docs and other researchers without deep software engineering experience with medical images.    Contributing We welcome all contributions to improve the lesson! Maintainers will do their best to help you if you have any questions, concerns, or experience any difficulties along the way.",https://github.com/carpentries-incubator/medical-image-processing,carpentries-incubator/medical-image-processing
79b26727-b3ce-443b-ac3c-9eaa9792e82e,ina-tool,INA tool,"INA Tool is an open-source digital tool developed by the eScience Center aimed at supporting the regulatory framework design process by facilitating the study, analysis, and decision-making through data visualization and interaction.",https://github.com/ESI-FAR/INA-tool,ESI-FAR/INA-tool
7a1f5054-74b0-459b-80b5-a41bc1abb744,plus-toolkit,Plus Toolkit,"Plus is an open-source software toolkit for data acquisition, pre-processing, and calibration for navigated image-guided interventions. Plus was originally developed for ultrasound-guided interventions (hence the name, Plus - Public software Library for UltraSound imaging research) and it contains all essential functions for implementing tracked ultrasound systems, but it is now widely used in all kind of interventions, with and without ultrasound imaging. [See more information on the Features page.](https://plustoolkit.github.io/features)",https://github.com/PlusToolkit/PlusLib,PlusToolkit/PlusLib
7a2fda23-00a6-4b0d-9c9d-8e6b4c4c7cbe,4tu-matlab-files-used-to-produce-the-figures-in-publication-induced-aseismic-slip-and-the-onset-of-seismicity-in-displaced-faults,"Matlab files used to produce the figures in publication ""Induced aseismic slip and the onset of seismicity in displaced faults""","These Matlab files have been used to generate the plots for: Jansen, J.D. and Meulenbroek, B., 2022: ""Induced Aseismic Slip and the Onset of Seismicity in Displaced Faults"". Accepted for publication in the Netherlands Journal of Geosciences.       The main file is fault_slip_main.m. It contains a set of input parameters to compute and plot depletion-induced stresses and fault slip along an inclined fault for user-defined parameters.Â        See the text file ReadMe.txt for further infromation.",,
7a3f4a94-fadb-4cb9-af15-059a6d97f868,4tu-qmi-quantum-measurement-infrastructure-a-python-3-framework-for-controlling-laboratory-equipment,"QMI - Quantum Measurement Infrastructure, a Python 3 framework for controlling laboratory equipment",QMI is a Python 3 framework for controlling laboratory equipment. It is suitable for anything ranging from one-off  scientific experiments to robust operational setups.,https://data.4tu.nl/v3/datasets/f30c8503-488e-42d5-9ee9-bf581b50248b.git,
7a5e8b57-2c31-4dbb-9da6-508a1481be3e,llview,LLview,"LLview is a set of software components to monitor clusters that are controlled by a resource manager and a scheduler system. Within its Job Reporting module, it provides detailed information of all the individual jobs running on the system. To achieve this, LLview connects to different sources in the system and collects data to present to the user via a web portal. For example, the resource manager provides information about the jobs, while additional daemons may be used to acquire extra information from the compute nodes, keeping the overhead at a minimum, as the metrics are obtained in the range of minutes apart. The LLview portal establishes a link between performance metrics and individual jobs to provide a comprehensive job reporting interface.",https://github.com/FZJ-JSC/LLview,FZJ-JSC/LLview
7a725ea4-bd12-47ce-b055-d610ffe18a90,4tu-code-accompanying-the-paper-modeling-the-effect-of-prior-austenite-grain-size-on-bainite-formation-kinetics,"Code accompanying the paper ""Modeling the effect of prior austenite grain size on bainite formation kinetics.""",This dataset contains the Python code of the model developed for the publication: Modeling the effect of prior austenite grain size on bainite formation kinetics. It contains three examples showing how to simulate the kinetics of bainite formation and how to extract the model parameters by fitting experimental data of bainite formation to the model.,,
7b78fba2-0452-440c-8df1-aaecb03e231e,4tu-supplementary-materials-for-the-paper-on-senders-s-models-of-visual-sampling-behavior,Supplementary materials for the paper: On Sendersâ€™s Models of Visual Sampling Behavior,This MATLAB script produces all figures in the paper,,
7b8f295e-7d7f-48d1-99c3-9400a4d5f78f,4tu-script-for-semi-automated-sensitivity-analysis-for-structural-equation-models-mediation-analysis-in-r,Script for Semi-automated Sensitivity Analysis for Structural Equation models (mediation analysis) in R,"For the automatization of the structural equation modelling, a function in R was developed that automatically performs structural equation modelling for a list of green space indicators. It stores the results as CSV tables and plots figures automatically as png files.",,
7ba0a86a-1be2-4bdc-a28f-36d30c4f4e63,4tu-code-underlying-the-publication-video-bagnet-short-temporal-receptive-fields-increase-robustness-in-long-term-action-recognition,"Code underlying the publication: ""Video BagNet: short temporal receptive fields increase robustness in long-term action recognition""","Previous work on long-term video action recognition relies on deep 3D-convolutional models that have a large temporal receptive field (RF). We argue that these models are not always the best choice for temporal modeling in videos. A large temporal receptive field allows the model to encode the exact sub-action order of a video, which causes a performance decrease when testing videos have a different sub-action order. In this work, we investigate whether we can improve the model robustness to the sub-action order by shrinking the temporal receptive field of action recognition models. For this, we design Video BagNet, a variant of the 3D ResNet-50 model with the temporal receptive field size limited to 1, 9, 17 or 33 frames. We analyze Video Bag-Net on synthetic and real-world video datasets and experimentally compare models with varying temporal receptive fields. We find that short receptive fields are robust to sub-action order changes, while larger temporal receptive fields are sensitive to the sub-action order. In this repository, we provide our code, including the implementation of Video Bag-Net.",https://data.4tu.nl/v3/datasets/9f4b04e3-81a5-4d03-a0da-ccb8d7d7d311.git,
7ba4405d-488d-47c5-aec3-82849515238c,4tu-code-underlying-the-publication-using-and-abusing-equivariance,Code underlying the publication: Using and Abusing Equivariance,"Code corresponding to ICCVw 2023 conference workshop paper ""Using and Abusing Equivariance"".     Abstract  In this paper we show how Group Equivariant Convolutional Neural Networks use subsampling to learn to break equivariance to their symmetries. We focus on 2D rotations and reflections and investigate the impact of broken equivariance on network performance. We show that a change in the input dimension of a network as small as a single pixel can be enough for commonly used architectures to become approximately equivariant, rather than exactly. We investigate the impact of networks not being exactly equivariant and find that approximately equivariant networks generalise significantly worse to unseen symmetries compared to their exactly equivariant counterparts. However, when the symmetries in the training data are not identical to the symmetries of the network, we find that approximately equivariant networks are able to relax their own equivariant constraints, causing them to match or outperform exactly equivariant networks on common benchmark datasets.",https://data.4tu.nl/v3/datasets/9fd566e4-f4a8-4585-b705-be66fc29af9b.git,
7bb00e27-3c7b-42cd-9d19-83438808d423,rosemary,Rosemary,"> What if developers and researchers could interact with [Linked Data](https://www.w3.org/DesignIssues/LinkedData) in their applications and analyses as they would any [RESTful API](https://www.geeksforgeeks.org/rest-api-introduction/), no Linked Data or [SPARQL](https://sparql.dev/) knowledge required?  [Rosemary](http://github.com/yasgui-with-rosemary/app) is a plugin for [Yasgui](https://yasgui.triply.cc/) facilitating creation of SPARQL queries without requiring knowledge of SPARQL.  **Scope**  Rosemary is a proof of concept. It currently does not support the full expressivity of SPARQL (only a restricted subset of [SELECT](https://www.w3.org/TR/sparql11-query/#select) queries) and that is not the intention. It is a tool which can be useful to developers and researchers **unfamiliar with Linked Data technologies** for:  1. discovering and exploring what data is in a **public** Linked Data store, 2. constructing custom Linked Data queries to retrieve data for:    1. software applications    2. data analyses    3. research projects  **Recipe**  1. Use [rosemary](http://github.com/yasgui-with-rosemary/app) to construct queries for retrieving relevant data from public Linked Data stores,  2. Publish the queries on [Github](http://github.com) using [grlc publisher](https://github.com/CLARIAH/yasgui-grlc-publisher), 3. Convert them to [RESTful APIs](https://www.geeksforgeeks.org/rest-api-introduction/) with [grlc](http://grlc.io).",https://github.com/yasgui-with-rosemary/app,yasgui-with-rosemary/app
7bef2586-7a3a-4eac-8dbb-ed4c8e836fdd,4tu-implementation-of-study-1-from-master-thesis-building-interactive-text-to-sql-systems,Implementation of Study 1 from Master Thesis Building Interactive Text-to-SQL Systems,Implementation to Master Thesis on Building Interactive Text-to-SQL Systems. This implementation shows how to implement approximate semantic evaluation when dealing with users.       Referral link to Dataset of Study 1 and 2: https://doi.org/10.4121/19733020,,
7c277c81-e09d-44ba-bf66-45c8ad62e9b9,lhapdf,LHAPDF,"LHAPDF is the standard tool for evaluating parton distribution functions (PDFs) in high-energy physics. PDFs encode the flavour and momentum structure of composite particles, such as protons, pions and nuclei; most cross section calculations are based on parton-level matrix elements which must be connected to the real interacting particles, hence PDFs are an essential ingredient of phenomenological and experimental studies at hadron and heavy-ion colliders (e.g. LHC, HERA, Tevatron, EIC, FCC) and in cosmic-ray physics.  PDFs themselves are fitted to a range of data by various collaborations. LHAPDF provides the definitive community library of such fits, in a standard data-format, as well as C++ and Python interfaces for evaluating them. Written as a general purpose C++ interpolator for estimating PDFs from discretised data files, it has also found more general uses, such as for fragmentation functions (essentially the inverse of PDFs).",https://gitlab.com/hepcedar/lhapdf,
7c34d23d-ad1a-4f55-a030-4a5af77f0506,democracy-topic-modelling,Democracy Topic Modelling,,https://github.com/backdem/democracy-topic-model,backdem/democracy-topic-model
7c68971e-51e7-4d62-9683-3a393ebbacd0,4tu-matlab-code-for-full-range-adaptive-cruise-control-with-integrated-collision-avoidance-strategy,Matlab code for Full Range Adaptive Cruise Control with Integrated Collision Avoidance Strategy,"This package contains Matlab scripts used in the article: Mullakkal-Babu, F. A., Wang, M., van Arem, B., & Happee, R. (2016, November). Design and analysis of full range adaptive cruise control with integrated collision avoidance strategy. In 2016 IEEE 19th International conference on intelligent transportation systems (ITSC) (pp. 308-315). IEEE.       The code describes the control algorithm for a full speed range ACC with collision avoidance function. The code outputs the simulated ACC behavior in several scenarios, including stop and go, normal, emergency braking, and cut in. You can specify sensor delay and actuator lag values.",,
7d1c5f79-e40d-4599-bca2-369c5a4e73df,nnpdf,NNPDF,[The NNPDF collaboration](http://nnpdf.science) determines the structure of the proton using Machine Learning methods. This is the main repository of the fitting and analysis frameworks. In particular it contains all the necessary tools to [reproduce](https://docs.nnpdf.science/tutorials/reproduce.html) the [NNPDF4.0 PDF determinations](https://arxiv.org/abs/2109.02653). See also our recent [ArXiv preprint](https://arxiv.org/abs/2410.16248),https://github.com/NNPDF/nnpdf,NNPDF/nnpdf
7d2ee43d-03ea-4f68-bd4c-3254abca247e,grainlearning,GrainLearning,"GrainLearning is a Bayesian uncertainty quantification toolbox for computer simulations of granular materials. The software is primarily used to infer model parameter distributions from observation or reference data, also known as inverse analyses or data assimilation. Implemented in Python, GrainLearning can be loaded into a Python environment to process your simulation and observation data, or used as an independent tool where simulations are run separately, e.g., from the command line.  If you use GrainLearning, please cite [this paper](https://doi.org/10.21105/joss.06338). If you want to know more about how the method works, the following papers can be interesting:  - H. Cheng, T. Shuku, K. Thoeni, P. Tempone, S. Luding, V. Magnanimo. An iterative Bayesian filtering framework for fast and automated calibration of DEM models. *Comput. Methods Appl. Mech. Eng., 350 (2019)*, pp. 268-294, [10.1016/j.cma.2019.01.027](https://doi.org/10.1016/j.cma.2019.01.027) - P. Hartmann, H. Cheng, K. Thoeni. Performance study of iterative Bayesian filtering to develop an efficient calibration framework for DEM. *Computers and Geotechnics 141*, 104491, [10.1016/j.compgeo.2021.104491](https://doi.org/10.1016/j.compgeo.2021.104491)",https://github.com/GrainLearning/grainLearning,GrainLearning/grainLearning
7d4c90e6-e47f-4596-9918-dda5602f9278,icgc-argo,ICGC ARGO ,"# The ARGO Vision  We know each cancer is different, yet we treat them much the same. A major challenge is that we donâ€™t have the information to know ahead of time which treatments will work and which won't. This is why we have ICGC ARGO.Â   The ARGO project is a new phase of the International Cancer Genome Consortium. Launched in 2019 after 10 successful years of the ICGC mapping genomic alterations that characterise over 50 cancer types.  The time has now come to translate this knowledge to improve outcomes for people affected by cancer. Mission  ICGC ARGO will uniformly analyze specimens from 100,000 cancer patients with high quality clinical data to address outstanding questions that are vital to our quest to defeat cancer.Â   Over the next ten years ICGC ARGO aims to deliver a million patient-years of precision oncology knowledge to the world, by making data available to the entire research community in a rapid and responsible way, to accelerate research into the causes and control of cancer.Â   **The ARGO project aims to address the following questions:**      How do we use current treatments better?     How does a cancer change with time and treatment?     How do we practically implement these approaches in healthcare and drug development?     How do we advance early detection and ultimately prevent cancer?  # Background  Research shows that each cancer is different, yet we treat them much the same. A major challenge is that we donâ€™t have the information to know ahead of time which treatments will work and which won't. This means that frequently clinicians either overtreat or undertreat cancers, or there is no meaningful treatment at all.Â   This is important as cancer incidence and deaths are rising worldwide as a result of the growth and aging of the human population. There were 17 million new cases of cancer (all cancers combined excluding non-melanoma skin cancer) worldwide in 2018 and it is predicted there will be 27.5 million new cancer cases worldwide each year by 2040; an increase of 61.7% from 2018, if recent trends in incidence of major cancers and population growth continue globally.Â   The ability to rapidly generate massive â€˜omics datasets has driven a new era in the world of medicine which promises to optimally manage disease, leading to improvements in outcomes for patients and efficiencies in healthcare systems. This is known as â€œPrecision Medicineâ€. Precision medicine aims to match each patient with the treatment that will work best for them and their genome provide individualised care, and better address those questions every cancer patient deserves an answer to, such as â€œwhich treatment will be most effective for me and what will be the likely outcomeâ€.Â   Numerous platforms for various cancer types have been established to address these vital questions in many countries around the world. Whilst these build on our knowledge base, there are currently no mechanisms to standardise the complex analyses, or efficient mechanisms for data sharing for cancer that will enable composite and pooled analyses of data from around the world. Based on the 10-year ICGC experience, ARGO stands poised to accelerate cancer research for the international community through its established infrastructure, expertise and workflows.Â   **The project size and scope will enable:**      An understanding of the regional differences in disease around the world.Â      The heterogeneity of cancer.     The diversity of environmental risk factors.Â      Describe new cancers with a common genomic background, and common outcomes; and the many different combinations of therapeutic interventions.",https://github.com/icgc-argo,
7d783341-fcb0-479d-aafa-5b295e6b4d86,internet-archive-scraper,Internet Archive scraper,"# ia-webscraping (Internet Archive scraper)  ia-webscraping provides code to set up an AWS workflow for collecting and analyzing webpages from the Internet Archive (IA).  Using HashiCorp's Terraform technology, the program provides scripts to define and launch a server on Amazon's AWS infrastructure. Specifically, it uses the AWS modules SQS (message queueing), Lambda (code), S3 (storage), and Kinesis Data Firehose (data transfer) to set up a pipeline that facilitates scraping pages from the IA. An AWS account is required to deploy the pipeline.  Once deployed, the pipeline can be fed a list of domains. The software obtains all available URLs in the IA for each of those domains. As the IA can store multiple snapshots of the same URL, a time period can be configured within which all available snapshots of a URL will be saved. The resulting list of URLs is passed on to a second function, that retrieves the corresponding pages from the IA. For each page, the human readable text and, optionally, all links in the page are saved to a database. The database consists of Apache Parquet files saved in an S3 bucket. Afterwards, the Parquet files can be downloaded manually and be read out for analysis in R, Python or other compatible programs.",https://github.com/UtrechtUniversity/ia-webscraping,UtrechtUniversity/ia-webscraping
7daca7c5-c2ee-443d-ac2a-ce59a737c2dd,cdk,Chemistry Development Kit,"The Chemistry Development Kit is a Java library with cheminformatics functionality. It can read many chemistry file formats (MDL molfile and SD file, XYZ, PDB, etc), line notations (SMILES), produce InChI, calculate molecular descriptors, depict chemical structures as 2D SVG, and a lot more.",https://github.com/cdk/cdk,cdk/cdk
7dfe357c-9495-44e4-ac4e-14947f3d9789,brainbrowser,BrainBrowser,"**BrainBrowser** is an open source JavaScript library exposing a set of web-based 3D visualization tools primarily targetting neuroimaging [Sherif et al. 2015]. Using open web-standard technologies, such as WebGL and HTML5, it allows for real-time manipulation and analysis of 3D imaging data through any modern web browser. The BrainBrowser Surface Viewer is a WebGL-based 3D viewer capable of displaying 3D surfaces in real-time and mapping various sorts of data to them. The BrainBrowser Volume Viewer is an HTML5 canvas-based viewer allowing slice-by-slice traversal of MINC, NIfTI and MGH/freesurfer volumetric data.   The **BrainBrowser Surface Viewer** is a WebGL-based 3D viewer capable of displaying 3D surfaces in MNI object, Wavefront object, as well as FreeSurfer binary and ASCII surface ï¬le formats. Surfaces and other data are loaded either over the network or from local ï¬les and then parsed in separate threads using Web Workers. Data maps can be applied to loaded surfaces and manipulated by changing color maps, setting thresholds or blending them with other data maps. A Surface Viewer web service has also been made available, allowing developers to make a simple API call to embed a Surface Viewer in their own web pages to visualize their own data. The BrainBrowser Surface Viewer uses three.js for 3D rendering. - [Source code](https://github.com/aces/bigbrain-surface-viewer)",https://github.com/aces/brainbrowser,aces/brainbrowser
7e0d31ae-f7bf-468e-9146-f00c4c45dc1b,verticox,Verticox+,"|   |   | |--|--| |Software DOI|  [![Software DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.13933626.svg)](https://doi.org/10.5281/zenodo.13933626)| |Reference paper DOI|[![Reference paper DOI](https://img.shields.io/badge/DOI-10.1007%2Fs40747--025--02022--4-blue)](https://doi.org/10.1007/s40747-025-02022-4)  # Verticox+ Verticox+ is a Cox proportional hazards algorithm for vertically distributed data, implemented for [vantage6](https://vantage6.ai/), a software framework for federated learning.  For more info, check the [documentation site](https://carrier-project.github.io/verticox/)",https://github.com/carrier-project/verticox,carrier-project/verticox
7eb2cb9a-b4a8-45f3-98fe-f60740362dd6,chemical-analytics-platform,Chemical Analytics Platform,"* The virtual machine contains a fully functional cheminformatics infrastructure like RDKit and PyMol * The virtual machine was used in several workshops, giving all participants the same environment and quick start",https://github.com/NLeSC/Chemical-Analytics-Platform,NLeSC/Chemical-Analytics-Platform
7ef56753-e62b-4833-9369-1d5ee455dd13,pyzacros,pyZacros,"* Provides a Python environment to run your favorite KMC code. * Seamlessly connected to ab-initio codes. * Support functions to handle outputs and plot results.  pyZacros is an Open Source project supported by the Netherlands eScience Center (NLeSC) and Software for Chemistry & Materials BV (SCM, and previously known as Scientific Computing & Modelling NV). The terms of the [LGPL-3.0 license]* apply. As an exception to the LGPL-3.0 license, you agree to grant SCM a [BSD 3-Clause license]** to the contributions you commit on this Github or provide to SCM in another manner.",https://github.com/SCM-NV/pyZacros,SCM-NV/pyZacros
7f6ab6c3-4e66-43dd-8fd9-d87da3820344,exploreasl,ExploreASL,"## Description  The software provides a complete head-to-tail approach that runs fully automatically, encompassing all necessary tasks from data import and structural segmentation, registration, and normalization, up to **CBF** quantification. In addition, the software package includes quality control (**QC**) procedures and region-of-interest (**ROI**) as well as voxel-wise analysis of the extracted data. To date, **ExploreASL** has been used for processing ~10000 **ASL** datasets from all major **MRI** vendors and **ASL** sequences and a variety of patient populations, representing ~30 studies. The ultimate goal of **ExploreASL** is to combine data from multiple studies to identify disease-related perfusion patterns that may prove crucial in using **ASL** as a diagnostic tool and enhance our understanding of the interplay of perfusion and structural changes in neurodegenerative pathophysiology.   Additionally, this (semi-)automatic pipeline allows us to minimize manual intervention, which increases the reproducibility of studies.   ## Documentation  Reference manual and tutorials for each ExploreASL version are found on the [GitHub website](https://exploreasl.github.io/Documentation). A general description of ExploreASL is in the [Neuroimage paper](https://pubmed.ncbi.nlm.nih.gov/32526385/). Additional resources are on the [ExploreASL website](https://www.ExploreASL.org) including the walkthrough document and how-to videos, but these are not regularly updated with new versions. For any help please use the [GitHub Discussion](https://github.com/ExploreASL/ExploreASL/discussions) or contact the ExploreASL team at [exploreasl.lab@gmail.com](mailto:exploreasl.lab@gmail.com).  ## Installation  To use **ExploreASL** within Matlab, you can download a stable release version from the [GitHub releases section](https://github.com/ExploreASL/ExploreASL/releases) or from [Zenodo](https://doi.org/10.5281/zenodo.7702237). Alternatively the software can also be found on [Dockerhub](https://hub.docker.com/r/exploreasl/xasl). Navigate within Matlab to the **ExporeASL** directory, to make **ExploreASL** the current working directory. To start ExploreASL from Matlab, type: ``` ExploreASL ```",https://github.com/ExploreASL/ExploreASL,ExploreASL/ExploreASL
802a07ad-71e3-4b39-9614-e5f9e6772f68,irida,IRIDA,"The **IRIDA** platform is composed of several different applications. Most users of IRIDA will use the web interface for managing and organizing their sequencing data, and for launching analytical pipelines on their data. Some advanced users of IRIDA might want to export their data to a separate instance of Galaxy or directly to the Linux command-line for more in-depth analysis. Administrative users and laboratory technicians will run tools for adding data to IRIDA from sequencing instruments.",https://github.com/phac-nml/irida,phac-nml/irida
8044f84d-b01e-438d-bab4-818fa6c1a836,nlppln,nlppln,"* Quickly build text mining and/or nlp workflows in Python * Combine tools written in different programming languages  Digital Humanities research often involves Natural Language Processing (NLP), in which a body of natural language text, or _corpus_, is analyzed using software. While there are many software packages available, constructing new research analyses by combining (parts of) existing packages remains challenging. This is due to the fact that individual software packages are designed to do a task and to do that task well; they are not primarily designed to interact with other, complementary packages. Another problem is that there are many tools available for English, but not for other languages.  nlppln (pronounced 'NLP pipeline') is an open source Python package that helps to address these problems, by making it easy to package existing tools in a uniform way as defined in the CWL (Common Workflow Language) standard for describing data analysis workflows. nlppln includes components to do tasks that are common in NLP, such as tokenization (multiple languages), lemmatization (for Dutch), and named entity recognition (for Dutch). These components are based on existing tools. Users can easily construct new analysis workflows by combining these pre-baked components with tools of their own creation.  Besides improving interoperability, nlppln also keeps a formal record of all steps taken in a workflow. This makes the research more transparent, and improves reproducibility.",https://github.com/nlppln/nlppln,nlppln/nlppln
80d71b6a-2ad3-4fca-b17a-e64287bf0fbb,open-matrices-stimulus-set-omss,Open Matrices Stimulus Set (omss),,https://github.com/aranvhout/OMSS_generator,aranvhout/OMSS_generator
816d1bc3-1f6e-4228-8eb8-f1a5f1e2f001,bartended-haddock3,haddock3-webapp,"Haddock3 (High Ambiguity Driven protein-protein DOCKing) is a an information-driven flexible docking approach for the modeling of biomolecular complexes. The haddock3 can be run from the command line. This software wraps the the haddock3 command line tool in a web application. The web application makes it easy to make a configuration file, run it and show the results.",https://github.com/i-VRESSE/haddock3-webapp,i-VRESSE/haddock3-webapp
82667cc8-1608-4e7b-9eda-76cd9af640b9,otree-demo-experiments,oTree Demo Experiments,,https://github.com/obeliss-nlesc/otree-experiments,obeliss-nlesc/otree-experiments
8267a6f6-dba7-47c7-b83b-9b80ad730cbd,pantools-pipeline-v4,PanTools-pipeline-v4,- Create a pangenome and run all major PanTools analysis functions for pangenomes. - Create a panproteome and run all major PanTools analysis functions for panproteomes.  - Create a pangenome and run all PanTools analysis functions required to make a PanVa instance,https://github.com/PanUtils/pantools-pipeline-v4,PanUtils/pantools-pipeline-v4
8276bf0b-db06-43c4-9ae0-676d6951bc52,pycoeus,Pycoeus,"Pycoeus is a command-line tool and python package designed for exploration of multiband geospatial datasets. It lets the user iteratively train and retrain classification models in seconds. Traditional machine learning approaches can lack intelligent predictions, while deep neural nets typically require vast amounts of training data and compute. Our tool combines a pretrained UNet for feature extraction with a RandomForest for classification. We therefore leverage strong points of both. The use of a UNet makes it possible to take into account complex patterns and encode these in a feature space. The RandomForest is then trained on top of those features with little labeled data, and only seconds (or minutes) of compute time required. This allows for an interactive and explorative user experience.  CoeusAI uses the functionalities of Pycoeus, it's main dependency, and makes them available with a user-friendly graphical user-interface to use in QGIS.",https://github.com/DroneML/pycoeus,DroneML/pycoeus
829801da-a416-496d-b325-3036809bcc5d,cdf2medmij-mapping-tool,CDF2Medmij-Mapping-tool,# CDF2Medmij-Mapping-tool  The CDF2Medmij-Mapping-tool is a data harmonization utility designed to transform cohort study datasets into MedMij-compliant FHIR formats. This tool enables interoperability between healthcare data sources by ensuring that disparate datasets adhere to the standardized information models used in personal health environments.  The application context of this tool is further described on the paper [MyDigiTwin: A Privacy-Preserving Framework for Personalized Cardiovascular Risk Prediction and Scenario Exploration](file:///Users/hcadavid/temp/2501.12193v1.pdf),https://github.com/MyDigiTwinNL/CDF2Medmij-Mapping-tool,MyDigiTwinNL/CDF2Medmij-Mapping-tool
829973cc-77d0-4a31-ab27-4747751532fe,classmodelform,classmodel/form,When you have a JSON schema describing the input of a program and you want to create a web form for it using Solidjs ui library and tailwind for styling. Then you can use the `@classmodel/form` js package.,https://github.com/classmodel/class-web,classmodel/class-web
82ae98e2-3fb4-433c-b8f7-5e059ca1a9c3,astrolibjl,AstroLib.jl,,https://github.com/JuliaAstro/AstroLib.jl,JuliaAstro/AstroLib.jl
833706f6-4753-451c-a27d-114f383cf146,4tu-matlab-tools-for-analysing-cell-curvature-interactions-used-in-the-publication-emergent-collective-organization-of-bone-cells-in-complex-curvature-fields,"Matlab tools for analysing cell-curvature interactions, used in the publication ""Emergent collective organization of bone cells in complex curvature fields"".","A collection of Matlab tools for analysing cell-curvature interactions, used in the publication ""Emergent collective organization of bone cells in complex curvature fields"", Callens et al., (bioRxiv, 2020).       The codes are for quantitative analysis of microscopy data, correlating intensities and cellular orientations to the underlying curvature. Example data is included, and the comments in the code provide explanation of the different steps.   Â    Required software tools:       Matlab (these codes were tested in 2018b and 2020a). Codes should work as soon as the software folder is copied in the working directory of Matlab. Running times for all codes on a standard desktop computer should not take more than a couple of seconds (at least below one minute). A set of example data is included.       Overview of the software folder:      example_data: data folder containing image data for a set of experiments (D8 convex structures & D5 convex unduloid for nuclei)   additional MAT-files: curvature maps and principal direction maps   Matlab scripts:            frequencyMap: generates frequency maps of actin signal of periodic units       intensityVSCurvatureMap: creates heat map of intensity vs k1 and k2, uses associated excel data sheets as input       intensityVSDistanceBatch: creates curve of intensity vs distance to k2<0       midlineIntensity: creates profile of actin intensity along the center line of a substrate       orientationROI: example script to compute SF orientation in a user-defined ROI       orientationROI_vs_PD: example script to compute SF orientation in a user-defined ROI and compare it to the principal directions       runx2IntBatch: computes normalized mean DAPI and RUNX2 intensities in masked images of the convex unduloid substrate (D8), at predefined ROIs       sheetSagging: script to compute vertical displacement of cell sheet and anchor density on spherical or cylindrical substrates. Requires user-input to draw mask.       nucleusHeat: script to compute density maps and plots of density vs distance to k2<0 for nuclei centroids.           Matlab functions are called in the associated scripts",,
8355c570-3f69-41dd-b883-f074d4acf0a0,reaxpro-wrappers,ReaxPro wrappers,This is a bundle of middle-layer mapping functions to facilitate chemistry workflows for heterogeneous catalytic simulations. It links the syntactic chemistry engines with the semantic ontology EMMO.,https://github.com/simphony/reaxpro-wrappers,simphony/reaxpro-wrappers
83e52fc0-6d91-4384-b0b3-07d38a572715,4tu-finite-strain-inverse-designed-spinodoids,finite-strain-inverse-designed-spinodoids,"Spinodal metamaterials, with architectures inspired by natural phase-separation processes, have presented a significant alternative to periodic and symmetric morphologies when designing mechanical metamaterials with extreme performance. While their elastic mechanical properties have been systematically determined, their large-deformation, nonlinear responses have been challenging to predict and design, in part due to limited data sets and the need for complex nonlinear simulations. This work presents a novel physics-enhanced machine learning (ML) and optimization framework tailored to address the challenges of designing intricate spinodal metamaterials with customized mechanical properties in large-deformation scenarios where computational modeling is restrictive and experimental data is sparse. By utilizing large-deformation experimental data directly, this approach facilitates the inverse design of spinodal structures with precise finite-strain mechanical responses. The framework sheds light on instability-induced pattern formation in spinodal metamaterialsâ€”observed experimentally and in selected nonlinear simulationsâ€”leveraging physics-based inductive biases in the form of nonconvex energetic potentials. Altogether, this combined ML, experimental, and computational effort provides a route for efficient and accurate design of complex spinodal metamaterials for large-deformation scenarios where energy absorption and prediction of nonlinear failure mechanisms is essential.",https://data.4tu.nl/v3/datasets/335d3b61-a83a-4c28-8a2f-0e622ac3ac0e.git,
84214e6a-9784-4a07-a87f-7b01e1b11bbc,mass-spec-studio-for-clinical-proteomics,Mass Spec Studio for Clinical Proteomics,"There are two phases to the development of a biomarker: discovery and validation. In discovery, an emerging best-in-class solution uses an untargeted mass spec method called â€œData Independent Acquisitionâ€ (DIA) that generates large, overlapping sets of signals. These signals must be mined for protein identity and abundance. After discovery, the activity shifts towards validation often using a more targeted mass spec method, called â€œParallel Reaction Monitoringâ€ (PRM). Easy-to-use but powerful software tools are required to accelerate clinical biomarker discovery.",,
844bd1c1-74ad-42e5-b24f-25af5c7c7321,virtual-health-gateway,Virtual Health Gateway,"The Virtual Health Gateway (VHG) is a unified system integrating patient-derived data across platforms and projects.   Through discussions with Canadian researchers, we determined that there is a growing demand for platforms that integrate data, not only from research instruments (e.g., medical imaging equipment), but also from patients who are in non-controlled settings. Non-controlled settings provide researchers with an opportunity to study real-world and long-term health outcomes.  The use of data generated through wearable devices, mobile devices, or online collection of patient-reported experiences and outcomes, presents numerous data management challenges. These include the requirements for advanced privacy and security controls and systems, technical issues associated with complex data sources (e.g., streamed data), and integration with data collected in other contexts. The Virtual Health Gateway is a platform that facilitates direct, consistent, and standardized capture of diverse data types from research study participants in real- world settings. The solution builds on previous CANARIE-funded work (Researcher Portal for Secure Data Discovery, Access, and Collaboration) which enables researchers to collect, manage, analyze, and share their study data by introducing participant engagement tools, processes, and interfaces.   Once integrated, the Virtual Health Gateway will not only allow participant information and data to be shared with researchers and clinicians, but also with study participants and caregivers. This can ultimately improve overall participant engagement and empower them throughout the research process.",https://vhg.indocresearch.org/,
852db33a-78c0-4b08-85c1-72b634941989,4tu-mpvas-experiments,mpvas-experiments,"Python code to reproduce results presented in the paper ""Privacy-Preserving Data Aggregation with Public Verifiability Against Internal Adversaries"". Specifically, to run an implementation of the mPVAS family of protocols and measures its runtime.     The source code was published by the paper's authors some time after the paper was published.     Usage  Minimal usage instructions: On a system running Debian 12, with GNU Make installed, run make install test run plot.     See README.md inside the git repository for detailed usage instructions.     Code  The source code is available as a git repository. The relevant code is stored in the src directory.",https://data.4tu.nl/v3/datasets/a3942b9c-6837-49d9-9ace-d980215c4254.git,
85603e11-e295-443a-917d-cecc027d168d,excited-workflow,EXCITED Machine Learning Workflow,"An open workflow for creating machine learning models for estimating the global biospheric CO2 exchange.  Using globally available data (e.g. ERA5, MODIS) as well as site-level data (Fluxnet) a model for the gross primary production and respiration can be trained. Additionally, the workflow aims to better constrain the CO2 exchange in terrestrial ecosystems on longer timescales using estimates from inverse models (e.g., CarbonTracker) as additional input data.  The workflow is currently geared towards carbon exchange (CO2), but can also be adapted for use with other fluxes such as CH4 or N2O.  The full workflow is split over several notebooks. These guide you through all steps required, from preprocessing, to model training and finally the dataset production.",https://github.com/EXCITED-CO2/excited-workflow,EXCITED-CO2/excited-workflow
859dce4f-0c9c-4371-921f-3cc26f45ee89,4tu-code-for-the-publication-parametric-excitation-and-friction-modulation-for-a-forced-2-dof-system,Code for the publication: Parametric excitation and friction modulation for a forced 2-dof system,"MATLAB script to reproduce results from:  Sulollari, E., van Dalen, K.N., and Cabboi, A. (2025). Parametric excitation  and friction modulation for a forced 2-DOF system. Nonlinear Dynamics, 113, 12793â€“12816.     This paper corresponds to Chapter 5 of the dissertation  ""Vibration-induced friction modulation""     This script generates the results shown in the figures of the paper.      Author: Enxhi Sulollari   Date: 20-08-2025     To get results for different parameters, change the values below.",,
85d4c61e-e8e6-4bb5-9bdc-244a9d9f0faf,amrvac,MPI-AMRVAC,"The MPI - Adaptive Mesh Refinement - Versatile Advection Code (MPI-AMRVAC) is a parallel adaptive mesh refinement framework aimed at solving (primarily hyperbolic) partial differential equations by a number of different numerical schemes. The emphasis is on (near) conservation laws and on shock-dominated problems in particular. A number of physics modules are included; the hydrodynamics and the magnetohydrodynamics module are most frequently used. Users can add their own physics module or modify existing ones. The framework supports 1D to 3D simulations, in a number of different geometries (Cartesian, cylindrical, spherical).  MPI-AMRVAC is written in Fortran 90 and uses MPI for parallelization. The VACPP preprocessor is used to extend Fortran with dimensional independent notation, but users are not required to learn the VACPP syntax.  The philosophy behind MPI-AMRVAC is to use a single versatile code with options and switches for various problems. The advantage of such a general approach is easier maintenance, the compatibility of different parts, and the automatic extension of new features to existing applications. MPI-AMRVAC is not a fool-proof black-box design. A user needs to write subroutines for initial conditions, and for source terms or special boundary conditions when needed.  MPI-AMRVAC is developed and maintained by an international team led by professor [Rony Keppens](https://perswww.kuleuven.be/~u0016541/) from [Centre for mathematical Plasma-Astrophysics (CmPA)](https://wis.kuleuven.be/CmPA), KU Leuven.",https://github.com/amrvac/AGILE-experimental,amrvac/AGILE-experimental
8629a662-7e3f-4fee-b339-9c1ea1142b00,rocket,Rocket,"* Framework for running all-pair computation applications on heterogeneous GPU clusters.  * Easy to use: User provides data set and necessary application-dependent functions, Rocket takes care of the rest. * Implemented in Java using the Constellation framework. * Includes three example application from bioinformatics, digital forensics, and localization microscopy.",https://github.com/JungleComputing/rocket,JungleComputing/rocket
86834031-bec6-4803-9e75-c33d1fc9c0d9,4tu-software-accompanying-the-publication-surrogate-dc-microgrid-models-for-optimization-of-charging-electric-vehicles-under-partial-observability,Software accompanying the publication: Surrogate DC Microgrid Models for Optimization of Charging Electric Vehicles under Partial Observability,"This repository will contain the code for the paper:Veviurko, G.; BÃ¶hmer, W.; Mackay L.; de Weerdt, M. Surrogate DC Microgrid Models for Optimization of Charging Electric Vehicles under Partial Observability, Energies 2022.",,
86e05c70-7ae7-4849-886f-6d85e76ea055,mmsoda-toolbox-for-matlab,MMSODA Toolbox for MATLAB,* Provides one programming interface to single-objective and multi-objective versions of SCEM-UA (by Vrugt et al.) * Provides a variety of plotting functions to monitor the optimization's progress and to improve understanding of the optimization results * Can do simplified data assimilation (direct insertion) * Can run locally or distributed on a cluster * Comes with a tutorial manual as well as extensive HTML documentation,https://github.com/NLeSC/esibayes,NLeSC/esibayes
86e70b48-d8b9-4464-9ce5-4100e8bfa965,4tu-imitation-learning-model-and-datasets-a-study-of-learning-search-approximation-in-mixed-integer-branch-and-bound-node-selection-in-scip,"Imitation learning model and datasets: ""A Study of Learning Search Approximation in Mixed Integer Branch and Bound: Node Selection in SCIP""","Imitation learning model and datasets corresponding to the AI article ""A Study of Learning Search Approximation in Mixed Integer Branch and Bound: Node Selection in SCIP"".",,
877b8dfb-2f0e-4c76-a0d1-6e8e27373c1b,pairedomicsdata,Paired omics data platform,* Provides web form to describe a metabolomics project * Is the document repository to store links between gene clusters and mass spectra * Is source for training datasets for software trying to find links between gene cluster and mass spectra * Uses JSON schema to validate projects * Constructed together the metabolomics/genomics community,https://github.com/iomega/paired-data-form,iomega/paired-data-form
877fbeea-2886-4fd9-aaf8-539415d1d334,4tu-mml-nbh-class-paper,mml_nbh_class_paper,"This repository contains the code and data processing scripts necessary to reproduce the analysis in ""The spatio-temporal evolution of social inequalities in cities: a multidimensional, multiscalar and longitudinal approach for neighbourhood classification"" by Ignacio Urria, Ana PetroviÄ‡, Maarten van Ham, and David Manley.     This repository provides the R scripts and functions used to create a cluster classification of multiscale bespoke neighbourhoods delineated around 100 by 100 metre grid cells in the Metropolitan Agglomeration of Amsterdam using a Non-Negative Matrix Factorisation (NMF).     The approach is multidimensional, multiscalar, and longitudinal, focusing on neighbourhood classification using various socioeconomic, demographic, and economic indicators measured across multiple spatial scales and time periods.     It is important to note that the authors do not have permission to share data. However, under certain conditions, these data are accessible for statistical and scientific research. For further information: microdata@cbs.nl.     Nevertheless, the code is shared to facilitate reproducibility and transparency, enabling other researchers to replicate, adapt, or extend the analysis with the same or different data sources.     Please refer to the README file for a more detailed description of the structure of the repository and the underlying data. Please refer to the original paper for a detailed description of the methodology and findings.",https://data.4tu.nl/v3/datasets/ce26a9cb-7238-4701-9cfb-d173d1c5dfc4.git,
879a49c6-fc3f-4097-8cdf-66ece5fe7413,4tu-scripts-for-awe-control-design-and-simulation,Scripts for AWE Control Design and Simulation,"Contains several scripts to trim, linearize, design controllers and simulate AWE systems. Also scripts to perform the SS are included.",,
87ae64e0-7533-4796-9158-ce2f5b3574b5,4tu-code-to-produce-the-results-of-the-publication-examining-the-role-of-intrinsic-and-reflexive-contributions-to-ankle-joint-hyper-resistance-treated-with-botulinum-toxin-a,Code to produce the results of the publication: â€œExamining the Role of Intrinsic and Reflexive Contributions to Ankle Joint Hyper-Resistance Treated with Botulinum Toxin-Aâ€,"This folder contains code to produce the results of ""R.C. van 't Veld, E. Flux, W. van Oorschot, A.C. Schouten, M.M. van der Krogt, H. van der Kooij, M. Vos-van der Hulst, N.L.W. Keijsers, E.H.F. van Asseldonk. Examining the Role of Intrinsic and Reflexive Contributions to Ankle Joint Hyper-Resistance Treated with Botulinum Toxin-A. Journal of NeuroEngineering and Rehabilitation. https://doi.org/10.1186/s12984-023-01141-8""",,
87b5aff7-48ae-45a0-92d2-4764fd917eb2,4tu-spacevlbi,spacevlbi,"Python package for simulating and optimising a space-based VLBI mission. This package enables multiple space telescopes to be modelled by propagating their orbital and attitude state. Elements of the spacecraft design that impact when observations can be performed can also be included in the simulation to assess and mitigate their impact on the science return of the mission (e.g. source visibility including Sun/Earth/Moon avoidance, star trackers, radiators, solar panels, ground station access times, etc.).    Checkout the README.md file for installation instructions, software requirements, etc. More information as well as the latest version of the software can be found in the GitHub repository.",https://data.4tu.nl/v3/datasets/b1b61036-229a-4a6f-8d28-81fa53f3be54.git,
87e0bb97-dc64-47c8-a324-37129218f1e3,lombscarglejl,LombScargle.jl,,https://github.com/JuliaAstro/LombScargle.jl,JuliaAstro/LombScargle.jl
87ea0ca0-b48d-4450-9e28-395a1ac1094a,4tu-code-for-paper-diameter-dependence-of-transport-through-nuclear-pore-complex-mimics-studied-using-optical-nanopores,Code for paper Diameter Dependence of Transport through Nuclear Pore Complex Mimics Studied Using Optical Nanopores,The dataset is structured in several zip archives further described. In the attached README the use of them is Â described in more detail.,,
883e4b92-c764-4d3a-b7d9-db02541a2a96,4tu-code-on-topology-switching-control-of-cvp-under-mitm-attack,Code on Topology Switching Control Of CVP Under MITM Attack,"The wireless communication used by vehicles in collaborative vehicle platoons (CVPs) is vulnerable to cyber-attacks, which threaten their safe operation. To address this issue, a safety preserving controller is presented. The proposed controller is based on topology-switching coalitional MPC, which utilises a RUIO to detect and isolate the cyber-attacks. Attacked communication links are then disabled to accommodate the attack. Furthermore, the MPC controller is designed to be resilient against undetected attacks and the uncertainty derived from disabling communication links. The uploaded code, illustrates the performance of the proposed scheme on a simulated CVP of four vehicles.       The code used to generate results presented in chapter 3 of the PhD dissertation of Twan Keijzer: Advances in Safety and Security of Cyber-Physical Systems - Sliding Mode Observers, Coalitional Control and Homomorphic Encryption",,
88409e84-0329-46cc-940c-b3657f5b3738,4tu-software-supporting-the-publication-background-electromagnetic-noise-received-via-ionospheric-propagation-in-a-remote-rural-location-in-spain-set-2,Software supporting the publication: Background Electromagnetic Noise Received via Ionospheric Propagation in a Remote Rural Location in Spain (set 2),"This software goes with the data set of a scientific experiment involving polarisation measurements in The Netherlands of electromagnetic waves refracted in the ionosphere during the impact of a solar X-ray flare, DOI 10.4121/21885825.   The software is used for reprocessing this dataset to demonstrate the significant drop in ambient electromagnetic noise when a solar X-ray flare impacts the ionosphere.",,
8848f44a-ef71-46fe-958a-5157f35e0ade,4tu-code-for-the-custom-search-engine-used-for-the-master-thesis-on-the-impact-of-biased-search-results-on-user-engagement-in-web-search,Code for the custom search engine used for the Master Thesis on the Impact of Biased Search Results on User Engagement in Web Search,Code for the custom search engine used for the Master Thesis on the Impact of Biased Search Results on User Engagement in Web Search.,,
884cdb58-37ad-4ae6-8693-f5cda13f0113,4tu-data-underlying-the-research-on-phylogenetic-network-rearrangement-move-heuristics,Data underlying the research on phylogenetic network rearrangement move heuristics.,Algorithms and data for the tests of a heuristic that determines an upper bound for the rearrangement move distance between two phylogenetic networks.,,
888bc2dc-a012-450e-a8e0-3ac7a7c6dc7b,cbrain,CBRAIN,"# Description  CBRAIN is a web-enabled grid platform that facilitates collaborative research on large, distributed data by creating an interface between a user (or a group of collaborating users) and high-performance computing centres (HPCs). Behind the web-portal, CBRAIN manages data access, transfer, caching and provenance, as well as data processing and reporting. CBRAIN is generic and modular, it can easily be extended with new data models and tools for a broad range of research disciplines.  ## It has several important capabilities:  * Files may be be stored anywhere: on local servers or on remote servers. * Files are moved automatically and upon demand (e.g. by user or a task), without interfering with platform components and without the user having to worry too much about their actual location. * Tasks can be launched at any scale: on local servers or on powerful compute clusters. CBRAIN supports clusters running Sun Grid Engine (SGE), MOAB, Torque/PBS, or simple UNIX processes, transparently. Other adapters are easy to implement. * Files can be displayed or accessed on the web interface according to their own internal representation. For example a JPEG file can be viewed, or an MRI file can be parsed slice-by-slice on the web interface. * Files and tasks are deployed as plugins, so a CBRAIN administrator can write his own or simply import them from other developers.  ## Where does it come from?  The CBRAIN service was originally conceived through a CANARIE grant awarded to Professor Alan C. Evans, at McGill University. The resulting internal code platform was meant to provide easy access to complex neuroimaging computational tools, for clinicians or neuroscience researchers with limited IT resources. Despite the name, the framework is designed to be general, and can accommodate any data and task for any application. This GIT repository contains the generic core of the framework, which is not specific to any field of science. Who should use CBRAIN?  Any research group that depends on medium to large-scale computational data analysis, can benefit from CBRAIN's data and task management back engine. CBRAIN provides tools for archiving raw and processed data files, viewing them, and processing then in batch. For example, an astronomy researcher's lab could deploy CBRAIN and configure its data servers for storage of its instrument's acquisitions, and to launch its in-house data processing/analysis software on some preconfigured supercomputer cluster.",https://github.com/aces/cbrain,aces/cbrain
88ade322-aa4e-46a2-867d-1bdda119e150,4tu-intervalmdpabstractions-jl-reproducibilitypackage,IntervalMDPAbstractions.jl_ReproducibilityPackage,"This dataset is a repeatability evaluation package for the paper ""Scalable control synthesis for stochastic systems via structural IMDP abstractions"", Frederik Baymler Mathiesen, Sofie Haesaert, Luca Laurenti, 2024. The core idea is to verify properties about stochastic dynamical systems by finding a finite-state representation, called an abstraction, which may more easily be verified. The repeatability package includes experiments of abstracting different types of stochastic systems (additive linear/affine, polynomial, neural network dynamic models, Gaussian processes, and stochastically switched systems) to Interval Markov Decision Processes (IMDPs), orthogonally decoupled IMDPs (odIMDPs), and mixtures of odIMDPs. odIMDPs are a new abstract model proposed in the paper, where the ambiguity sets of transition probabilities are specified as products of (marginal) interval ambiguity sets.     The dataset includes all benchmark instances, a Docker-based command-line interface, plotting and table generating code, code for comparison against baseline tools IMPaCT and SySCoRe. For instructions on how to run the package, please consult the README.md file of the dataset.",https://data.4tu.nl/v3/datasets/dc76361e-cd2f-4473-aaf6-1c39ed002e8d.git,
88ccae17-87b0-4dab-aa26-346aad7ca424,ewatercycle,ewatercycle,* Uses containers to run models in an isolated and portable way * Rain and sunshine required as input for the models can be generated by the package using ESMValTool * Exposes high level API modelled after PyMT to quickly get up and running * Has low level API follow the Basic Model Interface (BMI) to couple models together,https://github.com/eWaterCycle/ewatercycle,eWaterCycle/ewatercycle
88deebfe-8736-4db7-b75a-b652221ec5c0,4tu-gnss-perl-toolbox,GNSS-Perl-toolbox,"GNSS Perl toolbox with RINEX editing, filtering, statistics, time and file conversions, and a few other utilities. It contains Perl script and modules for     RINEX file editing, filtering and conversion (rnxedit, rnxstats, ...)GNSS date, time, file management and template processing (scanrnx32, gpstime, gpsdircmp, ...)   Also a few scripts for parsing raw receiver data are included.     The most recent version can be found on github ( https://github.com/hvandermarel/GNSS-Perl-toolbox ).",https://data.4tu.nl/v3/datasets/ef4dd243-135c-4e3b-8740-a1e6d6239d12.git,
890b6eba-933d-4d60-881a-3a91420d7c86,bridgedbr,BridgeDbR,"BridgeDbR is a tool that can do identifier mapping. Available [identifier mapping files](https://www.bridgedb.org/data/gene_database/) support ID mapping for genes and proteins (NCBI Gene, Ensembl, UniProt), metabolites (ChEBI, PubChem, InChIKey), and more.  To install this package, start R (version ""4.2"") and enter:  ```R if (!require(""BiocManager"", quietly = TRUE))     install.packages(""BiocManager"")  BiocManager::install(""BridgeDbR"") ```  To view documentation for the version of this package installed in your system, start R and enter:  ```R browseVignettes(""BridgeDbR"") ```",https://github.com/bridgedb/BridgeDbR,bridgedb/BridgeDbR
89d6dad1-c565-4a89-84c6-e7e1cd583fe0,4tu-video-annotation-software,Video Annotation Software,"The videos of surgical operations such as endoscopic videos, microscopic videos, and OR videos, are a rich source of data, that can reveal surgical improvement points and thus greatly benefit the improvement of surgical procedures.   For this purpose, a software system, Video Annotation Software, is designed and developed to enhance data extraction from the videos of different types of surgery. In addition to the surgical video analysis, the software system has the potential for many other video analyses in other fields.",,
8a130361-1fc7-4cf4-97f7-95520164ccbe,esmvalcore,ESMValCore,"* Finding and automatically downloading data from ESGF * ESMValTool preprocessor functions based on Iris for e.g. regridding, vertical interpolation, statistics, correcting (meta)data errors, extracting a time range, etcetera. * High flexibility: new preprocessor functions and support for more datasets can be easily added. * CF/CMOR compliant: data from many different projects can be handled (CMIP, obs4mips, ERA5, ICON, etc.). Routines are provided to CMOR-ize non-compliant data in-memory.  The ESMValCore software package provides the core functionality for ESMValTool, the Earth System Model eValuation Tool. It provides a configurable framework for finding CMIP files using a â€œdata reference syntaxâ€, applying commonly used pre-processing functions to them, running analysis scripts, and recording provenance. Numerous pre-processing functions, e.g. for data selection, regridding, and statistics are readily available and the modular design makes it easy to add more. The ESMValCore package is easy to install with relatively few dependencies, written in Python 3, based on state-of-the-art open-source libraries such as Iris and Dask, and widely used standards such as YAML, NetCDF, CF-Conventions, and W3C PROV. An extensive set of automated tests and code quality checks ensure the reliability of the package.  The ESMValCore package uses human-readable recipes to define which variables and datasets to use, how to pre-process that data, and what scientific analysis scripts to run. The package provides convenient interfaces, based on the YAML and NetCDF/CF-convention file formats, for running diagnostic scripts written in any programming language. Because the ESMValCore framework takes care of running the workflow defined in the recipe in parallel, most analyses run much faster, with no additional programming effort required from the authors of the analysis scripts. For example, benchmarks show a factor of 30 speedup with respect to version 1 of the tool for a representative recipe on a 24 core machine. A large collection of standard recipes and associated analysis scripts is available in the ESMValTool package for reproducing selected peer-reviewed analyses.",https://github.com/ESMValGroup/ESMValCore,ESMValGroup/ESMValCore
8a5e3dcd-c3ca-4991-9228-929ffecfbb78,pssmgen,PSSMGen,"* Easy-to-use tool to calculate PSSM * Generate PSSM files for docking models of protein-protein complexes * Generate PSSM-matched PDB files  PSSMGen is geared toward computing the pssm files for all models of a particular protein-protein complex. This tool assumes your files have following structure:  caseID  |_ pdb  |_ fasta  |_ pssm_raw  |_ pssm  |_ pdb_raw caseID is the ID of protein-protein complex, e.g. PDB ID 1AK4 or CAPRI target ID T161. Only the pdb dir must exist at run time, and it must contain the PDB files of the models generated by HADDOCK. Based on these PDB files, the code will extract protein sequences and generate the FASTA files (stored in the fasta subdir). It will then use the BLAST tool psiblast to compute PSSM (stored in pssm_raw subdir). Finally the sequences from PSSM files and PDB files will be aligned and then the consistent PSSM files and/or PDB files will generated. The consistent PSSM files are stored in pssm subdir. If applicable, the consistent PDB files are stored in pdb subdir, while the original non-consistent PDB files are moved to pdb_raw subdir. You can then use PDB files from pdb and PSSM files from pssm for further analysis.",https://github.com/DeepRank/pssmgen,DeepRank/pssmgen
8ab0421e-03ed-4eb0-8c37-1846b5b6eef0,4tu-tud-r-cafe-plot-a-thon-4tu-rd-stats,TUD R Cafe Plot-a-thon: 4TU.RD stats,Overview of general 4TU.RD stats.,https://data.4tu.nl/v3/datasets/6b06c437-50e8-4f1f-abd9-83e5c194753f.git,
8b29947b-8eac-48e9-a581-4716a93be90d,lofar-linked-data-platform,lofar-ld,- exposes LOFAR Long Term Archive catalog (medata) through SPARQL and FAIR Data Point APIs - uses domain-specific ontologies and controlled vocabularies for astronomy-related concepts,https://github.com/EOSC-LOFAR/lofar-ld,EOSC-LOFAR/lofar-ld
8b3facc1-e0e6-4b92-89e9-6199c3f5fa40,fairtally,fairtally,"Given a list of links to GitHub and GitLab repositories, `fairtally` can collect data on each repository's compliance with the five recommendations for FAIR software from fair-software.nl   - Given a list of links to GitHub and GitLab repositories, `fairtally` can collect data on each repository's compliance with the five recommendations for FAIR software from fair-software.nl - Works for repositories on github.com or gitlab.com - Command line tool, installable from PyPI",https://github.com/fair-software/fairtally,fair-software/fairtally
8b7739b3-0291-4b59-a6c8-9d4379a5917c,4tu-code-underlying-the-publication-maintenance-optimization-for-multi-component-systems-with-a-single-sensor,Code underlying the publication: Maintenance Optimization for Multi-Component Systems with a Single Sensor,"This is the code used to generate the results for the paper Maintenance Optimization for Multi-Component Systems with a Single Sensor by Ragnar Eggertsson, Ayse Sena Eruguz, Rob Basten, and Lisa M. Maillart. The paper introduces a novel model to optimize maintenance interventions for a multi-component system with a single sensor. The model is formulated as a partially observable Markov decision process. The Python script in this repository implements the algorithm, discussed in the paper, that solves the model. This algorithm is a type of incremental pruning algorithm. Running this algorithm generates the results presented in the section Illustrative Examples.",https://data.4tu.nl/v3/datasets/c8e927b4-baa5-4bda-ad15-f56a99b52f12.git,
8b8d8c43-0931-45f7-8e77-a32bb796d6e1,amber,AMBER,* Real-time radio transient searching pipeline * High-performance and portability,https://github.com/AA-ALERT/AMBER,AA-ALERT/AMBER
8be22043-109b-4cf9-84c5-7d589ff46df7,slicerrt,SlicerRT,"# Builds on a dynamic platform SlicerRT is an extension of [3D Slicer](https://www.slicer.org/), a free, open source software for visualization and image analysis. SlicerRT can be [installed](http://slicerrt.github.io/Download.html) from the 3D Slicer Extension Manager on Windows, Mac, and Linux to leverage the advanced features of 3D Slicer in adaptive radiation therapy research  # Covers common RT research workflows SlicerRT includes close to 20 modules that provide tools for radiation therapy research, including advanced deformable registration methods powered by the Plastimatch library. Standard DICOM-RT format is supported, thus integrating with the treatment planning systems   # SlicerRT is open research Our development and research work is public, including source code, data, manuals, presentations, etc. SlicerRT is distributed under the BSD-style Slicer [license](https://www.slicer.org/wiki/LicenseText) allowing academic and commercial use without any restrictions. See our [OpenHub](https://www.openhub.net/p/slicerrt) site for further details.",https://github.com/SlicerRt/SlicerRT,SlicerRt/SlicerRT
8bf8ea77-9043-486b-aeb2-8b0128c81d54,4tu-software-for-self-adaptive-and-on-the-fly-mapping-of-coastal-parameters-from-video-of-a-wave-field,Software for self-adaptive and on-the-fly mapping of coastal parameters from video of a wave field,"Software for self-adaptive and on-the-fly mapping of coastal parameters from video of a wave field. The software concerns code written in Python and has a Matlab-like file structure. The algorithm analyses video in orthorectified format to estimate depths, but also surface currents and wave phase velocities. For a quick user guide see the README.md or https://github.com/MatthijsGawehn/COCOS. The algorithm currently goes under the name of COCOS (COastal COmmunity Scout). The algorithm and its performance are published in MDPI Remote Sensing as: ""Gawehn, M.; de Vries, S.;Aarninkhof, S. A self-adaptive method for mapping coastal bathymetry on-the-fly from wave field video""",,
8c462e2c-b7c1-4338-9153-ce6a57a3a4fa,4tu-mean-field-quantum-networks,mean-field-quantum-networks ,"Simulation code for performance analysis on the d-choices load-balancing policy on entanglement generation switch (EGS), an architecture for quantum entanglement switches. The simulation codes describes the empirical behaviour of the EGS using the d-choices policy. The simulation codes are in the simulations/ folder with the extension `.jl`, and the code to plot the figures is `simulation/plot_python.py`. The directory `code` consist of old codes in earlier iterations and they are for archiving purpose only.",https://data.4tu.nl/v3/datasets/7502cb4b-374b-4cae-a3ba-3913a68ec6cb.git,
8c7f7d84-a6d0-4c6d-a695-40f1adb90322,4tu-script-for-aid-prigshare-automation-of-indicator-development-for-green-space-health-research-in-qgis,Script for: AID-PRIGSHARE: Automation of Indicator Development for Green Space Health Research in QGIS,"Accompanying Script to the PRIGSHARE Reporting Guidelines.The PRIGSHARE_QGIS_Script will produce green space and greenness indicators in distances from 100-1.500m every 100m automatically.  A frequent demand in the interdisciplinary field of green space health research is to reduce the effort to assess green space, especially for non-spatial disciplines. Realizing this issue, we developed AID-PRIGSHARE. AID-PRIGSHARE is an open-source script with an easy-to-use user interface that substantially reduces the time-intensive and complex task of green space indicator generation by automatization. AID-PRIGSHARE will simultaneously calculate indicators such as mean greenness, total public green space, access to green infrastructure, and green space uses for distances of 100-1500m in 100m steps around a (home) address if the input layers are provided. This substantially reduces the effort for sensitivity analysis and may support research that aims to understand better the individual characteristics of green spaces and their effect range.",https://data.4tu.nl/v3/datasets/963ae02a-dfa7-48dc-9527-78a64fd48af0.git,
8d06b4ab-e9b6-4a09-812f-acbcbb288345,4tu-data-and-code-underlying-the-publication-stratifying-oncogene-addicted-cohorts-by-drug-response,Data and code underlying the publication: Stratifying Oncogene-addicted Cohorts by Drug Response,"This repository consists of Data/Code to reproduce the results of the paper ""OncoStratifier: Stratifying Oncogene-addicted Cohorts By Drug Response"".  The full results of the experiments ( and detailed result table) are shared here as oncostratifier_results.zip.  The full supplementary tables are shared here as Oncostratifier_Supplementary_Tables.xlsx&nbsp;.  The code is shared here in the main branch.  All data to reproduce is public and can be found through the methodology of the paper.  The detailed plots and paper are at: https://oncostratifier-701b8c83a149.herokuapp.com/",https://data.4tu.nl/v3/datasets/b0b00f9f-064e-43cd-8391-a6c6512de715.git,
8e1a3713-5725-42db-9e70-ddc11660283b,minted,MINTED,"The MINTED project (Making Identifiers Necessary to Track Evolving Data) is funded by the CANARIE Network, and supports Ocean Network Canadaâ€™s need to implement PIDs to datasets to be renewed for the CoreTrustSeal, a certification of repositories using best practices.Â   MINTED aims to integrate dataset citation DOIs and RORs (Research Organization Registry identifiers) into ONCâ€™s Oceans 3.0 digital infrastructure. ONC's data are very dynamic due to continually accumulating data streams, data reprocessing, and data product code versioning. While there has been a growing recognition of the need for and benefits of data citations, as evidenced by the reception of the FAIR Principles, existing platforms and tools such as Dataverse and the Federated Research Data Repository (FRDR) are currently only able to serve the needs of static, or infrequently updated, datasets. At ONC, we have an opportunity to go further, building the infrastructure within our repository to implement theÂ 14 recommendations laid out by the RDA Working Group on Dynamic Data Citation.  MINTED allows data users to cite data in the repository from the beginning of a deployment, providing traceability of the dataset life cycle so that users can better interpret data integrity, respect the terms and conditions under which the data are made available, and increase the credibility of users. Data citation also allows users to link datasets to publication DOIs, contributor ORCIDs or RORs, funder reference IDs, ARK IDs, and more.Â   It makes strides in enabling reproducibility, provides curated datasets to end users, provides credit for publishing and providing data, helps repositories and users to track metrics of data usage and impact, and enhances metadata catalogues and products for citation content. Additionally, data citation stimulates related research as published datasets that have been cited lend themselves more readily to further analysis.Â   Lastly, the MINTED project also allows ONC to renew our certification with World Data System (WDS) CoreTrustSeal (CTS), identifying ONC as a repository that has implemented and supports FAIR (Findable, Accessible, Interoperable, and Reusable) data principles and best practices, encouraging confidence in the content within.Â   With the MINTED Project, Ocean Networks Canada is proud to be leading the way in implementing the RDA Recommendations on a large scale. In partnership with CANARIE, the DataCite Canada Consortium, and our own crack team of software engineers, ONCâ€™s work will help establish best practices for good data stewardship now and into the future.",,
8e7ce44f-28e1-445c-97a8-aedd6d27394d,4tu-implementation-regarding-publication-fuzzy-logic-based-model-predictive-control-a-paradigm-integrating-optimal-and-common-sense-decision-making,"Implementation Regarding Publication  ""Fuzzy-Logic-based model predictive control: A paradigm integrating optimal and common-sense  decision making""","The code in this repository is used to implement and test a new algorithm called FLMPC (Fuzzy logic based model predictive control). To find an optimal path in search and rescue for multi-robot systems, it is common to use MPC with stochastic cost functions. We decided to replace stochastic cost functions with fuzzy cost functions (the exact reasoning can be found in the paper (there is no DOI yet) ). In this repository, MPC with stochastic cost function, FLMPC and bi-level FLMPC have been implemented. This code generates simulations with people and obstacles. Robots are then spawned within them. The drones have no knowledge of the environment, but they can sense the environment around them, but the measurements are noisy. The task for the drones is to find people as quickly as possible. We published raw data in seperate repository (https://doi.org/10.4121/2479c468-624b-49b6-9e2e-63bd633c9bc2) because they were taking over 14 GB of disk space.",https://data.4tu.nl/v3/datasets/1284fd2b-663c-4c0d-9af7-7dc1ca390945.git,
8e8a0cb6-f0ec-4f2a-b1f8-89695625fe46,hpgem,HPGEM,Solve differential equations using Discontinuous Galerkin Finite Element Methods on complex meshes. The meshes can be imported from popular mesh format and can even be generated from experimental data (2d and 3d) using the [nanomesh](https://github.com/hpgem/nanomesh),https://github.com/hpgem/hpgem,hpgem/hpgem
8ea04189-da4f-458a-9e51-14654ec65d8c,eeg-epilepsy-diagnosis,EEG epilepsy diagnosis,"As this was a short lasting project, we mainly focused on processing multi-variate EEG (Electroencephalography) data as a proof of concept.  The software handles the data format and structure used in one particular study carried out by colleagues from Utrecht University in data collected in sub-saharan Africa, but uses generic external libraries (caret) for the machine learning.   This software was developed as part of the Young eScience Award 2015 (awarded to Wim Otte in 2015, but project took place in 2016).  The project description can be found here https://www.esciencecenter.nl/project/diagnosis-of-active-epilepsy-in-resource-poor-setting A technical report on the project can be found here:https://www.biorxiv.org/content/10.1101/324954v1",https://github.com/NLeSC/EEG-epilepsy-diagnosis,NLeSC/EEG-epilepsy-diagnosis
8ecff570-460f-478d-9acd-2fa0da432cab,minimism,minimism,"minimism is a skeleton for performing ice-sheets simulations spanning millions of years. It can scale to very high processor counts as opposed to its predecessor UFEMISM, which was bound by a single node. The simulations are performed on a mesh which is generates according to the layout of the ice-sheet, with areas needing more precision being more detailed.",https://github.com/IMAU-paleo/minimism,IMAU-paleo/minimism
8f9c3c78-ef90-40ea-8a9a-33f4001fd9fc,overture-score,Overture Score,"# What is Score?  Score facilitates the transfer and storage of your data seamlessly for cloud-based projects. File bundling, resumable downloads, and BAM/CRAM slicing make data transfer fast and smooth.  The method with which Score facilitates the transfer of data is through the use of pre-signed URLs. As such, Score can be thought of as a broker between an object storage system and user authorization system, validating user access and generating signed URLs for object access.  # Features  * Multipart Uploads and Downloads (high performance transfers) * Support for AWS S3, Azure Storage, Google Cloud Storage * Slicing of BAM and CRAM files by genomic region * Client includes some samtools functionality such as viewing reads from a BAM * MD5 validation of uploads and downloads * ACL security using OAuth2 and scopes based on study codes * Integrates with the SONG metadata system for data book keeping and consistency * REST API with swagger docs",https://github.com/overture-stack/score,overture-stack/score
900bb1fa-9dad-42fb-a38e-de7c05e364fc,dales,DALES,,https://github.com/dalesteam/dales,dalesteam/dales
901c6eb9-b7c8-4de3-b2b8-ca4ac027a12b,fsbrain,fsbrain,"The fsbrain R package provides a well-tested and consistent interface to neuroimaging data in R. It supports reading, writing, and visualizing various kinds of raw data and statistical results on brain surfaces and volumes. While the package provides a very convenient interface for working with data arranged in the standard FreeSurfer directory structure (SUBJECTS_DIR), fsbrain is not limited to this layout or FreeSurfer file formats. You can load brain meshes, volumes, and data from a range of other neuroimaging software packages and visualize them.  The plots produced by fsbrain can be integrated into R notebooks or written to high-quality bitmap image files, ready for publication. The rgl renderer used by fsbrain provides fast, hardware-accelerated rendering based on the OpenGL standard.",https://github.com/dfsp-spirit/fsbrain,dfsp-spirit/fsbrain
906ec70c-4266-4aba-88d4-1ca4c4b80e7f,asrlitcompare,asrlitcompare,,https://github.com/langtonhugh/asreview_irr,langtonhugh/asreview_irr
90b72623-261d-4dad-a809-f11ccfde974b,carbon-budget-explorer-cabe,Carbon Budget Explorer (CABE),"Many countries commit to rapid climate action, but their ambitions don't add up to reach the Paris goals. It is not trivial to define which countries should do more. This is a matter of fairness. But what is fair in this context? Should rich countries mitigate more, because they have the means to do so? Or the countries that have emitted a lot in the past? Clearly, there is no single answer, because it depends on what you think is fair. However, these principles can be quantified. The Carbon Budget Explorer allows navigating through the implications of different views on fairness.",https://github.com/carbon-budget-explorer/cabe,carbon-budget-explorer/cabe
90c32eb9-6158-4fbd-bc69-c5458bfbfc8c,mkdocs-rich-argparse,mkdocs rich argparse,,https://github.com/i-VRESSE/mkdocs_rich_argparse,i-VRESSE/mkdocs_rich_argparse
90fc5373-57f5-47b5-a294-b063dbc1d1b9,4tu-frequency-stability-of-graphene-nonlinear-resonators-matlab-code,Frequency stability of graphene nonlinear resonators: Matlab Code,"MATLAB code from master thesis Frequency stability of graphene nonlinear resonators   See http://resolver.tudelft.nl/uuid:96b47031-ff15-4057-ab33-1ac4863119be   Introduction These files contain code on three topics:    Calculate the Allan Deviation with more control about the number of points calculated.    Parse stream files from the Zurich Instruments UHF into .mat files, one per signal.    Model and configuration files for the simulations.    Allan Deviation The file is located in the root of this structure and called allandev.m. The comments contain the information required to run it.   UHFLIStreamParser This directory contains a set of files to parse stream files into .mat files per signal.   Main file is UHFLIStreamParser.m. The other files are helpers for this function or wrappers to select a batch of stream files to parse.   Simulation The simulation directory contains the Simulink model used in the master thesis: PLL_duffing_with_thermal_noise_nonlinear_damping.slx.   To run this, one needs to load parameters for the system. A start can be found in graphene_drum_parameters.m.   A phase sweep can be executed by running run_nonlinear_damping_experiments_in_simulations.m.   The other files in this directory are helper functions.    Author: Ties Verschuren Â© 2020 - MIT licence.",,
912f6e2f-a8f4-42e6-b4f1-1347486857a5,multilayergraphs,MultilayerGraphs.jl,"# MultilayerGraphs.jl   **MultilayerGraphs.jl** is a Julia package for the creation, manipulation and analysis of the structure, dynamics and functions of multilayer graphs.   ## Overview  A multilayer graph is a graph consisting of multiple standard subgraphs called *layers* which can be interconnected through [bipartite graphs](https://en.wikipedia.org/wiki/Bipartite_graph) called *interlayers* composed of the vertex sets of two different layers and the edges between them. The vertices in each layer represent a single set of nodes, although not all nodes have to be represented in every layer.   Formally, a multilayer graph can be defined as a triple $G=(V,E,L)$, where:  - $V$ is the set of vertices; - $E$ is the set of edges, pairs of nodes $(u, v)$ representing a connection, relationship or interaction between the nodes $u$ and $v$; - $L$ is a set of layers, which are subsets of $V$ and $E$ encoding the nodes and edges within each layer.  Each layer $\ell$ in $L$ is a tuple $(V_\ell, E_\ell)$, where $V_\ell$ is a subset of $V$ that represents the vertices within that layer, and $E_\ell$ is a subset of $E$ that represents the edges within that layer.  Multiple theoretical frameworks have been proposed to formally subsume all instances of multilayer graphs ([De Domenico  et al. (2013)](https://doi.org/10.1103/physrevx.3.041022); [KivelÃ¤ et al. (2014)](https://doi.org/10.1093/comnet/cnu016); [Boccaletti et al. (2014)](https://doi.org/10.1016/j.physrep.2014.07.001); [Lee et al. (2015)](https://doi.org/10.1140/epjb/e2015-50742-1); [Aleta and Moreno (2019)](https://doi.org/10.1146/annurev-conmatphys-031218-013259); [Bianconi (2018)](https://doi.org/10.1093/oso/9780198753919.001.0001); [Cozzo et al. (2018)](https://doi.org/10.1007/978-3-319-92255-3); [Artime et al. (2022)](https://doi.org/10.1017/9781009085809); [De Domenico (2022)](https://doi.org/10.1007/978-3-030-75718-2)).   Multilayer graphs have been adopted to model the structure and dynamics of a wide spectrum of high-dimensional, non-linear, multi-scale, time-dependent complex systems including physical, chemical, biological, neuronal, socio-technical, epidemiological, ecological and economic networks ([Cozzo et al. (2013)](https://doi.org/10.1103/physreve.88.050801); [Granell et al. (2013)](https://doi.org/10.1103/physrevlett.111.128701); [Massaro and Bagnoli (2014)](https://doi.org/10.1103/physreve.90.052817); [Estrada and Gomez-Gardenes (2014)](https://doi.org/10.1103/physreve.89.042819); [Azimi-Tafreshi (2016)](https://doi.org/10.1103/physreve.93.042303); [Baggio et al. (2016)](https://doi.org/10.1073/pnas.1604401113); [DeDomenico et al. (2016)](https://doi.org/10.1038/nphys3865); [Amato et al. (2017)](https://doi.org/10.1038/s41598-017-06933-2); [DeDomenico (2017)](https://doi.org/10.1093/gigascience/gix004); [Pilosof et al. (2017)](https://doi.org/10.1038/s41559-017-0101); [de Arruda et al. (2017)](https://doi.org/10.1103/physrevx.7.011014); [Gosak et al. (2018)](https://doi.org/10.1016/j.plrev.2017.11.003); [Soriano-Panos et al. (2018)](https://doi.org/10.1103/physrevx.8.031039); [Timteo et al. (2018)](https://doi.org/10.1038/s41467-017-02658-y); [BuldÃº et al. (2018)](https://doi.org/10.1162/netn_a_00033); [Lim et al. (2019)](https://doi.org/10.1038/s41598-019-39243-w); [Mangioni et al. (2020)](https://doi.org/10.1109/tnse.2018.2871726); [Aleta et al. (2020)](https://doi.org/10.1038/s41562-020-0931-9); [Aleta et al. (2022)](https://doi.org/10.1073/pnas.2112182119)).   MultilayerGraphs.jl is an integral part of the [JuliaGraphs](https://github.com/JuliaGraphs) ecosystem extending [Graphs.jl](https://github.com/JuliaGraphs/Graphs.jl) so all the methods and metrics exported by Graphs.jl work for multilayer graphs, but due to the special nature of multilayer graphs the package features a peculiar implementation that maps a standard integer-labelled vertex representation to a more user-friendly framework exporting all the objects an experienced practitioner would expect such as nodes (`Node`), vertices (`MultilayerVertex`), layers (`Layer`), interlayers (`Interlayer`), etc.  MultilayerGraphs.jl features multilayer-specific methods and metrics including the global clustering coefficient, the overlay clustering coefficient, the multilayer eigenvector centrality, the multilayer modularity and the Von Neumann entropy.  Finally, MultilayerGraphs.jl has been integrated within the [JuliaDynamics](https://github.com/JuliaDynamics) ecosystem so that any `Multilayer(Di)Graph` can be utilised as an argument to the `GraphSpace` constructor in [Agents.jl](https://github.com/JuliaDynamics/Agents.jl).   ## Installation  To install MultilayerGraphs.jl it is sufficient to activate the `pkg` mode by pressing `]` in the Julia REPL and then run the following command:  ```nothing pkg> add MultilayerGraphs ```  ## How to Contribute   The ongoing development of this package would greatly benefit from the valuable feedback of the esteemed members of the [JuliaGraph](https://github.com/orgs/JuliaGraphs/people) community, as well as from graph theorists, network scientists, and any users who may have general questions or suggestions.   We therefore encourage you to participate in [discussions](https://github.com/JuliaGraphs/MultilayerGraphs.jl/discussions), raise [issues](https://github.com/JuliaGraphs/MultilayerGraphs.jl/issues), or submit [pull requests](https://github.com/JuliaGraphs/MultilayerGraphs.jl/pulls). Your contributions are most welcome!  ## How to Cite  If you utilize this package in your project, please consider citing this repository using the citation information provided in [`CITATION.bib`](https://github.com/JuliaGraphs/MultilayerGraphs.jl/blob/main/CITATION.bib).   This will help to give appropriate credit to the [contributors](https://github.com/JuliaGraphs/MultilayerGraphs.jl/graphs/contributors) and support the continued development of the package.  ## Announcements   The package and its features were announced on the following platforms:  - [Discourse](https://discourse.julialang.org/t/ann-multilayergraphs-jl-a-package-to-construct-handle-and-analyse-multilayer-graphs/85988) - [Forem](https://forem.julialang.org/inphyt/ann-multilayergraphsjl-a-package-to-construct-handle-and-analyse-multilayer-graphs-3k22) - [Twitter](https://twitter.com/In_Phy_T/status/1560594513189638146)  ## References  1. De Domenico et al. (2013) [Mathematical Formulation of Multilayer Networks](https://doi.org/10.1103/PhysRevX.3.041022). *Physical Review X*;  2. KivelÃ¤ et al. (2014) [Multilayer networks](https://doi.org/10.1093/comnet/cnu016). *Journal of Complex Networks*;  3. Boccaletti et al. (2014) [The structure and dynamics of multilayer networks](https://doi.org/10.1016/j.physrep.2014.07.001). *Physics Reports*;  4. Lee et al. (2015) [Towards real-world complexity: an introduction to multiplex networks](https://doi.org/10.1140/epjb/e2015-50742-1). *The European Physical Journal B*;  5. Bianconi (2018) [Multilayer Networks: Structure and Function](https://global.oup.com/academic/product/multilayer-networks-9780192865540). *Oxford University Press*; 6. Cozzo et al. (2018) [Multiplex Networks: Basic Formalism and Structural Properties](https://doi.org/10.1007/978-3-319-92255-3). *SpringerBriefs in Complexity*;  7. Aleta and Moreno (2019) [Multilayer Networks in a Nutshell](https://doi.org/10.1146/annurev-conmatphys-031218-013259). *Annual Review of Condensed Matter Physics*;  8. Artime et al. (2022) [Multilayer Network Science: From Cells to Societies](https://doi.org/10.1017/9781009085809). *Cambridge University Press*;  9. De Domenico (2022) [Multilayer Networks: Analysis and Visualization](https://doi.org/10.1007/978-3-030-75718-2). *Springer Cham*.",https://github.com/JuliaGraphs/MultilayerGraphs.jl,JuliaGraphs/MultilayerGraphs.jl
91589468-a7a2-482d-863e-d5ba9369e107,sirup,sirup,"`sirup` allows you to manage your IP address form within python. This means you can change your IP address over time and hide your identity. This is useful for downloading data from websites for research projects. However, it is important to highlight that web scraping and IP rotation need to respect the law and should only be a last resort.   For more information and a tutorial, see this [blog post](https://blog.esciencecenter.nl/how-to-manage-your-ip-address-in-python-75a2c9eda648)",https://github.com/ivory-tower-private-power/sirup,ivory-tower-private-power/sirup
91656ad4-6442-4058-bfab-b5ee32c54b89,4tu-code-supporting-the-publication-gromit-benchmarking-the-performance-and-scalability-of-blockchain-systems,Code supporting the publication: Gromit: Benchmarking the performance and scalability of blockchain systems,This repository contains the code for the Gromit blockchain benchmarking tool. The tool is based Gumby framework and has been modified to support the benchmarking of blockchain fabrics. Gromit supports the following blockchain platforms and versions:  Algorand (v2.3.0)Avalanche (v1.1.1)Bitshares (v5.0.0)Hyperledger Burrow (v0.34.4)Ethereum (v1.9.24)Hyperledger Fabric (v1.4.9)Diem (v1.1.0)Stellar (v15.1.0)All code related to experiments can be found in the experiments directory.,https://data.4tu.nl/v3/datasets/2c7e436f-5c41-47e5-9b3a-15f1cc9708f8.git,
918b7012-929b-4c33-803e-51477356666f,4tu-artifact-to-the-paper-a-fast-and-verified-probabilistic-model-checking-pipeline-for-minimal-reachability-probabilities,"Artifact to the paper ""A Fast and Verified Probabilistic Model Checking Pipeline for Minimal Reachability Probabilities""","This artifact contains the proofs of correctness of the Interval Iteration (II) pipeline including min-reduction for MDPs as well as a modified version of mcsta from the Modest Toolset with models to reproduce the benchmarks.  The high-level proofs are described in the paper ""A Fast and Verified Probabilistic Model Checking Pipeline for Minimal Reachability Probabilities"" submitted at LOPSTR 2025.  The proof is divided into a correctness proof for the abstract algorithm, a proof of correctness for the underlying data structures, and a refinement to LLVM performed within the Isabelle Refinement Framework.  Running the proofs yields an LLVM implementation of the II algorithm.  Once compiled into a library, it can directly be used in our modified version of mcsta to reproduce the experiments from our paper.  To streamline the process, we provide scripts that perform the tasks automatically (e.g. copying, moving, removing files and running the benchmarks).     This artifact is designed to run on a 64-bit Linux distribution (we include a Docker image) with internet access and at least 32GB of RAM and support for the AVX512 instruction set.   If you do not use Docker, it requires installations of Isabelle 2025 including the AFP, clang and optionally, python.",https://data.4tu.nl/v3/datasets/4e5289d1-9fbf-47da-9f4c-9752c0321980.git,
91923831-1122-4007-9423-41880cffe2af,talkr,talkr,,https://github.com/elpaco-escience/talkr,elpaco-escience/talkr
9211bf0d-640a-49ef-b002-2435624a7488,arcticconnect,ArcticConnect,"## Arctic Web Map (AWM): Arctic Web Map (AWM) provides an Arctic-specific web mapping tool allowing researchers to customize map projections for scientifically accurate visualization and analysis, a function that is critical for arctic system research but missing in existing web mapping platforms.  It will also provide a visually appealing tool for education and outreach to a wider audience.  * https://webmap.arcticconnect.ca/#ac_3573/2/90.0/0.0  ## Arctic Scholar (AS): Arctic Scholar enables researchers, educators, interested private sector entities, government agencies, and the general public to access and share arctic data and information contained in assorted formats including publications, grey literature, research licenses, photo archives, field notes, and project metadata from arctic field stations.  * https://records.arcticconnect.ca/#ac_3573/4/90.00/0.00  ## Arctic Sensor Web (ASW): Arctic Sensor Web (ASW) enables research stations around the pan-Arctic to connect their sensors, including those that provide near-real time data, to a cloud service for visualization, information sharing, and collaborative analysis.  * http://sensorthings.arcticconnect.ca  ## Arctic BioMap (ABM): Arctic BioMap (ABM) will enable members of the scientific community and northern residents to contribute observations on arctic animal species for the purpose of biodiversity monitoring, assessment, research, management and education.""",https://github.com/GeoSensorWebLab/arctic-portal,GeoSensorWebLab/arctic-portal
92d2b3e7-3356-492d-a7e4-58c8bdc77984,4tu-iui2025-convxai,IUI2025_ConvXAI,"This repository contains all necessary data and code for IUI 2025 paper ""Is Conversational XAI All You Need? Human-AI Decision Making With a Conversational XAI Assistant"". It is also available on Github: https://github.com/delftcrowd/IUI2025_ConvXAI. In our study, we provided different XAI interfaces (XAI dashboard and conversational XAI interfaces with different setup). We collected their collaborative decision making data with AI systems. These data can be useful for people who are interested in human-AI decision making.",https://data.4tu.nl/v3/datasets/750aacba-0fed-4774-a9c3-dd21d1dada84.git,
930ea46b-9005-481e-aa26-a93bbbcf6bcf,4tu-artifact-for-the-paper-a-formally-verified-ieee-754-floating-point-implementation-of-interval-iteration-for-mdps,"Artifact for the paper ""A Formally Verified IEEE 754 Floating-Point Implementation of Interval Iteration for MDPs""","This artifact contains the proofs of correctness of the Interval Iteration (II) algorithm for MDPs as well as a modified version of mcsta from the Modest Toolset with models to reproduce the benchmarks. The high-level proofs are described in the paper ""A Formally Verified IEEE 754 Floating-Point Implementation of Interval Iteration for MDPs"" accepted at CAV 2025. The proof is divided into a correctness proof for the abstract algorithm, a proof of correctness for the underlying data structures, and a refinement to LLVM performed within the Isabelle Refinement Framework. Running the proofs yields an LLVM implementation of the II algorithm.  Once compiled into a library, it can directly be used in our modified version of mcsta to reproduce the experiments from our paper. To streamline the process, we provide scripts that perform the tasks automatically (e.g. copying, moving, removing files and running the benchmarks).     This artifact is designed to run on a 64-bit Linux distribution (we include a Docker image) with internet access and at least 32GB of RAM and support for the AVX512 instruction set. If you do not use Docker, it requires installations of Isabelle 2025 including the AFP, clang and optionally, python.",https://data.4tu.nl/v3/datasets/e5db01c4-34b5-4bd9-adc0-b9a32a1b9057.git,
931493f2-393a-46b6-98ac-9540ea5a0b7f,4tu-matlab-code,Matlab code,Matlab code for data processing,,
93cb9dcc-6ebb-4042-ac15-7a271981da86,powerfit,powerfit,PowerFit is a Python package and simple command-line program to automatically fit high-resolution atomic structures in cryo-EM densities. To this end it performs a full-exhaustive 6-dimensional cross-correlation search between the atomic structure and the density. It takes as input an atomic structure in PDB-format and a cryo-EM density with its resolution; and outputs positions and rotations of the atomic structure corresponding to high correlation values. PowerFit uses the local cross-correlation function as its base score. The score can optionally be enhanced by a Laplace pre-filter and/or a core-weighted version to minimize overlapping densities from neighboring subunits. It can further be hardware-accelerated by leveraging multi-core CPU machines out of the box or by GPU via the OpenCL framework. PowerFit is Free Software and has been succesfully installed and used on Linux and MacOSX machines.,https://github.com/haddocking/powerfit,haddocking/powerfit
93f00fd1-a93a-42a4-b9de-ea78a8ee9a5d,4tu-blade-element-momentum-theory-code-for-positive-and-negative-thrust-propellers,Blade Element Momentum Theory Code for Positive and Negative Thrust Propellers,"Blade-element-momentum theory code for predicting propeller performance at positive and negative thrust conditions for uniform inflow conditions. The code includes corrections for compressibility and root-tip effects and is able to handle the calculations for the operation of propellers in the energy-harvesting regime (negative thrust conditions).     For the validation data of the code, please refer to the paper ""Benchmarking of Aerodynamic Models for Isolated Propellers Operating at Positive and Negative Thrust"".     You will need to have suitable polar data of your propeller geometry to use this code. This code is written in MATLAB.     Check out the README.md file for instructions on how to use the code.",,
941d1b50-ab1e-49cc-8332-c9b895b2ec2e,ece2cmor3,ece2cmor3,"* Process EC-Earth climate model output to CMIP6-compliant NetCDF files * Create EC-Earth output configurations from CMIP6 data requests * Explore what variables can be produced with EC-Earth * Support for atmosphere, ocean, land and chemistry sub-models * Parallel execution of cdo commands processing the atmospheric GRIB files  ece2cmor3 is a python package that allows climate scientists to post-process raw EC-Earth climate model output to CMIP6-compliant, annotated NetCDF4 files with a few simple python scripts. The software is easily installed on your local HPC facility using Anaconda and makes efficiently use of multi-core machines by executing the post-processing in a parallel queue over variables (for the atmospheric data). Furthermore, ece2cmor3 contains various helper scripts to configure the EC-Earth namelists and extract EC-Earth-supported variables from a given data request.",https://github.com/EC-Earth/ece2cmor3,EC-Earth/ece2cmor3
944100e6-b27d-42ba-8b6c-f88427327787,4tu-source-code-and-data-for-the-experiments-presented-in-deep-reinforcement-learning-for-active-wake-control,Source code and data for the experiments presented in Deep Reinforcement Learning for Active Wake Control,"This is a simulation study to illustrate benefits of reinforcement learning (RL) for active wake control in wind farms. The repository includes a simulator (./code/wind_farm_gym), implementation of RL agents (./code/agent), and configurations for the experiments presented in the paper (./code/configs), as well as the simulation results (./data). For more detailed instructions, see README.md.",,
944a6b81-a5a6-40ce-82bd-f943014b9b6d,4tu-software-underlying-phd-thesis-ai-in-the-sky-advancing-wildlife-survey-methods-in-africa-with-deep-learning-and-aerial-imagery, Software underlying PhD thesis: AI in the Sky - Advancing Wildlife Survey Methods in Africa with Deep Learning and Aerial Imagery.,"ZyPro is a deep learning framework designed to support the PhD research project â€œAI in the Sky: Advancing Wildlife Survey Methods in Africa with Deep Learning and Aerial Imagery.â€ The research aims to improve wildlife monitoring using semantic segmentation and object detection on remote sensing imagery. ZyPro provides tools for training, testing, and predicting with U-Net-based neural networks, as well as specialized modules for remote sensing image processing, such as large image handling, multi-channel data processing, image clipping, tiling, and data augmentation. This software project includes custom loss functions, flexible training pipelines, and various utilities to facilitate high-quality analysis of remote sensing data.",https://data.4tu.nl/v3/datasets/2ff22b95-7bcc-4ab3-bf64-b2a8d2fdc96f.git,
9460425e-35e7-4512-ad0a-04537a460f27,stratpal,StratPal,,https://github.com/MindTheGap-ERC/StratPal,MindTheGap-ERC/StratPal
9472f6f8-95e0-4cd8-baa2-1c3e0f9c99e0,tortellini-github-action,tortellini,* Run license analysis on your repository * Run license analysis on a list of repositories * Detect licensing violations * Summarize potential licensing issues * Generate a report with summary,https://github.com/tortellini-tools/action,tortellini-tools/action
94c1c589-b96b-452e-bcc8-860ea0adc78f,4tu-code-for-tiresias-predicting-security-events-through-deep-learning,Code for Tiresias: Predicting Security Events Through Deep Learning,"This repository contains the code for for Tiresias that was implemented as part of the IEEE S&amp;P DeepCASE paper [PDF], it provides a Pytorch implementation of Tiresias [PDF]. We ask people to cite both works when using the software for academic research papers.",https://data.4tu.nl/v3/datasets/55fc9c66-c70f-4fdb-bbcf-797caf6bccaf.git,
94f2e3cf-f25a-4714-8a92-5a23182667a0,4tu-software-for-industrial-mobile-manipulation-virtual-micro-challenge-2022,Software for Industrial Mobile Manipulation Virtual Micro Challenge 2022,"Virtual Micro Challenge 2022 is a part of the Industrial Mobile Manipulation Challenge (IMMC) - an international initiative funded by EIT-Manufacturing, aiming to promote mobile manipulation technology and make progress in the field of human-machine co-working in manufacturing.   This repository contains software packages for Virtual Micro Challenge 2022 and it is supposed to be used by the participants and organizers.",,
94f7cfe6-8968-42d5-ae0f-7ad1d57d044a,4tu-greenspaceperception-github-code-repository-analysis-underlying-the-publication-how-well-do-ndvi-and-openstreetmap-data-capture-people-s-visual-perceptions-of-urban-greenspace,"GreenspacePerception (Github) code repository: analysis underlying the publication ""How well do NDVI and OpenStreetMap data capture peopleâ€™s visual perceptions of urban greenspace?""","These notebooks allow to collect spatial data, specifically NDVI, OpenStreetMap and Google Street View metadata, and conduct the quantitative analysis to research how well NDVI and OpenStreetMap data capture what people visually perceive as being urban greenspaces. These notebooks were used in complementation by data collection through a crowdsourcing questionnaire to collect people's perceptions of places presented in Google Street View imagery. The quantitative analysis was followed up by a qualitative analysis, of which the codebook is added to this repository as well as an .xlsx file.  This code was developed for three cities in Europe: Barcelona, Rotterdam, and Gothenburg, but can be adapted to fit other geographical contexts.",,
95c15e68-3c63-4338-b16e-bb00377bcd1f,snakemake-storage-plugin-rucio,snakemake-storage-plugin-rucio,,https://github.com/bouweandela/snakemake-storage-plugin-rucio,bouweandela/snakemake-storage-plugin-rucio
95d0c7fe-e0bf-4ffe-bcd0-6d1d073a26cd,4tu-dueca-dusime-middleware-for-implementing-distributed-real-time-simulations,DUECA/DUSIME Middleware for Implementing Distributed Real-Time Simulations,"C++ Middleware for distributed real-time simulation and data acquisition and control projects.Â        Modules written to use DUECA can communicate with other modules running in a possibly distributed DUECA process, no matter where these other modules are located. In addition each module can specify its update rate, and access to data of the ""right time"" is automatically given by DUECA.       DUSIME (Delft University SIMulation Environment) builds forth on DUECA, to give you an environment tailored to implement real-time (hardware in the loop and man in the loop) simulations.",,
95ec22f4-b3bd-4c6e-8fa7-c4fe2b647a46,osmium,Osmium,"* Uses Xenon to copy files and run executables on different storage systems and batch schedulers * Copies input files from your application server to the scheduler location and the result files back * To prevent polling for the job status, osmium can push status updates to your application server * Used in production to run jobs locally and on grid infrastructure",https://github.com/NLeSC/osmium,NLeSC/osmium
95fbf413-3c1f-4666-8e2e-dec4f987b993,4tu-python-code-for-sand-scraper-analysis,Python code for Sand Scraper Analysis,This repository contains the code that was used to analyze data collected with the Sand Scraper.,,
95fcb929-bcf2-4e48-ae72-4872054d8dd4,trustchain,Trustchain,"Trustchain can be useful for any community wishing to share digital information in a verifiable and trustworthy manner.  It enables eligible legal entities (organizations or individuals) to issue digital credentials which can later be verified by any interested party.  These credentials can relate to individuals in a decentralized digital identity system, but could equally be used to prove the authenticity of digital media or other digital artefacts.  When used for identity, the system avoids many of the problems typically associated with centralized digital identity systems. Users remain in control of their data at all times and share it selectively. There is no central data collection or storage, and no user registration necessary. By taking advantage of robust decentralized infrastructure, the system can be deployed at extremely low cost without compromising on security or resilience.  There are many possible use cases that Trustchain could support, but two examples are:    - Qualified medical practitioners moving between hospitals could verifiably prove their medical registration status.   - Workers in any industry could prove their educational or professional qualifications, or their right to work in a given jurisdiction.  Aside from individual identity, Trustchain also enables ""credentials for data"". Use cases here include certification of digital media (videos, photos, text) released by online publishers or journalists, or verifiable provenance of any published datasets such as AI models or training data.",https://github.com/alan-turing-institute/trustchain,alan-turing-institute/trustchain
9606daf5-39fc-4033-a131-5c046c2fe275,qubols,qubols,,https://github.com/quantumapplicationlab/qubols,quantumapplicationlab/qubols
967b7117-3745-412a-8e49-9ee72f597301,4tu-filtering-explanding-graph-signals,Filtering-explanding-graph-signals,"Research Objective is to design online algorithms for graph filter design over expanding graphs under conditions of known and unknown connectivity. The data-sets used in this paper are available online. Code for generating synthetic data is included. In the folder Recsys_new, the experimental setup and online algorithms for movie rating prediction for Movielens100k is provided. In the folder Stochastic_Synthetic_New, the experimental setup and online algorithms for signal interpolation for synthetic expanding graphs is included. In Stochastic_covid, the code for Covid case count prediction over a growing city network is provided.",https://data.4tu.nl/v3/datasets/d97a97df-0f71-4581-b081-6262eaee2082.git,
96811bfe-f1bf-46e0-8fdb-404252e60e9f,4tu-nsg2023-eminversion,NSG2023-EMinversion,"The repository contains Python scripts to calculate lookup tables using semi-analytic and low induction number approximation forward models of frequency domain electromagnetic induction measurements of 2-layered electrical conductivity earth models. Additionaly, the repository holds field data acquired using a DUALEM842s instrument. Finally, Jupyter Notebooks scripts to display the results.",https://data.4tu.nl/v3/datasets/92494f17-c1aa-4c07-b79e-7ae08c3da4ed.git,
96b8fcac-3854-4ecf-8f1c-712595b53259,cwrc-writer,CWRC-Writer,"The Canadian Writing Research Collaboratory (CWRC) has developed an in-browser text markup editor (CWRC-Writer) for use by individual scholars and collaborative scholarly editing projects that require a light-weight online editing environment. This package is the base code that builds on the TinyMCE editor, and is meant to be bundled together with two other packages that provide document storage and entity lookup. A default version of the CWRC-Writer that uses GitHub for storage is available for anyone's use at https://cwrc-writer.cwrc.ca/.  CWRC-Writer provides close-to-WYSIWYG editing and enrichment of text files with meaningful visual representations of markup, the ability to add Named Entity Annotations with identifiers from LOD sources, the ability to combine TEI markup for the text with stand-off RDF annotations, the ability to export into different forms (XML, RDF, and JSON-LD), and the option of using Github for document storage.",https://github.com/cwrc/CWRC-WriterBase,cwrc/CWRC-WriterBase
96eee11b-4a25-47a3-9141-323a5f0ded6b,howfairis,howfairis,"- Analyze a git repository's compliance with the Five recommendations for FAIR software from https://fair-software.eu - Works for repositories on github.com or gitlab.com - Command line tool, installable from PyPI",https://github.com/fair-software/howfairis,fair-software/howfairis
97087cc0-9545-4a54-b2e4-86125ecaa161,4tu-handzone-software-for-hybrid-learning-environment-for-robotics-in-architecture-engineering-and-construction,"HANDZONe: Software for Hybrid Learning Environment for Robotics in Architecture, Engineering, and Construction","HANDZONe is a Hybrid Learning Environment developed to support education on robotics, specifically within architecture, engineering, and construction disciplines. It features a virtual lab with tutorials and exercises introducing fundamental knowledge and skills for programming and operating a robotic arm. It utilizes the Unity engine to create a digital twin of the robotic arm and a web server to enable real-time communication between the physical and digital labs.",,
9719beee-7fe0-45ee-8a80-aa5f53a9b605,motus-wildlife-tracking-system,Motus Wildlife Tracking System,"# About Motus When tracking wildlife with automated radio telemetry over vast distances, the challenge of deploying enough receivers to get detections grows exponentially. To remedy this, data can be shared between all researchers so that essentially everyone is sharing receivers. This greatly expands the potential for this technology, but it comes with the added responsibility of coordinating projects, detection data and metadata â€“ thatâ€™s where Motus comes in.  # What is Motus? The Motus Wildlife Tracking System is an international collaborative network of researchers that use automated radio telemetry to simultaneously track hundreds of individuals of numerous species of birds, bats, and insects. The system enables a community of researchers, educators, organizations, and citizens to undertake impactful research and education on the ecology and conservation of migratory animals. When compared to other technologies, automated radio telemetry currently allows researchers to track the smallest animals possible, with high temporal and geographic precision, over great distances.  # How does Motus work? The philosophy behind Motus is that weâ€™re all working together. At its core, Motus is community science. A community of researchers around the world conducting research on animals that are tracked by a network of coordinated receiving stations. These stations are maintained by a community of researchers, organizations, non-profits, governments, and individuals. In order for this concept to work, the system requires a centralized database and management system that all participants use. Most importantly, in order for your tags to be detected on any other station in the network, or for other project tags to be detected elsewhere, projects, receivers and tags need to be registered with, and have data processed by Motus. While any automated telemetry project can operate in isolation, operating as a Motus project combines the collective impact of local, regional, and even hemispheric projects into one massive collaborative effort that expands the scale and scope of everyoneâ€™s work and maximizes the use of scarce research dollars. It also makes data available and more useful for future projects, collaborative endeavors and large-scale meta analyses.  # Whatâ€™s the cost? There is NO cost to register your project and receivers to the Motus network, contribute data from those receivers, and use the resources on the website. In order for transmitters to be detected by the network, they must be reregistered with Motus and are charged a nominal fee to support data processing and ongoing maintenance and development of the research software platform. See the [collaboration policy](https://motus.org/policy/) and fee schedule for more information.  # Services **Motus Client Data API:** This hosted web Application Programming Interface (API) is linked to the central Motus database, which serves as a backbone for much of the proposed data access and data visualizations functionalities for the hosted web platform, and can also be accessed by third party applications including the Motus R package. [Link](https://motus.org/canarie-api/client-api/service/info)  **Motus R Packages:** This service handles data access, processing, summary and analysis through dedicated R packages operated by Motus users. [Link](https://motuswts.github.io/motus/)  **Motus SensorGnome Software:** This service handles sensor operations, data collection and scheduling, and connectivity through a web interface hosted on the SensorGnome sensor, which is used by the Motus Wildlife Tracking System. Its main purpose is to detect radio signals from encoded tags attached to small animals, store and transmit the information to the main Motus database. [Link](https://docs.motus.org/sensorgnome/)  **Motus Server Data API:** This hosted service is used for internal communications between Motus servers (e.g. metadata registrations, internal data transfers). This API can also be used by equipment manufacturers collaborating with Motus (e.g. register tags or receivers metadata). [Link](https://motus.org/canarie-api/server-api/service/info)  **Motus Visualization API:** The Motus Wildlife Tracking System (Motus) is a novel automated radio-telemetry system for tracking the local, regional and continental movements of animals such as birds, bats and insects. This hosted web service provides an access point to a variety of data outputs (and formats, such as JavaScript code, html outputs, multimedia files).  # Data Ownership/Privacy The collaborative nature of Motus relies on a certain level of transparency with respect to data. While basic project and tag summary information is made publicly available, researchers have the ability to customize data accessibility and keep their project and data private if necessary. See the [collaboration policy](https://motus.org/policy/) for more information.",https://github.com/MotusWTS/motus,MotusWTS/motus
975e26fc-41da-4156-b537-e0c7d8735876,rucio-policy-package-for-km3net,Rucio policy package for KM3NeT,Always wondered how to create a custom policy for rucio? Well this is the custom policy for rucio from [KM3NeT](https://www.km3net.org/). In combination with the rucio-deployment it might be very informational. Or perhaps you want to edit this specific policy? That can also be done by opening a pull request on the repository!,https://git.km3net.de/rucio/km3net_rucio_policy,
97c2eae0-cfef-43cb-a9a0-96fc79487064,e2edutch,e2e-Dutch,"* Coreference resolution for Dutch in Python * Adaptation of the state-of-the-art e2e model with BERT to identify references to the same entity in Dutch texts * Easy to use,  can be used stand-alone or as part of a Stanza pipeline   E2e-Dutch is an adaptation of the state-of-the art [e2e-model for English](https://github.com/kentonl/e2e-coref) for Dutch. It contains a model trained on the largest publicly available dataset for Dutch, SoNaR.",https://github.com/Filter-Bubble/e2e-Dutch,Filter-Bubble/e2e-Dutch
97f8e031-2bb5-43aa-8e0a-110e20f405b8,4tu-etv-routing-assignment,etv_routing_assignment,"This repository contains the code developed for scheduling simulations for electric towing vehicles (ETVs). It is applied to Amsterdam Airport Schiphol. This code was developed at Delft University of Technology, as part of Simon van Oosterom's PhD Thesis project (2025). It is being made public both to act as supplementary data for publications and the PhD thesis of Simon van Oosterom, and in order for other researchers to use this repository in their own work.",https://data.4tu.nl/v3/datasets/78f33eba-0629-4271-b6a3-ec28f1569320.git,
98327f52-e4ab-48d9-9438-4218ad17a72c,ewatercycle-wflowjl,ewatercycle-wflowjl,"**This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**  Wflow.jl plugin for eWatercycle.  The Wflow.jl documentation is available at https://deltares.github.io/Wflow.jl/ .",https://github.com/eWaterCycle/ewatercycle-wflowjl,eWaterCycle/ewatercycle-wflowjl
9858ebdf-103a-476b-88ea-8b792abcbffe,4tu-auction-tas,auction-tas,"This repository contains the code developed for the research work of Ilias Parmaksizoglou (2024), specifically for the studies related to Truck Appointment Systems:     A novel auction-based truck appointment system for marine terminals   The repository is made public to serve as supplementary material for the above publication and the PhD thesis of Ilias Parmaksizoglou, and to allow other researchers to use and build upon this work in their own studies.",https://data.4tu.nl/v3/datasets/0d33f8c2-d196-41c0-aabd-124cf01ef0bb.git,
989179af-65ec-4914-8b8e-e3dd221cbf93,ocean-networks-canada,Ocean Networks Canada,"# Who We Are  **Ocean Networks Canada (ONC)** is a world-leading research and ocean observing facility hosted and owned by the University of Victoria, and managed by the not-for profit ONC Society. ONC operates unparalleled observatories in the deep ocean and coastal waters of Canadaâ€™s three coastsâ€“the Arctic, the Pacific and the Atlanticâ€“gathering biological, chemical, geological and physical data to drive solutions for science, industry and society.  ONCâ€™s observatories collect data via cabled, mobile and community networks. Our infrastructure supplies Internet connectivity to thousands of subsea instruments up to 300 kilometres offshore, to depths of 2,660 metres. Sensor data are also captured on ferries, gliders, fixed buoys and moorings, and from coastal installations hosting radar, ship traffic sensors, weather stations and onshore cameras. ONCâ€™s national coastal community observatory program, developed in collaboration with Indigenous communities and other partners, supports coastal monitoring, school programs, ocean citizen science and youth training. Data that drives innovation  ONC enables advances in marine safety and many disciplines including high energy particle physics, forensics, material sciences, oceanography, seismic resilience, and climate change impacts and solutions.  ONCâ€™s sophisticated data management system, Oceans 3.0, is recognized by the International Council for Science as a World Data System, providing data 24/7 from a vast array of sensors and instruments. These live data, and archives, are open-source, interoperable and accessible to more than 32,000 global users. Ocean Intelligence  Ocean intelligence is the bridge that connects data and knowledge to action and change. It is derived from diverse sources and ways of knowing, including the natural and social sciences, observations and modelling, the arts and from Indigenous knowledge attained over generations. In-depth knowledge of the ocean is the foundation of ocean intelligence. ONC applies this knowledge, guided by our values, to make responsible decisions and advance policy for a healthy ocean and a resilient planet.  # Source code: Please contact [ONC](https://wiki.oceannetworks.ca/display/O2KB/Source+Code) for more information on the source code.  ONC does maintain a [GitHub repository](https://github.com/orgs/OceanNetworksCanada/repositories) for some projects, however most code is integrated into ONC's system and not readily usable.  # Services: **Oceans 3.0 API - Data Product Delivery Service:** Allows researchers to programmatically request and download Ocean Networks Canada (ONC) data and data products for specific properties at a location or on a device. Depending on the data product, data can be either returned immediately or downloaded as files in multiple supported formats. Data products can be pre-generated analysis datasets or generated on-demand, based on input  parameters such as time range, subsampling, filtering or output format information  **Oceans 3.0 API - Deployments Discovery Service:** Allows researchers to programmatically discover Ocean Networks Canada (ONC) devices that have observation data for specific device categories, properties, time ranges, geographic locations, data products and more. Data Products are downloadable representations of ONC observational data, provided in formats that can be easily ingested by analytical or visualization software.  **Sensor Observation Service - OM-JSON implementation:** [Link](https://wiki.oceannetworks.ca/display/O2A/sos) The Sensor Observation Service (SOS) provides an API for managing deployed sensors and retrieving sensor data and specifically 'observation data.'  **Sensor Observation Serviceâ€šsuperseded by OM-JSON implementation:** This service will be superseded by the new OM-JSON implementation.  **WARN Status:** The WARN Status service provides an API for querying the status of WARN earthquake and tsunami detection.  **Oceans 3.0 API - Device Categories Discovery Service:** Allows researchers to programmatically discover Ocean Networks Canada (ONC) devices that have observation data for specific device categories, and properties.  **Oceans 3.0 API - Devices Discovery Service:** Allows researchers to programmatically discover Ocean Networks Canada (ONC) devices that have observation data.  **Oceans 3.0 API - Locations Discovery Service:** Allows researchers to programmatically discover Ocean Networks Canada (ONC) locations that have observation data.  **Oceans 3.0 API - Properties Discovery Service:** Allows researchers to programmatically discover Ocean Networks Canada (ONC) devices that have observation data for specific device categories, and properties.  **Oceans 3.0 Dashboards:** Allows users to highlight and share a variety of data via user-configurable multidisciplinary Data Dashboards.  **Oceans 3.0 Sandbox:** Allows researchers to define, test, use and share processing code for user-defined data products in a custom-designed programming environment.",,
992bcb33-691b-411c-9a73-5e163a9aff3e,4tu-keithley-arduino-data-logger-and-controller,Keithley_Arduino_data_logger_and_controller,"We investigated the flow and heat transfer characteristics of blocks made from MnFePSi magnetocaloric material produced using an extrusion-based additive manufacturing technique. An experimental setup was developed to collect data on pressure drop, flow rate, bulk fluid temperature, and solid surface temperature. Six samples were tested, each fabricated with varying fiber diameters and inter-fiber spacings, covering a range of void fractions and surface areas. The overlall dimensions of the samples  were 16.1 x 16.1 x 6.2 mm^3. Pressure drop experiments were conducted for each sample at three distinct temperatures, approximately 7â€¯Â°C, 22â€¯Â°C, and 50â€¯Â°C. For each temperature, the flow rate ranged from values above 2 Lpm to below 0.4 Lpm. Heat transfer experiments were performed by self-heating the samples using electrical current to create a temperature difference with the water stream. The temperatures of the 4 surfaces of the blocks parallel to the flow direction were recorded, and a heat transfer model was used to derive heat transfer coefficients. This repository stores two datasets related to this research project. One dataset consists of .csv files with pressure drop and heat transfer data generated by a custom-developed data logging system. A second dataset contains scanning electron microscope (SEM) images of the top and bottom surfaces of the 3D printed blocks, taken to determine their average fiber diameter and inter-fiber spacing.",https://data.4tu.nl/v3/datasets/2d5a2551-3565-4a6f-9df0-99d7af18ca6d.git,
9a011493-0747-47ce-9dca-075cd01ef38b,ewatercycle-hype,ewatercycle-hype,"Generate HYPE forcing and run the HYPE hydrological model in a container using the eWaterCycle package.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/ewatercycle-hype,eWaterCycle/ewatercycle-hype
9a7906bf-328d-40cb-bdd0-176561440f97,4tu-code-underlying-the-publication-a-benchmark-for-the-application-of-distributed-control-techniques-to-the-electricity-network-of-the-european-economic-area,Code underlying the publication: A Benchmark for the Application of Distributed Control Techniques to the Electricity Network of the European Economic Area,"The European Economic Area Electricity Network Benchmark (EEA-ENB) is a multi-area power system representing the European network of transmission systems for electricity to facilitate the application of distributed control techniques. In the EEA-ENB we consider the Load Frequency Control (LFC) problem in the presence of renewable energy sources (RESs), and energy storage systems (ESSs). RESs are known to cause instability in power networks due to their inertia-less and intermittent characteristics, while ESSs are introduced as a resource to mitigate the problem. In the EEA-ENB, particular attention is dedicated to Distributed Model Predictive Control (DMPC), whose application is often limited to small and homogeneous test cases due to the lack of standardized large-scale scenarios for testing, and due to the large computation time required to obtain a centralized MPC action for performance comparison with DMPC strategies under consideration. The second problem is exacerbated when the scale of the system grows. To address these challenges and to provide a real-world-based and control-independent benchmark, the EEA-ENB has been developed. The benchmark includes a centralized MPC strategy providing performance and computation time metrics to compare distributed control within a repeatable and realistic simulation environment.",https://data.4tu.nl/v3/datasets/78170934-ebb2-4774-9cf9-ec584ad088a0.git,
9b17ed10-80df-4a2c-b97f-db7e6833302c,stm,STMtools,"STMtools is an open-source Python toolbox to read, write, and enrich Space-Time Matrix (STM) [1], [2]. It uses utilizes Xarrayâ€™s multi-dimensional labeling feature, and Zarr's chunk storage feature, to efficiently read and write large Space-Time matrix.   It also supports STM enrichment with contextual data. The enrichment functionality is implemented with Dask. Therefore it can be performed in a paralleled style on Hyper-Performance Computation (HPC) system.  [1] https://doi.org/10.1109/IGARSS47720.2021.9553453 [2] https://doi.org/10.1109/IGARSS47720.2021.9554887",https://github.com/TUDelftGeodesy/stmtools,TUDelftGeodesy/stmtools
9b18ffab-a61c-482e-be0b-5177fca45d52,deeprankcore,DeepRank2,"# DeepRank2  |     Badges     |                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | | :------------: | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- | |  **fairness**  | [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/6403/badge)](https://bestpractices.coreinfrastructure.org/projects/6403)                                                                                                                                                                                                                                | |  **package**   | [![PyPI version](https://badge.fury.io/py/deeprank2.svg)](https://badge.fury.io/py/deeprank2) [![Codacy Badge](https://app.codacy.com/project/badge/Grade/b1bde03fc0334e07b0cd8a69ce2adeb3)](https://app.codacy.com/gh/DeepRank/deeprank2/dashboard?utm_source=gh&utm_medium=referral&utm_content=&utm_campaign=Badge_grade)                                                                                                                                                                                                                                     | |    **docs**    | [![Documentation Status](https://readthedocs.org/projects/deeprank2/badge/?version=latest)](https://deeprank2.readthedocs.io/en/latest/?badge=latest) [![RSD](https://img.shields.io/badge/RSD-deeprank2-pink)](https://research-software-directory.org/software/deeprankcore) [![DOI](https://zenodo.org/badge/450496579.svg)](https://zenodo.org/badge/latestdoi/450496579) [![DOI](https://joss.theoj.org/papers/10.21105/joss.05983/status.svg)](https://doi.org/10.21105/joss.05983)                                                                        | |   **tests**    | [![Build Status](https://github.com/DeepRank/deeprank2/actions/workflows/build-repo.yml/badge.svg)](https://github.com/DeepRank/deeprank2/actions) ![Linting status](https://github.com/DeepRank/deeprank2/actions/workflows/linting.yml/badge.svg?branch=main) [![Coverage Status](https://coveralls.io/repos/github/DeepRank/deeprank2/badge.svg?branch=main)](https://coveralls.io/github/DeepRank/deeprank2?branch=main) ![Python](https://img.shields.io/badge/python-3.10-blue.svg) <!--- ![Python](https://img.shields.io/badge/python-3.11-blue.svg) --> | | **running on** | ![Ubuntu](https://img.shields.io/badge/Ubuntu-E95420?style=for-the-badge&logo=ubuntu&logoColor=white)                                                                                                                                                                                                                                                                                                                                                                                                                                                            | |  **license**   | [![License](https://img.shields.io/badge/License-Apache_2.0-blue.svg)](https://opensource.org/license/apache-2-0/)                                                                                                                                                                                                                                                                                                                                                                                                                                               |  ## Overview  <p align=""center"">   <img src=""./deeprank2.png"" width=""700""> </p>  DeepRank2 is an open-source deep learning (DL) framework for data mining of protein-protein interfaces (PPIs) or single-residue variants (SRVs). This package is an improved and unified version of three previously developed packages: [DeepRank](https://github.com/DeepRank/deeprank), [DeepRank-GNN](https://github.com/DeepRank/Deeprank-GNN), and [DeepRank-Mut](https://github.com/DeepRank/DeepRank-Mut).  As input, DeepRank2 takes [PDB-formatted](https://www.cgl.ucsf.edu/chimera/docs/UsersGuide/tutorials/pdbintro.html) atomic structures, and map them to graphs, where nodes can represent either residues or atoms, as chosen by the user, and edges represent the interactions between them. DeepRank2 has the option to choose between two types of queries as input for the featurization phase:  - PPIs, for mining interaction patterns within protein-protein complexes, implemented by the `ProteinProteinInterfaceQuery` class; - SRVs, for mining mutation phenotypes within protein structures, implemented by the `SingleResidueVariantQuery` class.  The physico-chemical and geometrical features are then computed and assigned to each node and edge. The user can choose which features to generate from several pre-existing options defined in the package, or define custom features modules, as explained in the documentation. The graphs can then be mapped to 3D-grids as well. The generated data can be used for training neural networks. DeepRank2 also offers a pre-implemented training pipeline, using either [CNNs](https://en.wikipedia.org/wiki/Convolutional_neural_network) (for 3D-grids) or [GNNs](https://en.wikipedia.org/wiki/Graph_neural_network) (for graphs), as well as output exporters for evaluating performances.  Main features:  - Predefined atom-level and residue-level feature types   - e.g. atom/residue type, charge, size, potential energy   - All features' documentation is available [here](https://deeprank2.readthedocs.io/en/latest/features.html) - Predefined target types   - binary class, CAPRI categories, DockQ, RMSD, and FNAT   - Detailed docking scores documentation is available [here](https://deeprank2.readthedocs.io/en/latest/docking.html) - Flexible definition of both new features and targets - Features generation for both graphs and 3D-grids - Efficient data storage in HDF5 format - Support for both classification and regression (based on [PyTorch](https://pytorch.org/) and [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/))  ðŸ“š [Documentation](https://deeprank2.rtfd.io/)  ðŸ“£ [Discussions](https://github.com/DeepRank/deeprank2/discussions)  ## Acknowledgments  DeepRank2 software has been developed within the [3D-Vac project](https://research-software-directory.org/projects/3dvac), funded by the Netherlands eScience Center (NLESC.OEC.2021.008). In addition to this core budget, we have obtained two fundings specific for software sustainability (SS). The first one was dedicated to improving the user experience of DeepRank2 by introducing a more adaptable interface intended for a wider audience. It also aimed to consolidate the original [DeepRank](https://github.com/DeepRank/deeprank), [DeepRank-GNN](https://github.com/DeepRank/Deeprank-GNN) and [DeepRank-Mut](https://github.com/DeepRank/DeepRank-Mut) packages into DeepRank2, intended to become the standard for developers and users. The second SS budget is still ongoing, and aims at expanding the current DeepRank2 package for handling highly diverse 3D molecular complexes composed of nucleic acids (e.g., DNA, RNA), proteins, and inorganic molecules.",https://github.com/DeepRank/deeprank2,DeepRank/deeprank2
9b4e9cb9-4bdc-44de-971d-d2bc47c2d2fa,aiida-champ,Aiida-CHAMP,,https://github.com/TREX-CoE/aiida-champ,TREX-CoE/aiida-champ
9b8ec850-eb99-45a0-a39d-852099b5a8ff,dhdt,dhdt,"*dhdt* is a modular geodetic imaging framework written in Python. It uses data from imaging satellites to extract (above) surface kinematics and elevation (change). This is done via image matching techniques and geometric principles.  This library is generic, but has a preference towards open satellite data, as in recent years many space agencies have adopted an open data policy. Consequently, functions for the Sentinel-2 satellites of the Copernicus system are closest to completion.  This library has two aspects  * photohypsometry: extracting elevation change from changing shadow cast * photogrammetry: extracting displacement or disparity through changes in observation time and of angle  This library can be used for a multitude of purposes, though the main focused lies towards the following products:  * glacier elevation change * glacier velocity * ocean circulation * natural mass movements * earthquake displacements",https://github.com/GO-Eratosthenes/dhdt,GO-Eratosthenes/dhdt
9bca27f1-6392-45ab-af25-7dd2507ada2a,obiba-agate,OBiBa Agate,"## Applications Declared applications can use Agate as their user registry and user notification service. The communication between the application and Agate requires application authentication: an application is assigned a secret key that must be provided when accessing Agate. See corresponding documentation of each OBiBa application to know how to register an application key.  Once identified an application can authenticate a user, get its profile, send emails to all users or a group of users having access to this application.  ## Groups Users can be member of one or several groups. Application access can be declared on a group basis: users who are members of a group inherit from its application accesses.  ## Users A user is the one who will be authenticated and who will receive the notification emails. A user entry can be added by different ways:  * Agate administrator can directly add a new user, * A user can submit a join request, either from the Agate web interface or from the Mica sign up page.  The user join requests are protected by Google reCaptcha service.  A user can manage its personal information from the Agate web interface or from the Mica user profile page. Password management (first password and password recovery) is automatically done by the exchange of emails between the user and the Agate server: no administrator action is required.  ## Email Templates Email notifications are HTML template-based, allowing to fully customize your communication with users: links to resources, style, site policies etc.  ## Administration With Agate, you can: * Extend user profile attributes (apply to the join request form), * Manage users, groups and applications, * Manage user sessions: timeouts and user activity tracking.",https://github.com/obiba/agate,obiba/agate
9c20d4df-daa4-469b-a616-e3ef6131cec4,4tu-kinematic-orbit-combination,kinematic-orbit-combination,"This dataset contains supporting data for the combination of Swarm kinematic orbits which may be conducted using the software tools provided in the linked Github repository. It contains SLR normal point spreadsheets, spreadsheets of statistical analysis of residuals along station-satellite lines of sight, and 'gap' files which describe when data is missing. Together with the software tools, this can be used to automatically combine Swarm kinematic orbits and assess their quality.",https://data.4tu.nl/v3/datasets/243a0639-0420-44ee-82ca-a9d83cf1ea63.git,
9cb19304-b47e-4c19-abaa-646050de6a03,pattyvis,Via Appia Visualization,* A 3D interactive web application for archeology * Visualizes LiDAR data and Structure-From-Motion data as well as extensive meta-information. * Explore an extensive Archeology dataset from the comfort of your own home.,https://github.com/NLeSC/pattyvis,NLeSC/pattyvis
9d3546a3-f3bf-4e84-9dbe-2473fe76ff6f,effort-sharing,Effort Sharing,"This package combines a variety of data sources to compute fair national emissions allocations, study variability, and compare results with NDC estimates and cost-optimal scenario projections.  * Gather country-level data (population, GDP, historical emissions, etc.) * Compute global future emission pathways based on configurable scenarios * Calculate allocations for countries/regions using various effort-sharing rules * Compare allocations, NDCs, and cost-optimal scenarios * Conduct variance decomposition (Sobol analysis)",https://github.com/imagepbl/effort-sharing,imagepbl/effort-sharing
9d88beea-f156-4810-974a-346fd86647a6,nunaliit,Nunaliit,"Nunaliit has been designed to make it relatively simple to create interactive mapping web sites based on your data and multimedia, and to permit web users to contribute additions and changes where permitted. By default, it uses a very flexible document oriented database to store any text-based attributes or data objects.  Altas creators may create a series of ""schemas"" to help define how to display and input documents that share a common set of attributes including geometries and relationships to other schemas. This flexible system allows for iterative development of functionality as you begin to see patterns in the data. Your editor users can create their own attributes and provide any sort of attachment. As the data evolves, schemas and atlas functionality can be modified to accommodate attributes or attachment types that might not have been originally anticipated.  Documents can have any number of attachments such as zip files, images, videos, office documents, mapping data, etc. Many popular image, video, sound, and location formats can be accepted and converted to web-ready formats by the Nunaliit server-side automatically.  A modern HTML/CSS/JS web client with OpenLayers mapping functionality is served to a user's browser and interacts with the database and a handful of media conversion servlets to permit rich interactivity with the data and editing/uploading of content and map geometries.  Nunaliit has also been designed to support distributed multi-master atlases at remote locations including map-less remote offline use on mobile devices for video, photo, sound, and location recording as well as document, attribute, and schema editing. At the moment, a developer build of an iPad application exists and has been tested in the Yukon and Northwest Territories by a partner group. Efforts are underway to support Android devices as well.",https://github.com/GCRC/nunaliit,GCRC/nunaliit
9dbe8f43-6381-417d-b93a-946e00f07e5d,4tu-gsi-toolbox,GSI_ToolBox,"A C++ code for sampling a newly-proposed scattering kernel for gas-surface interaction applications. This kernel incorporates geometric surface morphology effects on the scattering dynamics in a physical way, through the use of electromagnetic wave scattering theory.",https://data.4tu.nl/v3/datasets/07a5820f-4914-4e5c-b48f-61369583ea9c.git,
9e443705-fb8b-408f-9d9a-ad914af8bbad,democracy-datasets,Democracy Datasets,,https://github.com/backdem/democracy-datasets,backdem/democracy-datasets
9e53f938-ed3a-4555-bdf7-af0bc1302b5a,hep-yoda,YODA,"YODA is a small set of data analysis (specifically histogramming) classes being developed by MCnet members as a lightweight common system for MC event generator validation analyses, particularly as the core histogramming system in Rivet.  YODA is a refreshingly clean, natural and powerful way to do histogramming... and there are plenty of improvements still to come. Our mission is to make the most powerful, expressive, and focused approach to binned computational data handling, with the nicest possible balance of power and simplicity in the user interface. We hope you'll agree it's a good thing, but if not (or even if so) please get in touch and let us know about your thoughts, problems, and feature requests.",https://gitlab.com/hepcedar/yoda,
9e6a64e7-cecd-4aa9-a738-9ffdf112fc5f,unraphael,Unraphael,"The study of Raphael's paintings reveals numerous faithful reproductions, highlighting both the prolific nature of his workshop's output and the enduring appeal of his designs. Traditionally, comparing these reproductions has been a labour-intensive process, constrained by manual examination and limited case studies.   The software called **Unraphael** introduces a digital workflow tool designed to enhance the analysis of paintings and their reproductions by uncovering the various transfer methods employed in creating copies. Focusing on Raphaelâ€™s oeuvre, this software covers a comprehensive pipeline utilizing advanced computer vision techniques to precisely extract and compare figure outlines across reproductions. This pipeline includes preprocessing, background removal and individual figure extraction, image alignment, and clustering based on structural similarity indices and brushstroke signatures. This enables the differentiation of various transfer techniques with high accuracy.   This software demonstrates that computer vision can substantially refine the examination of differences and similarities between original works and their reproductions. It not only offers new insights into Raphael's artistic practices but also enhances our understanding of copying techniques from the 15th and 16th centuries.",https://github.com/DecodingRaphael/unraphael,DecodingRaphael/unraphael
9eaca5c7-e3cb-4fe0-9b9e-28e2436aab0e,4tu-code-for-the-paper-characterizing-residential-segregation-in-cities-using-intensity-separation-and-scale-indicators,"Code for the paper ""Characterizing residential segregation in cities using intensity, separation, and scale indicators""","This code is used for computing the results of the paper ""Characterizing residential segregation in cities using intensity, separation, and scale indicators"".Â        This code identifies and characterizes residential segregation patterns from demographic data. It is applied in a Dutch case study. It is written in python, using notebooks.       This source code should be stored in a folder named code. The folder code and the folder data (see https://doi.org/10.4121/19597258) should be located in the same directory.   FORMAT *.mkd;   *.ipynb;   *.txt;   *.csv   RECOMMENDED HARDWARE 1. Processor: IntelÂ® Coreâ„¢ i5-10210U CPU   2. RAM: 32GiB of RAM (DDR4)   3. GPU: IntelÂ® UHD Graphics GPU   RECOMMENDED OPERATING SYSTEM Ubuntu 21.10, 64-bit   REQUIRED VERSION OF PYTHON 3.9.7   REQUIRED LIBRARIES USED see requirements.txt   EXTRA FILE parameter.csv specifies some parameters used in the analysis.   SEQUENCE OF SCRIPTS The scripts should be run in the following order:       1. demographics_preprocess.ipynb   2. extract_city_boundary.ipynb   3. extract_street_network.ipynb   4. extract_zones_in_gemeente.ipynb   5. shortest_path.ipynb   6. adjacency_matrix.ipynb   7. correlation_matrix.ipynb   8. exposure.ipynb   9. cluster_analysis.ipynb   10. descriptive_stats.ipynb   11. combine_zones_into_municipalities.ipynb",,
9f571625-c23c-4464-8288-a3b4cb086392,cityblocks,Cityblocks,"# Cityblocks  Python utility to:  - Download global LCZ map from [Demuzere et al.](https://zenodo.org/records/7670653). - Extract an area of interest - Generate a new dataset where each pixel is replaced by a 2D tile corresponding to the LCZ type  The generated file can be displayed in QGIS to create stunning visualizations in 3D.  ## How to use  ```sh # Install the tool pip install git+https://github.com/Urban-M4/cityblocks.git  # Download global LCZ data.  cityblocks download  # Extract area of interest cityblocks extract ""4.724808,52.273620,5.182114,52.458729""  # Convert LCZ data to 2d tiles cityblocks convert ```  The generated data can be imported in QGIS and styled with the style spec provided with the software. For more info, see the [full README](https://github.com/Urban-M4/cityblocks)",https://github.com/Urban-M4/cityblocks,Urban-M4/cityblocks
9f6fa05d-6b88-4c8c-937a-71741772c233,cosimplat,Cosimplat,"The CoSimPlat - Python Version is a co-simulation framework which uses the so called Long Polling paradigme to enable Event-Based real-time communication among the players exploiting SQL databaseâ€‹. The whole mechanism is made possible thanks to the standardization of the Payload, â€‹i.e. a JSON template containing Meta and Packets. Precisely, the Payload is the total amount of information that is shared between the players at every step, while the Packets are intended as the customizable part of the Payload which has to be agreed a priori by all the players of the cosimulation.",https://github.com/ESI-FAR/RESCUE-cosimplat-py,ESI-FAR/RESCUE-cosimplat-py
9f911520-488f-47b5-a576-32718cb53b3e,annubes,ANNUBeS,"The use of animals in neuroscience research is a fundamental tool to understand the inner workings of the brain during perception and cognition in health and disease. Neuroscientists train animals, often rodents, in behavioral tasks over several months, however training protocols are sometimes not well defined and this leads to delays in research, additional costs, or the need of more animals. Finding strategies to optimize animal training in safe and ethical ways is therefore of crucial importance in neuroscience.  ANNUBeS, which stays for _Artificial Neural Networks to Uncover Behavioral Strategies_, is a deep learning framework meant to generate synthetic data and train on them neural networks aimed at developing and evaluating animals' training protocols in neuroscience. The package gives the users the possibility to generate behavioral data in a very flexible way, that can be used to train neural networks in the same way that animals are trained, and study whether the developed models can predict the behavior of the animals. The ultimate goal of the framework is to lead researchers to more efficient training protocols, thus improving neuroscience practices.",https://github.com/ANNUBS/annubes,ANNUBS/annubes
9fcfc2c8-6d5e-40fa-a1b4-6017513ec61e,4tu-numerical-model-configurations-for-a-synthetic-spring-neap-tidal-cycle-for-long-term-morphological-modelling,"Numerical model configurations for ""A synthetic spring-neap tidal cycle for long-term morphological modelling""","The dataset includes all Delft3D-FM model configurations used for the results in the paper ""A synthetic spring-neap tidal cycle for long-term morphological modelling"".Â    The model output files are not included.   The original datasets used for constructing model input are not included as well, but are freely accessible. See the paper for references.",,
a02c6b5b-df00-479d-8bc3-29efe1e79ff6,blueobelisk-euclid,BlueObelisk Euclid,,https://github.com/BlueObelisk/euclid,BlueObelisk/euclid
a0370190-6983-4927-89a2-c0bd5383ea83,overture-song,Overture Song,"**SONG** is a robust metadata and validation system used to quickly and reliably track genome metadata scattered across multiple cloud storage systems. In the field of genomics and bioinformatics, metadata managed by simple solutions such as spreadsheets and text files require significant time and effort to maintain and ensure the data is reliable. With several users and thousands of genomic files, tracking the state of metadata and their associations can become a nightmare. The purpose of SONG is to minimize human intervention by imposing rules and structure to user uploads, which as a result produces high quality and reliable metadata with a minimal amount of effort. SONG is one of many products provided by Overture and is completely open-source and free for everyone to use.",https://github.com/overture-stack/song,overture-stack/song
a0527a00-d83b-4927-8258-7680f8b99108,eecology-classification,eEecology Classification,"- Load annotated accelerometer points from MAT files (Matlab) - Load annotated GPS points from CSV files - Extract data features out-of-the-box or define your own - Train a model using annotated time series of accelerometer or GPS data - Supports tree learner and Random Forest models - Evaluate the trained model - Automatically annotate accelerometer data (or GPS) using a trained model - Visualize all steps in the browser  Data about whereabouts from birds can be gathered using a gps tracker with accelerometer from uva-bits and analysed using this tool. Often, we are not directly interested in the forces measured by the accelerometer. What we actually want to know is whether the bird was sitting, walking, foraging of flying. Assigning a behavior to data measurements is called annotation (or classification). This tool lets you train a model on a small annotated set, and then use the model to automatically annotate a larger dataset.",https://github.com/NLeSC/eEcology-Classification,NLeSC/eEcology-Classification
a0b33a8e-a83f-4f68-a20b-c0541648dbda,fairtool,fairtool,,https://github.com/neelravi/fairtool,neelravi/fairtool
a1aac62d-83a3-4fc9-ab15-9ef944175cfc,4tu-matlab-model-of-multiscale-model-for-co2-electrolysis,MATLAB model of multiscale model for CO2 electrolysis,"This Matlab script is a multi-scale modelling framework (ranging from the single channel scale over the electrolyser scale to the process scale) embedded in a techno-economic analysis for a CO2 electrolyser as described in the manuscript ""Techno-economic assessment of CO2 electrolysis: How interdependencies propagate across scales"" and the Chapter 4 of the PhD dissertation of Bagemihl. This framework allows to optimise the electrolyser performance under economic considerations and is the first of its kind which does not consider the electrolyser as blackbox. Instead mass transfer effects along the channel length are taken into account via the multi-scale model. This code is for the exemplary case of CO2 to ethylene conversion with the only side product being hydrogene. The code can be adapted for other catalysts and reactants by changing the kinetics in the single channel scale model.",,
a1ac53e3-d8ef-4458-a4be-f49da45aa7d8,4tu-code-to-paper-review-of-image-segmentation-techniques-for-the-layup-defect-detection-in-the-automated-fiber-placement-process,Code to paper: Review of image segmentation techniques for the layup defect detection in the Automated Fiber Placement process,"This code was used to generate the results for the paper ""Review of image segmentation techniques for the layup defect  detection in the Automated Fiber Placement process""",,
a1bb60b8-7c62-49b9-8cb0-21d22c102d57,iscore,iScore,* Provides a state-of-the-art scoring predictor trained on the data of Docking Benchmark 5 (https://zlab.umassmed.edu/benchmark/) * Provides easy-to-use interface to train a new scoring predictor * Easily extended with more different and various features of protein interface * Active and competitive player in the CAPRI challenge for 3D structure predictions of protein complexes (http://www.capri-docking.org)  iScore offers simple solutions to classify protein-protein interfaces using a support vector machine approach on graph kernels. The simplest way to use iScore is through dedicated binaries that hide the complexity of the approach and allows access to the code with simple command line interfaces. The two binaries are iscore.train and iscore.predict that respectively train a model using a trainging set and use this model to predict the near-native character of unkown conformations.,https://github.com/DeepRank/iScore,DeepRank/iScore
a1c8904b-96c7-4653-ba6f-685a17519077,4tu-model-for-the-dynamic-behaviour-in-magnetic-nanoparticles-scripts-and-experimental-data,Model for the dynamic behaviour in magnetic nanoparticles: scripts and experimental data,"Contains the MATLAB scripts to run the created model and to plot the results of this model (magnetic behaviour) against the measured data (also included). Required:- MATLAB R2021a- Statistics and Machine Learning Toolbox, (Global) Optimalisation, Signal Processing, and Parallel Computing toolbox needed- This .zip file, extracted. As used in Modelling of Dynamic Behaviour in Magnetic Nanoparticles (https://doi.org/10.3390/nano11123396)",,
a1e4150c-6843-40b1-82d6-d9d777bdbeff,the-fair-extension,the FAIR extension,"[![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8B-yellow)](https://fair-software.eu)  ""The FAIR extension"" is an open-source web browser extension that allows researchers to do FAIR metrics evaluations directly to the web source. ðŸ‘©ðŸ»â€ðŸ’»ðŸš¦<br>  --- ## Quickstart for Users ### To run the extension as a user:<br> 1. Go to the [Google Chrome Store](https://chrome.google.com/webstore/detail/the-fair-extension/pjmiddoifhbhbfcppdbmaigdeoiochdg) and add the extension to your browser.<br> 2. Navigate to the webpage you want to evaluate.<br> 3. Activate the extension by clicking on its icon.<br> 4. The extension will detect and highlight DOIs on the webpage. Click ""Evaluate"" next to a DOI to assess it.<br> 5. Open the extension popup by clicking its icon on the top-right of your browser to see the evaluation results.<br>  __Note:__ The evaluation process takes 2 to 5 seconds, depending on the website.  Here's an example of how the extension works using [this dataset](https://dataverse.nl/dataset.xhtml?persistentId=doi:10.34894/Q80QUE).  --- ## Quickstart for Developers  ### To edit the project:<br> 1. Download the project as a Zip folder.<br> 2. Extract the folder.<br> 3. Open the project in a code editor such as Visual Studio Code.<br>  ### Working files:  - **Manifest.json**: Controls the structure of the extension, from essential elements like the title to complex features such as permissions and workflows.<br>  - **background.js**: Acts as a medium between the extension's files, maintaining communication between content scripts and popup JSON files. It also handles redirection to the DOI.<br>  - **popup.js**: Triggered by index.html. It modifies the user's view of the popup, including the DOI evaluation. This file also controls the communication between the extension and the FAIR-enough API.<br>  - **index.html**: The front end of the Chrome extension's popup. You can edit the style of the extension in this file.<br>  - **contentScript.js**: Triggered when the extension is given access. It injects scripts into the website to highlight text, detects DOIs in the webpage, injects code to highlight them and adds buttons next to them, and sends the indicated DOI to background.js for evaluation.<br>  --- ## Use Cases    1. A student wanting to use a DO (e.g. software package) must know which to choose. The FAIR extension will indicate which one is more FAIR and aid the decision-making process 2. A Data steward recommending sources 3. A researcher who wants to display all FAIR metrics of their DOs on a research profile 4. A PI that wants to evaluate an aggregated metric for a project.  --- ## About ðŸ‘“  The scientific communityâ€™s efforts have increased regarding evaluating the FAIRness of research Digital Objects such as publications, datasets, or research software. However, this requires a steep learning curve for the average researcher when learning the FAIR evaluation frameworks, disengaging some.  This project aims to use technology close this gap and make this process more accessible by bringing the FAIR evaluation to the researcherâ€™s profiles. â€œThe FAIR extensionâ€ is an open-source, user-friendly web browser extension allowing researchers to do FAIRness evaluations directly from scholarly aggregators such as PURE, Google Scholar, or ResearchGate without additional knowledge. The browser extension follows the FAIR metrics group specification, building on top of the community-accepted FAIR evaluatorsâ€™ APIs.Â   We aim to demonstrate that low-entry-level tools can bridge the gap between the researcher and the FAIR metrics. e.g. A data steward recommending FAIR sources or a researcher evaluating all FAIR metrics directly on scholarly profiles.  The development of this extension is made possible thanks to the [Netherlands eScience Center fellowship grant 2022-2023](https://www.esciencecenter.nl/fellowship-programme/)  ### - Contact  If there is an issue, please open one at [Issues](https://github.com/MaastrichtU-Library/the-FAIR-extension/issues). Else, contact directly at [p.hernandezserrano@maastrichtuniversity.nl](p.hernandezserrano@maastrichtuniversity.nl)  ### - Research Software Quality Checklist  Following the Software Sustainability Instituteâ€™s software evaluation approach  - [Software Evaluation: Criteria-based Assessment](https://software.ac.uk/sites/default/files/SSI-SoftwareEvaluationCriteria.pdf) - [Sustainability Evaluation Report](https://docs.google.com/document/d/1YFrTor7yVQ-3Xpcn3ds38T0qhvSzjCUJCvVsf2b8TRg/edit)  <img src=""https://github.com/MaastrichtU-Library/the-FAIR-extension/blob/main/img/sustainabilityreport.png"" alt=""logo"" lign=""middle"" style=""width: 50%; height: 50%""/>  --- ## Terms of Use ðŸ“ƒ  ### - Licence  Copyright (c) 2022 Maastricht University Library  [The code contained in this Github repository](https://github.com/MaastrichtU-Library/the-FAIR-extension) is free software: you can redistribute it and modify it under the terms of the [Apache 2.0 License](https://opensource.org/licenses/Apache-2.0) compatible with many copyleft licenses such as the GNU General Public License.  ### - Research Software Citation  Machine-actionable citation file: [CITATION.cff](https://github.com/MaastrichtU-Library/the-FAIR-extension/blob/main/CITATION.cff)   Should you require another format for citation, you can convert it using the [cff-converter Tool](https://github.com/citation-file-format/cff-converter-python)  ### - Paper Citation   ``` @article{10.3897/rio.8.e95006, 	author = {Pedro Hernandez Serrano and Vincent Emonet}, 	title = {The FAIR extension: A web browser extension to evaluate Digital Object FAIRness}, 	volume = {8}, 	number = {}, 	year = {2022}, 	doi = {10.3897/rio.8.e95006}, 	publisher = {Pensoft Publishers}, 	pages = {e95006}, 	URL = {https://doi.org/10.3897/rio.8.e95006}, 	eprint = {https://doi.org/10.3897/rio.8.e95006}, 	journal = {Research Ideas and Outcomes} }  ```",https://github.com/MaastrichtU-Library/the-FAIR-extension,MaastrichtU-Library/the-FAIR-extension
a20d0996-dfa8-4acb-960c-1b44051fbc87,arpra,Arpra,"Arpra is a C library for (Ar)bitrary-(p)recision (r)ange (a)nalysis of IEEE-754 floating-point computations, based on GNU MPFR. The main use-case of Arpra is to maintain computed upper and lower bounds of numerical error for all variables, at all times, throughout a computation. Arpra uses mixed trimmed interval/affine arithmetic with deviation term reduction to accomplish this.  Affine arithmetic is a variant of interval arithmetic which accounts for variable correlations. As such, it does not suffer from the so-called 'dependency problem', where intervals grow overly large due to lack of consideration for variable correlations. The problem is described further at <https://en.wikipedia.org/wiki/Interval_arithmetic#Dependency_problem>. By combining the results of interval arithmetic and affine arithmetic, one avoids both the dependency problem of interval arithmetic, and the nonlinear function overshoot/undershoot problem of affine arithmetic.  Arpra implements affine arithmetic using a GNU MPFR backend. MPFR is an arbitrary-precision floating-point library, meaning the floating-point MPFR variables can be of arbitrary precision. For more information, refer to the MPFR project website at <http://www.mpfr.org/>. By implementing affine arithmetic with an arbitrary-precision backend, one is able, for example, to test how a change in numerical precision or integration scheme affects local and global error during a long numerical simulation, without the interval 'explosion' problem regular interval arithmetic suffers from.  For further information on the implementation and features of Arpra, such as range trimming and deviation term reduction, refer to the original published article:  Turner, J. P., & Nowotny, T. (2021). Arpra: An Arbitrary Precision Range Analysis Library. Frontiers in Neuroinformatics, 30. https://doi.org/10.3389/fninf.2021.632729",https://github.com/arpra-project/arpra,arpra-project/arpra
a2125c40-0db3-4d9b-b164-092dcf0e72e0,foot-progression-angle-estimation,Foot Progression Angle Estimation,,https://gitlab.utwente.nl/bss_development/movement-science/fpa-estimation,
a24d5ae6-25ea-45a7-aa25-ded6d3bc022e,nanopubjl,NanopubJL,"* Adds a search widget to the Jupyter Lab interface * Automatically injects python code necessary for fetching the desired result * Quickly use Nanopublications as part of your data science workflow in Jupyter  A Jupyterlab extension for searching, fetching and publishing Nanopublications from a python notebook. Uses the nanopub python library. This extension is composed of a Python package named NanopubJL for the server extension and a NPM package named NanopubJL for the frontend extension.",https://github.com/fair-workflows/NanopubJL,fair-workflows/NanopubJL
a28ab566-df71-4361-af84-b080f4a399da,ms2deepscore,MS2DeepScore,"- Predict chemical similarity based on MS/MS mass spectra  A classical way to compare MS/MS mass spectra is to quantify their peak overlap, often done by using variations of cosine similarity scores. Those measures tend to work well for nearly equal spectra, i.e. cases of very high peak overlap. We recently introduced Spec2Vec an unsupervised machine learning approach for computing spectrum similarities based on learned relationships between peaks across large training datasets [ref]. Spec2Vec based similarity scores were observed to correlate more strongly than classical cosine-like scores with actual structural similarities between the underlying compounds. Additional core advantages are its fast computation, which allows to compare query spectra against very large libraries, and the fact that -as an unsupervised method- it can be trained on non-annotated data. However, the downside of an unsupervised approach is that it does not make use of the large fraction of labels that we have for the training data. The used training data (MS/MS spectra from GNPS) contains smiles/InChI annotations hence does allow to create molecular fingerprints for quantifying the structural similarities.",https://github.com/matchms/ms2deepscore,matchms/ms2deepscore
a34feb22-d94f-48e6-b353-f3309e24a81a,4tu-data-and-software-supporting-the-publication-telecom-band-quantum-interference-of-frequency-converted-photons-from-remote-detuned-nv-centers,"Data and software supporting the publication ""Telecom-band quantum interference of frequency-converted photons from remote detuned NV centers""","Read the README.txt       TPQI_Data_Processing.ipynb: This is the notebook that generates the main figures from the paper. It defines functions that are used for the analysis, loads the data, processes and visualizes it. It also performs Monte-Carlo simulations to estimate errorbars on the data in Figure 4.       the *.dat files are the raw data files, where the colums are quantities measured in a 1D sweep. The files *_metadata.txt give the header for the datafiles, which shows the unit of the quantities measured. It also contains values of various other settings during the experiment.       The *.npz are numpy arrays the result of analyzing large .hdf5 files containing time-tags of single-photon and marker events in our time-taggers. These are then used as further input to the generation of the figures, where they are shown in histograms across various time axes.       Further information on the acquisition and the processing of the data can be found in the supplementary of the paper.",,
a352c44f-2396-41f1-be62-ab451b5fd939,ldot,Ldot,"Conducting scientific studies can be very challenging, in particular if you take into account all relevant privacy and security related laws and guidelines.    **Participant registration** Safely store and share participant's personal information in one central system instead of spreadsheets and makeshift database prevents possible data leaks.   **Workflow support** In addition to the storage of participant's personal information, Ldot can help you with all research logistics, so you can focus on your actual research data. Ldot improves the quality of your study by helping you to conduct all steps from your study protocol, whilst being compliant with laws and guidelines like GDPR and GCP. Ldot is suitable for studies of any size.",,
a3a011ad-5713-44af-aa8e-90d6781bf45f,4tu-data-analysis-supporting-material-for-a-hardware-efficient-leakage-reduction-scheme-for-quantum-error-correction-with-superconducting-transmon-qubits,"data_analysis - Supporting material for ""A hardware-efficient leakage-reduction scheme for quantum error correction with superconducting transmon qubits""","code used for the analysis: average leakage lifetime, steady state, analysis of the Lindblad simulations, code that reproduces the analytical results about the res-LRU.The weight estimation and blossom decoder are available at https://github.com/obriente/qgarden This data pertains to the publication https://arxiv.org/abs/2102.08336 and is part of a collection of supporting materials.  See README for more information.",,
a3a3b5de-eef6-4b9a-9724-bf77120fe3ac,quicklb,QUICKLB,"# Summary  QUICKLB is a quick loadbalancer for unbalanced HPC applications. QUICKLB is a very efficient library that sacrifices optimality for speed. It shines in  distributed applications that have unpredictable unbalanced computational peaks in their domains. The loadbalancer is written in Fortran with a Python wrapper to call it. Thus it is suitable for both Python and Fortran HPC applications. The loadbalancer can be ""bolted on"" already existing applications, by pointing it  to the data and compute functions, QUICKLB will then take care of the computation  offloading.",https://github.com/NLESC-JCER/QUICKLB,NLESC-JCER/QUICKLB
a3aa54bb-1079-4d95-9b06-aabe630e5f09,cffinit,cffinit,"When you made some software and you want to include instructions on how to cite it, CITATION.cff files are the answer. However, sometimes it's tricky to ensure you write valid CFF. This tool helps mitigate that problem by generating the CFF text using a web form with form validation and user feedback.  - authors can be rearranged - keywords can be rearranged - includes some automated checking/form validation with feedback to the user - compliant with http://www.yamllint.com/ - compliant with Citation FIle Format specification v1.2.0 - does not support the full CFF spec yet",https://github.com/citation-file-format/cff-initializer-javascript,citation-file-format/cff-initializer-javascript
a3bb8b0c-5a28-408a-aa8b-d1f697a4e713,copasi,COPASI,"- run simulations of chemical networks based on differential equations - run simulations of chemical networks based on stochastic processes - easily switch between differential equations and stochastic processes within a single model - calculate steady states and determine their stability - estimate model parameter values using experimental data - optimize arbitrary model variables using a menu of many optimization algorithms - perform sensitivity analysis of model variables - import/export models in the standard SBML format, or through OMEX files  COPASI is one of the the most widely used software for simulations of bio/chemical reaction networks. COPASI is widely used to model metabolic and signaling pathways, pharmacology, genetics, and many areas of chemistry, with special emphasis on microkinetic modeling.   Users define models in COPASI by focusing on the biochemistry components (species, reactions, kinetic rate laws, etc.) and the software automatically builds the corresponding mathematical equations. This feature makes it quickly accessible to biologists and chemists as model-building strongly relies on their domain knowledge, and minimizes the mathematical and programming skills required to a minimum.  COPASI allows many different types of time-dependent models (i.e. models where diffusion is not explicit), using several formalisms: ODEs, stochastic differential equations (SDEs), stochastic simulation with the Gillespie algorithm, and hybrid models where some variables are modeled with ODEs and others with the Gillespie approach. Additionally all the models can include discrete events, making such models hybrid continuous-discrete.   The most widely used functionality of COPASI is parameter estimation, where one fits parameter values of a model to experimental data. Users are able to associate an arbitrary number of experiments with the model variables and COPASI automatically builds a least-squares objective function for minimization which is carried out by nonlinear optimization algorithms (gradient-based, evolutionary algorithms, simulated annealing, particle swarm and several others). The user can additionally add any type of nonlinear constraints (for example, that a ratio of two concentrations must be above a certain value).  COPASI provides several methods to characterize models including algorithms for sensitivity analysis, stability analysis, cross-sections, Lyapunov exponents, and generic optimization. Additionally COPASI exposes its features to other programs through libraries and APIs available in several languages (Java, R, Python, etc.)",https://github.com/copasi/COPASI,copasi/COPASI
a3f076bf-fa36-4031-b855-dedcb3d9398b,4tu-topological-sb-matching,topological_SB_matching,"This implements the matching and generative learning models on graphs and simplicial complexes, including interpolating the single-cell dynamics, brain signals, ocean currents and generating seismic waves and traffic flows.",https://data.4tu.nl/v3/datasets/84d9dcb3-ab8a-4bcf-96c7-d0606bd91538.git,
a41f23a7-3be9-43dc-8cf1-e1dbee7c6f39,research-lifecycle-approach-using-islandora,Research Lifecycle Approach using Islandora,"The Islandora Research Data Management (RDM) Platform builds on Islandora 8 to include research data management features with a national linked data focus, bringing together external, locally managed, and shared services applied throughout the research data lifecycle. The Islandora RDM Platform provides integrations with identifier, metadata, authentication, storage, and dissemination systems and services. The Islandora RDM platform supports the creation, management, discovery, aggregation, and preservation of research data.",https://github.com/roblib/rdm-playbook,roblib/rdm-playbook
a41f6cdf-6e69-421f-a2e3-61ed34dbbc65,4tu-metropolis-2-cd-r-study-code,Metropolis 2 CD&R Study Code,"BlueSky version that was used to simulate scenarios for investigating conflict prevention, detection and resolution in constrained urban airpsace.",,
a453a6df-5a66-418e-9414-450e05cad8bc,streetscapes,Streetscapes,,https://github.com/Urban-M4/streetscapes,Urban-M4/streetscapes
a461336b-fefc-4fde-a7c8-a1a501524a44,cerise,Cerise,"* Lets you make running workflows on HPC resources easy * You provides users with a CWL API, they combine tools into a workflow * Installs software, accepts workflows, stages files, executes * REST/CWL/Docker/Python client  Cerise is a generic service for running CWL workflows on compute resources (i.e. clusters, supercomputers, and simply remote machines). It tries to offer a consistent environment for workflows, so that a workflow sent to resource A will work unchanged on resource B as well.  To achieve this, and to offer a bit of safety and perhaps security, Cerise does not allow running arbitrary CWL command line tools. Instead, it expects the user to submit a workflow document that refers to predefined steps built into the service.  Defining these steps, and adding them to the service, is called specialising the service. A specialisation of Cerise is always specific to a project (which determines which steps are available and what inputs and outputs they have), and to a compute resource (which determines how they are implemented). Thus, two workflows sent to two different specialisations to the same project, but to different compute resources, should give the same result (assuming the calculation is deterministic!).",https://github.com/MD-Studio/cerise,MD-Studio/cerise
a4a4412c-1c90-4075-b8b1-a70354326b6e,4tu-code-underlying-the-publication-multibody-dynamic-modelling-and-simulation-of-flexible-instruments-used-in-brachytherapy,Code underlying the publication: Multibody dynamic modelling and simulation of flexible instruments used in brachytherapy,"This repository contains the relevant files for performing simulations of the behaviour of flexible instruments in curved channels using MATLAB and the computer program SPACAR. One application, that of simulating brachytherapy (BT) source cable behaviour and needle insertion in the curved applicator channels, is described in the article 'Multibody dynamic modelling of the behaviour of flexible instruments used in cervical cancer brachytherapy'. For these computer models, BT instruments were discretised in finite elements. Simulations were performed in the full version of SPACAR by formulating nodal contact force and motion input models, and defining kinematic and dynamic modules. Example files for performing rigid and flexible multibody simulation of a needle in an S-shaped channel are included in this repository. These files may be modified to simulate deformations and associated forces and moments of any type of slender elastic rod inside a rigid (circular) channel or environment.     The code in this repository is a simplification of the code used in:  Title: Multibody dynamic modelling of the behaviour of flexible instruments used in cervical cancer brachytherapy.Authors: Robin Straathof, Jaap P. Meijaard, Sharline van Vliet-PÃ©rez, Inger-Karine K. Kolkman-Deurloo, Remi A. Nout, Ben J.M. Heijmen, Linda S.G.L. Wauben, Jenny Dankelman, and Nick J. van de Berg.Journal: Medical PhysicsDOI: FIXMECite repository: FIXME (4TU.Researchdata)License: see license file in repository.",,
a4df59b0-51db-47ba-90c9-a425b0f57d15,flamingo,flamingo,"* Compute quantum chemistry properties with  minimal configuration * Fast screening of candidates based on inexpensive molecular properties * Store results in HDF5  Flamingo allows screening hundreds of thousands of smiles in a short time by filtering the candidates using as criteria the presence (of absence) of different functional molecular groups or the numerical value of molecular properties like the synthesizability score, drug-likeness, etc.",https://github.com/nlesc-nano/flamingo,nlesc-nano/flamingo
a513e810-a9c5-4c9d-bb79-2e76e6627a7e,dataprocessing,Data Processing (What Works When for Whom),"* provides tools for processing health text, for example for text anonymization * includes scripts for health text analysis like with the models LIWC and Daap * contains software for data visualization like with pyplot and tsne  The project What Works When for Whom involved automatic analysis of therapeutic emails related to alcohol abuse of hundreds of people. This software collection contains the scripts and notebooks used for this analysis related to text preprocessing, data anonymization, text analysis among others with statistical psychological models, machine learning and data visualization.",https://github.com/e-mental-health/data-processing,e-mental-health/data-processing
a53885ad-8a3b-4e71-8c1a-65e9c716fe94,4tu-eac-charging-infra-optimization,eAC_charging_infra_optimization,"This repository contains the code developed for determining the optimal size of charging infrastructure for a network of airports, to service a fleet of electric aircraft operating with a battery swapping system. It is being made public both to act as supplementary data for publications and the PhD thesis of Simon van Oosterom, and in order for other researchers to use this repository in their own work.",https://data.4tu.nl/v3/datasets/cc5f5357-3cc2-4a25-8017-9d24db02d8d7.git,
a57a6d68-f613-49dc-af61-2686d64f861f,ewatercycle-infra,eWatercycle SURF Research Cloud,"A Jupyter notebook environment that has everything installed to run hydrological models without having knowledge about how to and install and run the models.  The setup instructions in this repository will create an eWaterCycle catalog item (a sort-of VM template) in the [SURF Research Cloud](https://www.surf.nl/en/services/surf-research-cloud) that when started will create a machine with:  * Explorer: web visualization of available models / parameter sets combinations and a way to generate Jupyter notebooks * Jupyter Hub: to interactivly generate forcings and perform experiments on hydrological models using the [eWatercycle Python package](https://research-software-directory.org/software/ewatercycle) * ERA5 and ERA-Interim global climate data, which can be used to generate forcings * Installed models and their example parameter sets  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/infra,eWaterCycle/infra
a60b2a5a-9777-4c38-ac3c-4b708644f759,perfectfit,PerfectFit,"PerfectFit is a text-based virtual coach system that helps the users in quitting smoking and being physically more active. The software provides the backbone of the system and the logic for intervention administration.  The core of the system is the open-source text- and voice-based contextual assistant [Rasa] https://github.com/RasaHQ/rasa). Rasa is a machine learning framework, that provides automated conversations based on personalized contexts, that can be connected to messaging and voice services through standard interfaces. Perfect Fit leverages the Rasa framework, for the development of custom conversation flows. Such logics are used to guide the VC behaviour.  An SQL database has been developed and connected to the system  to store and use the information needed to tailor the intervention to the user's needs. . To allow the VC to asynchronously initiate the conversation, a scheduler based on [Celery](https://github.com/celery/celery) system is available for tasks queuing. The scheduler can be used to ask questions for collecting information in predefined timepoints used to check and track the status of the user. In the configuration described, Perfect Fit is designed as an intelligent conversational agent for smoking cessation, that can be integrated with different messaging channels using standard or custom interfaces.  Perfect Fit can be used to conduct research on VCs adoption or it can be used as an add on to existing widespread messaging services for the provision of virtual coaching.  The conversational engine is connected to the [NiceDay App](https://niceday.app/en/home/), an application that provides the possibility to chat with a therapist to be supported in mental health care. In this configuration, the therapist is substituted by the VC, with which the users can interact through the chat. The two systems have been integrated using connectors developed ad hoc.",https://github.com/PerfectFit-project/virtual-coach-main,PerfectFit-project/virtual-coach-main
a66a55d5-0047-45c4-8752-11a74299a16d,octseg,OCTSEG,"* The retinal layers and the blood vessels of retinal OCT scans can be segmented * Automated segmentation of 6 prominent retinal layers:  the inner limiting membrane, outer nerve fiber layer boundary, and retinal pigment epithelium * Automated segmentation of the blood vessel positions on circular scans and batch processing of circular scans and manual correction of possible segmentation errors * Visualization of the data and the segmentation results, including enface views and thickness maps * Export of the segmentation results to CSV text files  * Includes segmentation of phantom images * Successfully used in ""A Lightpath for Optical Coherence Tomography Imaging"" project",https://github.com/NLeSC/OCTSegmentation,NLeSC/OCTSegmentation
a68dbc21-80a7-4260-9e9d-9aebb25e59f0,4tu-lbow-linear-buoyancy-wave-package,LBoW - Linear Buoyancy Wave Package,"LBoW is a python package for solving linear buoyancy wave problems, like for example uniform stratified flow over a bell-shaped hill. The software presents a semi-analytical implementation of linear theory for stratified flow (i.e., the Taylor-Goldstein equation).       Checkout the README.md file for installation instructions, software requirements, etc. More information as well as the latest version of the software can be found in the GitHub repository",,
a6920895-57c0-4dbb-8b53-58184d0b6003,remote-bmi,Remote BMI,"Remote BMI allows you to communicate with BMI enabled numerical models using a RESTful API.  The [Basic Model Interface (BMI)](https://bmi.readthedocs.io/en/stable/) is a standard interface for models.  The interface is available in different languages and a [language agnosting version in SIDL](https://github.com/csdms/bmi/blob/stable/bmi.sidl).  To have a consumer of the model and the provider of the model seperated you can use [grpc4bmi](https://grpc4bmi.readthedocs.io/), but this only works on languages that have a [gRPC](https://grpc.io/) implementation. This repository replaces the gRPC protocol with an REST API. The [REST API specification](openapi.yaml) is in the [OpenAPI](https://swagger.io/specification/) format.",https://github.com/eWaterCycle/remotebmi,eWaterCycle/remotebmi
a7366d43-bacf-4abc-9a71-08416f464049,ewatercycle-pcrglobwb,ewatercycle-pcrglobwb,"Generate PCRGlobWB forcing and run the HYPE hydrological model in a container using the eWaterCycle package.  **This software is part of the eWaterCycle stack. For a complete overview, please visit the [eWaterCycle project page](https://research-software-directory.org/projects/ewatercycle-ii).**",https://github.com/eWaterCycle/ewatercycle-pcrglobwb,eWaterCycle/ewatercycle-pcrglobwb
a873e054-85c6-44ec-939e-c588296d1d63,4tu-absorption-coefficient-estimator-using-domain-adaptation,Absorption Coefficient Estimator using Domain Adaptation,"Extracting absorption coefficients from a room impulse response using a convolutional neural network with domain adaptation     In building design, it is important to consider certain materials for certain acoustical properties. Specifically, the time it takes for an audio signal to decrease in volume by 60 dB is important. This can be estimated with Sabine's and Eyring's formula's, which both make use of the average absorption coefficient of the materials in a room. This absorption coefficient indicates how much of the original audio signal is absorbed into the material. However, measuring these absorption coefficients for a material is difficult and time consuming.  In this study, a machine learning approach is used to estimate the absorption coefficients, by using the room impulse response in combination with the layout of a room. A room impulse response is the characterizing sound of a room. These two pieces of data are processed through a convolutional neural network and a multilayer perceptron, respectively, and combined to make the final prediction of absorption coefficients. A novel approach of simulating the data is used, and a real dataset is used in conjunction with the simulations to use a state-of-the-art regression loss function made for domain adaptation. The results show that the machine learning approach still has a large error compared to using Eyring's formula, and that machine learning is not yet a viable option to use instead of conventional methods. This is the repository for the Research Project where the code to reproduce this subject is housed. Information about installing and using the codebase is available in the README.",https://data.4tu.nl/v3/datasets/473ac7b5-601c-468f-9936-3566164a2096.git,
a8774521-bc6c-4a1b-a814-0caa60354df9,4tu-soft-gripper-ar-framework,Soft Gripper AR Framework,"Open-source  framework  for  real-time  three-dimensional reconstruction of soft robots in eXtended Reality (Augmented and Virtual Reality)..Contains ROS code for interfacing with the actuator, together with unity scripts that allow for live animation data to be visualized in AR, as part of research into integrating XR technologies in the soft robot domain.",,
a88a92f6-6188-4137-b83a-e1df44b839ca,mimetic-operators-library-enhanced-mole,Mimetic Operators Library Enhanced (MOLE),,https://github.com/csrc-sdsu/mole,csrc-sdsu/mole
a8a7acf0-9105-4692-b449-1a8457ca549b,4tu-code-underlying-the-publication-humans-disagree-with-the-iou-for-measuring-object-detector-localization-error,"Code underlying the publication: ""Humans disagree with the IoU for measuring object detector localization error.""","The localization quality of automatic object detectors is typically evaluated by the Intersection over Union (IoU) score. In this work, we show that humans have a different view on localization quality. To evaluate this, we conduct a survey with more than 70 participants. Results show that for localization errors with the exact same IoU score, humans might not consider that these errors are equal, and express a preference. Our work is the first to evaluate IoU with humans and makes it clear that relying on IoU scores alone to evaluate localization errors might not be sufficient. In this repository, we provide a Jupyter notebook containing the code for our data analysis.",https://data.4tu.nl/v3/datasets/69828485-f662-4b28-9969-76f7943483ef.git,
a8cbb151-9b5d-4eca-91b1-1b43f242970b,2022-romanowska-002,Out of Africa - Conditional isotropic diffusion,"A generalisation of a demic diffusion model, where a number of agents (turtles) are initialised at a given region in a given map, and then reproduce themselves at each time step with a certain probability and only if there is a free adjacent grid cell (patch). A reimplementation of the classical study by Young and Bettinger (1992) investigating the possible drivers behind the Out of Africa dispersal of modern humans. The module code is an adaptation of the content of ""ch1_Y&Bdispersal.nlogo"" in the public repository associated to Romanowska, Wren, & Crabtree 2021 (https://github.com/SantaFeInstitute/ABMA/blob/master/ch1).  ![Interface screenshot](netlogo_implementation/documentation/Out%20of%20Africa%20dispersal%20-%20Conditional%20isotropic%20diffusion%20interface.png)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |map (input spatial data)|png/bmp (grid or raster)|image file containing the colour representation of spatial data differentiating land and sea. The sea must be represented in white. The example file attached (""world.png"") was taken from the public repository associated to Romanowska, Wren, & Crabtree 2021 (https://github.com/SantaFeInstitute/ABMA/blob/master/ch1/ch1_map.png).| |number-turtles|integer|number of agents to be created and placed in the 2D array (the world)| |inputX|integer|initial x coordinate of the center of the region where agents are created in initialisation| |inputY|integer|initial y coordinate of the center of the region where agents are created in initialisation| |maxInitialDistanceFromPoint|integer|maximum distance from input point where agents are created in initialisation| |numberOfTurtles|integer|number of agents to be created and placed in the 2D array (the world)| |pop_growth|float|probability of an agent reproducing once each time step. Value set through slider in interface (0 - 1, 0.05 increment).| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |world|object (world)|worldâ€™s grid initialised with input spatial data| |agents|object (agentset)|agents positioned over land patches (grid cells) with a particular colour as a proxy of their lineage.| |tickcounter|integer|simulation time step counter| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Romanowska-002,
a97a7f4d-0dca-4375-85a5-a00ed83870e0,lara,LARA,"**LARA** (Laboratory Automation Robotic Assistant) is an open source **Research Data Management Suite** and 2nd generation Electronic Lab Notebook with the goal to automate most of the data and metadata entering process by providing a rich infrastructure for lab communication, orchestration, data/metadata storage, evaluation and exchange. It utilises [Ontology](https://gitlab.com/opensourcelab/scientificdata/ontologies) based semantic web technology to make data findable [via SPARAQL](https://w3c.org/sparql).  **LARA** is designed to reduce manual data entry by humans to the bare minimum, since **scientists should fokus on the creative part of their research** (""no one likes to enter data by hand, if a machine can do it""). This is achieved by a very fine grained, [gRPC](https://grpc.io/) based API and a semantic, ontology based representation of most items.  Please note, that **LARA** is still in a vivid development stage, so please be not disappointed, if there are ""hickups"" or ""gaps"". We are very happy for any feeback and proposals via [LARA gitlab issues](https://gitlab.com/groups/LARAsuite/-/issues), it should be a community project, so please feel free to contribute !! **Thank you very much in advance !!**   ### [Planning](https://gitlab.com/LARAsuite/lara-django-projects)  Scientific research is structured in **LARA** in [Projects](https://gitlab.com/LARAsuite/lara-django-projects) and [Experiments](https://gitlab.com/LARAsuite/lara-django-projects):  [Projects](https://gitlab.com/LARAsuite/lara-django-projects) can have as many sub-experiments as desired, similarily, [Experiments](https://gitlab.com/LARAsuite/lara-django-projects) can have many *sub-experiments*.   ### [Process](https://gitlab.com/LARAsuite/lara-django-processes) [Orchestration](https://gitlab.com/opensourcelab/laborchestrator) / [Scheduling](https://gitlab.com/opensourcelab/pythonlabscheduler)  Experiments can be structured in [Processes](https://gitlab.com/LARAsuite/lara-django-processes) and [Procedures](https://gitlab.com/LARAsuite/lara-django-processes). [Processes](https://gitlab.com/LARAsuite/lara-django-processes) describe ""what"" is done in an experiment, while [Procedures](https://gitlab.com/LARAsuite/lara-django-processes) describe ""how"" it is done.  [Processes](https://gitlab.com/LARAsuite/lara-django-processes) and [Procedures](https://gitlab.com/LARAsuite/lara-django-processes) can be denoted in the [pythonLab Procdure/Process description language](https://gitlab.com/opensourcelab/pythonLab),  a very powerful, yet simple language, even suited for very complex procdures and processes, including loops, conditions, etc., enabling closed loop automation / experiments.   These [Processes](https://gitlab.com/LARAsuite/lara-django-processes) and [Procedures](https://gitlab.com/LARAsuite/lara-django-processes) are then executed by the [Lab Orchestrator](https://gitlab.com/opensourcelab/laborchestrator) and [pythonlab scheduler](https://gitlab.com/opensourcelab/pythonlabscheduler), which communicate with the corresponding [SiLA servers](https://sila-standard.com/), like [instruments, reactors, robots](https://gitlab.com/opensourcelab/devices), machine learning algorithms or even humans (human feedback is informed/integrated via an app).   The [Laborchestrator](https://gitlab.com/opensourcelab/laborchestrator) is a microservice that receives messages from the [LARA Django API](https://gitlab.com/LARAsuite/lara-django) to orchestrate the execution of experiments and procedures on the corresponding SiLA servers. The [pythonlab scheduler](https://gitlab.com/opensourcelab/pythonlabscheduler) microservice calculates the best schedule in which order the process and procedure steps should be executed.   ### [Data Collection](https://gitlab.com/LARAsuite/lara-django-data-collector)  The [Data Collector](https://gitlab.com/LARAsuite/lara-django-data-collector) is a microservice that receives messages from the [Laborchestrator](https://gitlab.com/opensourcelab/laborchestrator) to collect data from [corresponding SiLA servers](https://gitlab.com/opensourcelab/devices), if data is ready for collection and stores the data in the corresponding [LARA data database](https://gitlab.com/LARAsuite/lara-django-data). Larger datasets are not directly stored in the database, but in an S3-compliant object storage, [minio](https://min.io/) by default, and only linked to the entries in the database. We recoommend to store the data in an efficient, open data format, like [SciDat](https://gitlab.com/opensourcelab/scientificdata/scidat), which combines tabular data with (semantic, [JSON-LD](https://json-ld.org/) based) metadata in a single, compact file.  ### [LARA database](https://gitlab.com/LARAsuite/lara-django)  The [LARA database](https://gitlab.com/LARAsuite/lara-django) is [python django](https://www.djangoproject.com/) based and consist of many modules (the current set can be [easily expanded](https://gitlab.com/LARAsuite/lara-django-app-cookiecutter) by specific applications):   Currently we have modules for [Procdures, Processes and Methods](https://gitlab.com/LARAsuite/lara-django-processes), [Material - Parts, Devices and Labware](https://gitlab.com/LARAsuite/lara-django-material),  [Substances, Polymers, Mixtures and Reactions](https://gitlab.com/LARAsuite/lara-django-substances), [Sequences, like DNA, RNA, peptide/protein sequences](https://gitlab.com/LARAsuite/lara-django-sequences), [3D Structures of molecules](https://gitlab.com/LARAsuite/lara-django-structure), [Organisms](https://gitlab.com/LARAsuite/lara-django-organisms), [Samples](https://gitlab.com/LARAsuite/lara-django-samples), [Data](https://gitlab.com/LARAsuite/lara-django-data) and for the management of [Scientists, Institutions, Companies, etc.](https://gitlab.com/LARAsuite/lara-django-people).  **LARA** differentiates between ""abstract"" entities, like ""the substance ethanol in general with its generic properties molecular weight and sum formula etc."" or ""thermometer"" as a ""generic thermometer as a device for measureing temperature"" and an ""instance"" of these abstracta: ""the bottle of ethanol in a particular lab from a particular vendor"" / ""the thermometer of vendor x with a temperature range 250-350K"".  These instance information of individual objects is stored in the corresponding stores. Currently,  [Substance Store](https://gitlab.com/LARAsuite/lara-django-substances-store), [Material Store](https://gitlab.com/LARAsuite/lara-django-material-store) and [Organism Store](https://gitlab.com/LARAsuite/lara-django-organisms-store).  These stores can also be used as **""free"" inventories** for the lab (ordering workflows will follow soon).  The *""class/abstractum""* versus *""instance/individuum""* paradigm is a common design paradigm in **LARA**.  ### [Semantic Database](https://gitlab.com/LARAsuite/lara-django)  For the semantic representation of the items a triple store is used, which is based on [openlink virtuoso](https://virtuoso.openlinksw.com/). The terms are defined in [Ontologies](https://gitlab.com/opensourcelab/scientificdata/ontologies). This semantic representation is used to make the data findable via [SPARQL](https://w3c.org/sparql). The semantic information generated automatically, when new database items are created.  ### [Data(-base) Synchronisation](https://gitlab.com/LARAsuite/lara-django-data-sync)  Selected parts of projects, experiments and data can be synchronised with other LARA instances via the [Data Synchronisation](https://gitlab.com/LARAsuite/lara-django-data-sync) microservice.  ### [Data Evaluation](https://gitlab.com/LARAsuite/lara-django-data-evaluator)  Data can be evaluated via the [Data Evaluation](https://gitlab.com/LARAsuite/lara-django-data-evaluator) microservice, which is based on [pandas](https://pandas.pydata.org/), [numpy](https://numpy.org/) and [scipy](https://www.scipy.org/).  The evalated data is then visualised via default [Data Visualisations](https://gitlab.com/LARAsuite/lara-django-data).  For more advanced evaluations and visualisations, [Jupyter Notebooks](https://jupyter.org/) can be used, as all data is accessible via the [LARA gRPC APIs](https://gitlab.com/LARAsuite/lara-django).  ### [Search](https://gitlab.com/LARAsuite/lara-django)  For simple searches, each Module has a search function, which can be used to search for specific items. For more advanced, complex searches, the [LARA SPARQL Endpoint](https://gitlab.com/LARAsuite/lara-django) can be used.   ## [Installation](https://gitlab.com/LARAsuite/lara-django/-/blob/master/README.md?ref_type=heads)  Installation is made as easy as possible via only two commands. The recommended installation method for testing is currently via [docker compose](https://docs.docker.com/engine/install/) as described in the [README.md of the lara-django repository](https://gitlab.com/LARAsuite/lara-django). [docker compose](https://docs.docker.com/engine/install/) is now part of a recent docker installations, so no additional installation is required.  ## Related Projects  More generic Modules can be found at our [OpenSourceLab Project](https://gitlab.com/opensourcelab).",https://gitlab.com/LARAsuite,
a9e5ecf3-af9f-41fe-80a4-61729b1c9fe0,loris,LORIS,"**LORIS** was originally developed for the NIH MRI Study of Normal Brain Development (NIHPD; Evans and Brain Development Cooperative Group, 2006), which was funded in 1999. LORIS was designed from inception to facilitate active multi-site study management and centralized archiving and retrieval of multi-modal data.  The initial instance of LORIS included behavioural data entry functionality, scoring algorithms and normative lookup tables coded directly into the database for the full battery of measures. An imaging browser was added soon after for management and quality control procedures. Querying capabilities were eventually included using our Data Query Tool with the public dissemination phase of the study in 2006.  Having completed the full lifecycle of the NIHPD multi-site study, LORIS was adopted as the data platform for IBIS (Infant Brain Imaging Study), the Autism sibling study of brain development. Eventually, LORIS began to proliferate globally to numerous projects.  Numerous tools and modules have since been incorporated into the platform to facilitate data management, quality control, and sharing efforts. Web-based imaging visualization was introduced via BrainBrowser in 2011.  LORISâ€™ extensive suite of modules is deployed in numerous projects globally with code contributions coming from an ever-growing community of developers.",https://github.com/aces/Loris,aces/Loris
a9f263f8-4e4d-4be9-a780-39a7fb0551c4,4tu-cirrus-hl-campaign-codes-code-accompanying-dissertation,CIRRUS-HL-campaign-codes---Code-accompanying-dissertation,"This repository provides codes relevant for the data analysis and key plots generation of the dissertation it accompanies: ""Microphysical properties and interplay of natural cirrus, contrail cirrus and aerosol at different latitudes"" by Elena De La Torre Castro  The data it handles consists of cirrus ice particle in situ measurements performed during the CIRRUS-HL campaign in June and July 2021 with the german research aircraft HALO. It includes the microphysical properties of these cirrus measurements.  It also combines this data with the backward trajectories from the air masses, in order to analyse the cloud properties according to their cloud history and formation location. Further data from aerosol in situ measurements,  simulations of aerosol and ice particle properties, nitrogen oxides (NOy) measurements and reference measurements from the aircraft systems.  The cloud particle data was obtained with a combination of cloud probes: Cloud Droplet Probe (CDP), Cloud Imaging Probe (CIPG), and Precipitation Imaging Probe (PIP). In this repository, the method to obtain a combined particle size distribution is included.  For that purpose, example files of one flight are provided in the campaign dataset. For creating final results plots, combined files of measurements and model data are provided. The required data to use this codes is given in the Data folder of this campaign dataset.  The authors of the data, variables and basic description of the data are included in the corresponding files within the Data folder.  The repository basically containes Python scripts and jupyter notebooks. The particular explanation of each code is provided in this README in the section Structure.",https://data.4tu.nl/v3/datasets/0686c532-6e2d-41d7-9305-72c49ea816ab.git,
aa17cbef-77a2-4815-af80-6caf55b4ccf5,tilburg-university-coding-cafe,Tilburg University Coding CafÃ©,"# Tilburg University Coding CafÃ©  Do you want to learn how to write better code? Finally learn version control? Make your code more robust? Figure out how to smoothly write software within teams? Then the Coding CafÃ© might just be your cup of tea!  The Coding CafÃ© is an informal meetup where researchers get together and talk research software! As a community, we will exchange our experiences, challenges, and best practices. The Coding CafÃ© is an established format in universities around the country. Inspired by its success, we decided to host our own sessions as well.  And what would be a coding session without a good snack? To keep all your coding brain cells fueled, we will provide a selection of freshly delivered pizza.  ## Setup The Coding CafÃ© is a local implementation of the [CAFE playbook](https://code-cafes-nl.github.io/cafe_playbook/). Our sessions have one coding concept that is put in the spotlight. You will learn all you need to get started in a short presentation. After that, you get to practice what you've learned hands-on!  Participants are also welcome to stray off and find some peers to take a look at your own code. It's all FAIR game! For example, they can bring their own programming questions, conundrums, and pesky bugs to discuss and (hopefully) resolve with the help of the community!  ## Quick info Type of instruction: Community lunch meetup Time: 11:00 - 13:00 hrs Preparation: Bring your own laptop  ## Who can attend? The Coding CafÃ© is open to all researchers (including PhDs), support staff and student assistants. Whether you have never written a line of code in your life or have published a dozen packages on CRAN or PyPI, everyone is welcome.  ## Materials This registered entry connects to a GitLab group where we collect the materials from the invited talks of past sessions. The licenses vary per contribution and can be found there.  ## Funding From July 2024 to August 2025, the Coding CafÃ© has been funded by the Netherlands eScienceCenter within their Fellowship program. The invited talks were held by amazing engineers and teachers from there as well! Thanks so much for making this possible!",https://gitlab.uvt.nl/coding-caf,
aa8a5aab-b39f-4467-bfca-8678f9a030bc,eastroviz,eAstroViz,"This is a visualization and RFI mitigation tool of the Netherlands eScience center eAstronomy project.  This tool can convert and visualize radio astronomy measurement sets (i.e. visibilities), as well as most LOFAR intermediate data products, such as raw voltages, filtered data and beam formed data. In addition, this tool can perform RFI mitigation, and can be used to quickly develop new RFI mitigation algorithms.  In addition, this repository contains the real-time RFI mitigation code developed for LOFAR, in the directory LOFAR-source.  Finally, we have developed a GPU prototype version of the code as well. This code was developed by Linus Schoemaker in the context of his masters project. The code is in the directory GPU-source.",https://github.com/NLeSC/eAstroViz,NLeSC/eAstroViz
aaa2adff-551b-476f-89ca-39841ecfc026,chemspax,ChemSpaX: A Python tool for local chemical space exploration,* The main function and Python file are used to place functional groups. The user only needs to provide an MDL Molfile (.mol) and xyz file for the skeleton on which functional groups need to be placed. Examples are provided in the readme. * This is useful for researchers interested in building libraries of chemical 3D structures.,https://github.com/EPiCs-group/chemspax,EPiCs-group/chemspax
ab38f115-edf0-4e95-a5b6-102bfde1874f,4tu-r-code-underlying-the-analysis-reported-in-the-manuscript-machine-learning-approach-for-pitch-type-classification-based-on-pelvis-and-trunk-kinematics-captured-with-wearable-sensors,"R code underlying the analysis reported in the manuscript ""Machine learning approach for pitch type classification based on pelvis and trunk kinematics captured with wearable sensors""","The study utilized classifiers integrated in the caret R package including K-Nearest Neighbours (KNN), Naive Bayes (NB), Random Forest (RF) and Support Vector Machine (SVM). We investigated the performance of the classifiers in both binary and multiclass classification, including additional Logistic Regression (LOGREG) for binary and Multinomial Logistic Regression (MNOM) for multiclass classification task.     We used a database created by PITCHPERFECT that characterises each pitch with 3 features used directly from the system (pelvis peak angular velocity, trunk peak angular velocity and separation time between them). Data were pre-processed and analysed using R programming language (version 4.3.1). All continuous features were scaled and centred.     We set up our training and testing cases following the 80\% (training) and 20\% (testing) split. To achieve a fair understanding of the generalizability of the classifiers, in the designated training set Leave-One-Group-Out Cross-Validation (LOGO-CV) was carried out. The performance of the classifiers is evaluated by four evaluation criteria - Accuracy, Sensitivity , Precision and F1-score. Hyperparameters were tuned using grid search, a default method for optimizing tuning parameters in the caret package. Feature selection was performed using correlation analysis. Since the correlation between the features was low, the models were trained and tested using all variables derived from PITCHPERFECT system.",,
ab4d5624-b503-49dd-af44-72f725313721,4tu-optimization-code-further-supporting-material-for-a-hardware-efficient-leakage-reduction-scheme-for-quantum-error-correction-with-superconducting-transmon-qubits,"optimization_code - Further supporting material for ""A hardware-efficient leakage-reduction scheme for quantum error correction with superconducting transmon qubits""",Extract of the code used in Section IB for the optimization over t_p as described in the text.     This code pertains to the publication https://arxiv.org/abs/2102.08336 (after peer review) and is part of a collection of additional supporting materials.  See README for more information.,,
ab85b9cd-016a-49a3-b82e-8087c229b85f,4tu-numerical-and-analytical-model-for-taylor-flow-in-tubular-co2-electrolyser,Numerical and analytical model for Taylor flow in tubular CO2 electrolyser,"Numerical simulations in COMSOL based on the unit cell approach for gas-liquid Taylor flow in a tubular flow cell. The COMSOL files are generated and analysed via a link to matlab (therefore livelink needs to be used to run all files; see README.txt). The Matlab files further include an analytical correlation which can be run without COMSOL. The code pretains to the data shown and discussed in the publication ""Electrochemical Reduction of CO2 in Tubular Flow Cells under Gasâ€“Liquid Taylor Flow"" and Chapter 2 of the PhD dissertation of Bagemihl.",,
abb5d7ea-32f0-4e99-9efe-fa76e50d8ca5,4tu-axies-identifying-and-evaluating-context-specific-values-code,Axies: Identifying and Evaluating Context Specific Values - code,"A Collaborative Platform for Identifying Context-Specific Values. Code developed for the paper ""Axies: Identifying and Evaluating Context Specific Values"", published at AAMAS '21.",,
abe7d25d-6662-4960-acdf-302e8d1effc2,4tu-epistemic-bellman-operators,epistemic-bellman-operators,"Code underlying dissertation work ""Bayesian Model-Free Deep Reinforcement Learning"". This data set contains code for reproducing the experiments conducted in the chapter ""Epistemic Bellman Operators"", with the goal of validating the mathematical theory in the chapter. The code has no applications other than supporting the mathematical theory.",https://data.4tu.nl/v3/datasets/253d2933-5712-48d8-ae79-6835dbdd8bd0.git,
ac11323e-6cd4-4fba-b70b-097167fcffbd,4tu-code-to-produce-the-results-of-the-publication-neurophysiological-validation-of-simultaneous-intrinsic-and-reflexive-joint-impedance-estimates,Code to produce the results of the publication: â€œNeurophysiological Validation of Simultaneous Intrinsic and Reflexive Joint Impedance Estimatesâ€,"This folder contains code to produce the results of ""R. C. van â€™t Veld , A. C. Schouten, H. van der Kooij and E. H. F. van Asseldonk. Neurophysiological Validation of Simultaneous Intrinsic and Reflexive Joint Impedance Estimates. Journal of NeuroEngineering and Rehabilitation. https://doi.org/10.1186/s12984-021-00809-3""",,
ace89e98-78b1-4273-9f52-0ae3563f4ab8,2022-daems-001,Resource exploitation procedure,![Interface screenshot](netlogo_implementation/documentation/resourceExp%20interface.png)    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |resource|integer|Amount of resources per patch| |stock|integer|Amount of resources carried by the agent| |storage|integer|Amount of resources stored at homebase| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |homebase-storage|integer|Amount of resources stored in the homebase. Tracked as value in a reporter and through plot on the interface| ||||,https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-DAEMS-001,
acec59b9-8a53-4eae-9927-a70f749f8908,4tu-attitudes-toward-a-virtual-smoking-cessation-coach-analysis-code,Attitudes Toward a Virtual Smoking Cessation Coach: Analysis Code,"This is the analysis code underlying the paper ""Attitudes Toward a Virtual Smoking Cessation Coach: Relationship and Willingness to Continue"" by Nele Albers, Mark A. Neerincx, Nadyne L. Aretz, Mahira Ali, Arsen Ekinci and Willem-Paul Brinkman. In this paper, we conduct a mixed-methods analysis of people's relationship and willingness to continue working with the text-based virtual coach Sam.   Data:   Our analysis is based on the data collected in an online experiment in which more than 500 daily smokers interacted with the text-based virtual coach (i.e., a conversational agent) in up to 5 sessions spread over at least 9 days. In each session, Sam proposed a new preparatory activity for quitting smoking or becoming more physically active, with the latter possibly aiding the former. After the 5 sessions, participants filled in a post-questionnaire in which they answered 6 questions about their attitude toward Sam by means of a rating on a scale from -5 to 5 and a free-text response to the follow-up question ""Why do you think so?"" The questions were adapted based on the ones by Provoost et al. (2020). Our paper focuses on people's responses to the questions ""Do you prefer continuing or stopping to work with the conversational agent Sam?"" (-5: ""Definitely prefer stopping"", 5: ""Definitely prefer continuing"") and ""How would you characterize your relationship with the conversational agent Sam?"" (-5: ""Complete stranger"", 5: ""Close friend""). The complete dataset can be found here: https://doi.org/10.4121/19934783.v1. Notably, this dataset contains only data used to study the acceptance of the virtual coach Sam. Further data from the same study has been published in separate repositories (e.g., https://doi.org/10.4121/20284131.v2).   Virtual coach:   The implementation of the virtual coach is available here: https://doi.org/10.5281/zenodo.6319356.   Analysis:   Our mixed-methods analysis is based on the publicly available Bachelor's theses by Nadyne L. Aretz and Mahira Ali:      Reasons for wanting to continue or stop using a virtual coach for quitting smoking and increasing physical activity: http://resolver.tudelft.nl/uuid:cb4e9c2e-27fb-4074-95f0-307d4d964be9.   Relationship with Sam: Â http://resolver.tudelft.nl/uuid:a79c34d7-ab6f-4385-b566-c0c3302a580e.  In this repository, we provide all code needed to reproduce the analyses reported in our paper. We thereby largely rely on Docker. Please refer to the README-file for more information.   In case of questions about the analysis or data, please contact Nele Albers (n.albers@tudelft.nl).",,
ad498529-f7c8-4a70-b39a-6d3f223cccbd,4tu-speed-robust-scheduling-supplemental-material,Speed Robust Scheduling Supplemental Material,"This supplemental material proves two claims of the main paper named ""Speed-Robust Scheduling"". It provides a Mathematica sheet and its output, proving the last claim of Lemma 11; a Python script used to certify that for the relevant values of number of machines and unit-size jobs, there exists a 4/3-robust solution (Lemma 15); a compressed data file containing the input of the script check.py; a file with human-readable data.",,
ad58ee70-7b9f-43fc-84c6-bc6e89395858,4tu-code-underlying-the-publication-self-supervised-ppg-representation-learning-shows-high-inter-subject-variability,"Code underlying the publication: ""Self-Supervised PPG Representation Learning Shows High Inter-Subject Variability""","This repository provides the implementation of a Self-Supervised Learning (SSL) framework for photoplethysmography (PPG) signal representation, as detailed in the paper ""Self-Supervised PPG Representation Learning Shows High Inter-Subject Variability."" The framework addresses label scarcity in PPG data analysis by utilizing signal reconstruction as a pretext task to learn informative representations, with a focus on applications such as activity recognition. The study highlights that, while SSL improves downstream supervised task performance and enables the use of simpler models, significant inter-subject variability remains a challenge, limiting the modelâ€™s generalization capabilities.",https://data.4tu.nl/v3/datasets/e64258fe-f957-465f-919a-f02a70ac17f0.git,
ad934a1b-8f11-4492-82d6-500487e8c351,3d-e-chem-sygma,SyGMa,"- Predict human metabolites for a given (drug) molecule by: - Systematically applying chemical rules for phase 1 and phase 2 biotransformations - Ranking of predicted metabolites based on empirical success rates for each rule  This tool is a reimplementation of the metabolic rules outlined in:   [Ridder, L., & Wagener, M. (2008) SyGMa: combining expert knowledge and empirical scoring in the prediction of metabolites. ChemMedChem, 3(5), 821-832.](http://onlinelibrary.wiley.com/doi/10.1002/cmdc.200700312/full)",https://github.com/3D-e-Chem/sygma,3D-e-Chem/sygma
adc0be99-552c-4705-90d4-7fa6a720fff4,4tu-air-traffic-control-reinforcement-learning-environment,Air Traffic Control Reinforcement Learning Environment,Simplified air traffic control environment suitable for applying Reinforcement Learning techniques for conflict resolution.,,
adcf04f1-199f-418e-8786-99e5093a090d,stroll,Stroll,"* Fast and high accuracy * Easy to use: integrates with Stanza, formerly StanfordNLP, and the VU Readingmachine  Stroll is trained on the largest publically available dataset for Dutch, SoNaR. For the given input sentences, it finds the frames (typically the verbs),  and its corresponding arguments. There are 4 arguments, Arg0 - Arg4, and 13 modifiers, ArgM-*.",https://github.com/Filter-Bubble/stroll,Filter-Bubble/stroll
ae0124ff-4fc8-4b8c-9cb0-5ce2fdb41bbe,lue,LUE,"LUE is software for storing and manipulating large amounts of information for large amounts of objects. This information can optionally have a reference in time and space. For example, LUE can represent collections of wandering animals and their properties, some of which are changing through time, or the elevation of the surface of one or multiple areas. The software is useful, and currently mainly used, in the context of environmental modelling of biological and physical systems, represented by agents and fields, but we make sure that the software is as generic as possible, allowing it to be useful in other contexts as well.  Currently, LUE contains two main parts: *LUE data model* and *LUE framework*. LUE data model is an implementation of the LUE physical data model, which allows users to perform I/O to the ""LUE Scientific Database"". It allows for the storage of large amounts of objects and their location in time and space, and their properties.  LUE framework is a collection of data types and algorithms that can be combined to translate large amounts of information. It allows computations to be performed on hardware ranging from laptops to compute clusters, without the user having to know about high-performance computing and the related technology.",https://github.com/computationalgeography/lue,computationalgeography/lue
ae0e6758-6f48-4756-888c-1d5d5d8ba781,umple,Umple,"Umple is an open-source software modeling tool and compiler. It incorporates textual language constructs for UML modeling, including associations and state machines. It includes traits, aspects, and mixins for separation of concerns. It supports embedding methods written in many object-oriented languages, enabling it to generate complete multilingual systems. It provides comprehensive analysis of models and generates many kinds of diagrams, some of which can be edited to update the Umple code. Umple runs on the command line, in a web browser or in integrated development environments. It is designed to help developers reduce code volume, while they develop in an agile, model-driven manner. Umple is also targeted at educational users where students are motivated by its ability to generate real systems from their software models.",https://github.com/umple/umple,umple/umple
ae579b5f-6241-41d1-bb59-ee84339d42da,kunefe,kunefe,# Kunefe  Kunefe is a Python package that helps users run containerized applications (Docker) on HPC systems. It can:  - convert an existing Docker image into an Apptainer image - connect to a remote host via SSH and run shell commands - generate scripts for batch jobs - submit generated job scripts to HPC batch queues (SLURM) - copy files and folders to a remote system - retrieve files and folders from a remote system - install Apptainer on a remote system - monitor the job queue (SLURM),https://github.com/mess-nlesc/kunefe,mess-nlesc/kunefe
ae6c13f2-99de-4786-af5b-ec112bc69699,4tu-deep-learning-educational-material,Deep Learning - Educational Material,"Collection of educational material including lab notebooks, assignments, and lecture material developed for the course Deep Learning of the Engineering with AI Minor programme, EEMCS, TUDelft.",https://data.4tu.nl/v3/datasets/1d32abdc-f90d-455b-8386-ef60b5465bc4.git,
af34e397-5492-4499-9b1d-b893446a0e76,amuse,AMUSE,* provides a homogeneous environment to design astrophysical simulations used by students and researchers * couples different simulation codes in a Python environment to be run locally or distributed * provides a very easy way to solve astronomical problems that involve complicated physical interactions  * used for more than 40 scientific papers and PhD theses,https://github.com/amusecode/amuse,amusecode/amuse
af388ed1-4a07-40bc-aedf-212d424e5c33,q-cardia,q-cardIA,"# q-cardIA This is a python infrastructure for developing and deploying automated quantitative cardiovascular image analysis.   q-cardIA is maintained by a team of students and researchers based in the Medical Image Analysis group at the Eindhoven University of Technology.  The suite of libraries and tools will provide and end-to-end framework covering from data loading and model training to the application of trained models and the visualisation and interaction with data and analysis results.  - [qcardia-data](https://github.com/q-cardIA/qcardia-data): A PyTorch and MONAI based library to build and handle medical imaging data pipelines. Can be used to quickly get highly customizable Dataloaders, including data augmentation, for deep learning purposes, especially for already supported datasets. - [qcardia-models](https://github.com/q-cardIA/qcardia-models): A PyTorch based library to build and handle medical imaging model pipelines. Can be used to quickly get highly customizable U-Net or EncoderMLP models for deep learning purposes.  - [qcardia](https://github.com/q-cardIA/qcardia): A library to deploy trained deep learning models for quantitative cardiac image analysis. - [qcardia-app](https://github.com/q-cardIA/qcardia-app): A python-based application to visualization and interact with results of deployed quantitative cardiac image analysis models.",https://github.com/q-cardIA/,
afe691ac-e1aa-49c2-8c33-11e23a44c072,4tu-code-accompanying-the-doctoral-dissertation-theory-of-chirality-induced-spin-selectivity-in-two-terminal-transport,Code Accompanying the Doctoral Dissertation: Theory of Chirality Induced Spin Selectivity in Two Terminal Transport,"Code accompanying the Doctoral Dissertation of Karssien Hero Huisman.    The code (dataset) aims to reproduce the figures in chapters 2-5 of the dissertation.  Each chapter has a corresponding .zip file(s).  The code in the .zip files are used in the following chapters (also see READ_ME_thesis.rtf):  Chapter 2: CISS_effect.zipChapter 3: CISS_CoulombInteraction.zipChapter 3,Appendix: Nearest_Neigbour_Coulomb.zipChapter 4: Coulomb_Bipartite.zipChapter 5: VibrationalModes_Troisi_Compact.zip  Each .zip file contains : jupyternotebook (.ipynb) and (.py) files and a READ_ME file.    The research objective is to calculate the currents for opposite magnetizations of the leads. ( chapters 2-5).  In order to do that we calculate:  i) Transmission coefficients (spin-dependent) with the non-equilibrium Green's function technique. (chapters 2-5)  ii) The chemical potential of the voltage probe (chapter 2).  iii) The electron density for systems where Coulomb interactions are present is self-consistently calculated. (chapters 3 &amp; 4)  iv) The inelastic transmission due to vibrational modes (chapters 5)    We thus perform a numerical, theoretical study.",,
b0058b27-daa6-4821-92e4-fd494ceb010e,4tu-software-for-detecting-structural-heterogeneity-in-single-molecule-localization-microscopy-data,Software for Detecting Structural Heterogeneity in Single-Molecule Localization Microscopy Data,"This software implements the a-priori knowledge-free unsupervised classification of structurally different particles, employing the Bhattacharya cost function as dissimilarity matrix.",,
b09a12e6-9d1f-428d-9982-a8fb70251a97,tissue-simulation-toolkit,Tissue Simulation Toolkit,"Welcome to Tissue Simulation Toolkit (TST) 2.0, a library for two-dimensional simulations of Glazier and Graner's Cellular Potts model (Glazier and Graner, 1993).  TST 2.0 is an efficient C++ library for two-dimensional Cellular Potts Simulations. It is suitable for simulations with live visualization as well as batch simulations on clusters.  TST 2.0 provides many recent extensions to the CPM, including  - Efficient edgelist algorithm - Infinite number of PDE layers (forward Euler) - A reaction diffusion solver on the CPU and on CUDA - Interaction of CPM cells and PDE (secretion, absorption) - Chemotaxis - Length and connectivity constraints - Act-CPM model (Niculescu et al., PLOS Comput Biol 2015) - Discrete fibrous extracellular matrix (molecular dynamics)  The current version of the TST includes example programs for the following published simulations:  - Differential adhesion driven cell sorting (Glazier and Graner, 1993) - Cell elongation dependent vasculogenesis (blood vessel growth) (Merks et al., 2006) - Contact-inhibition dependent vasculogenesis and angiogenesis (Merks and Glazier, 2005; Merks and Glazier, 2006; Merks et al, PLoS Comput Biol 2008) - Hybrid cellular Potts and bead-spring modeling of cells in fibrous extracellular matrix (Tsingos and Bakker et al, Biophys J. 2023)  and visualization of:  - Cells, according to cell type or anything you wish - Chemical fields, using color ramps and contour lines (level sets)",https://github.com/mathbioleiden/Tissue-Simulation-Toolkit,mathbioleiden/Tissue-Simulation-Toolkit
b19a2903-10b2-4731-90cb-acb5572d34d4,4tu-source-code-for-the-paper-constructing-phylogenetic-networks-via-cherry-picking-and-machine-learning,"Source code for the paper ""Constructing Phylogenetic Networks via Cherry Picking and Machine Learning""","The contents of this repository is the source code for the paper&nbsp;""Constructing Phylogenetic Networks via Cherry Picking and Machine Learning"" by Giulia Bernardini, Leo van Iersel, Esther Julien, and Leen Stougie. The research question for this project is the following: ""Can machine learning increase the performance of a heuristic to construct phylogenetic networks given a set of phylogenetic trees with the same set of taxa?"" This question has been answered by formalizing a method that uses ML and so-called method ""cherry picking"", and performing numerical experiments based on python code.",https://data.4tu.nl/v3/datasets/39886858-4dc3-4472-a9e7-f979231cba73.git,
b1ca1721-15e1-4811-88ef-e1afbcfdd7fd,ceiba,Ceiba,"* Ceiba solves the problem of computing, storing, and securely sharing computationally expensive simulation results. * Ceiba allows users to request new tasks, receives the taskâ€™s results to be stored, and returns some available data when requested. * Ceiba offers a command-line interface to interact with the service, like logging in, computing, querying.    Scientific simulations generate a large volume of data that needs to be stored and processed by multidisciplinary teams across different geographical locations. Distributing computational expensive simulations among the available resources, avoiding duplication, and keeping the data safe are challenges that scientists face every day.  Ceiba and its command-line interface Ceiba-cli solve the problem of computing, storing, and securely sharing computationally expensive simulation results. Researchers can save significant time and resources by easily computing new data and reusing existing simulation data to answer their questions.",https://github.com/nlesc-nano/ceiba,nlesc-nano/ceiba
b1e74d89-5d9c-43a7-b19b-fec01d905c9c,4tu-script-underlying-the-paper-on-robustly-convergent-and-efficient-iterative-methods-for-anisotropic-radiative-transfer,"Script underlying the paper ""On Robustly Convergent and Efficient Iterative Methods for Anisotropic Radiative Transfer""","*** Solver for anisotropic radiative transfer in an XY-geometry *** authors: J. DÃ¶lz, O. Palii, M. SchlottbomÂ    corresponding author: Matthias Schlottbom, University of Twente, m.schlottbom@utwente.nl       *** Scientific reference *** JÃ¼rgen DÃ¶lz, Olena Palii, Matthias Schlottbom   On robustly convergent and efficient iterative methods for anisotropic radiative transfer.Â    Journal of Scientific Computing 90 (94), 2022.Â    https://doi.org/10.1007/s10915-021-01757-9   When using this code, cite that paper.       *** Description *** For using the code, unzip the repository and run solve.m.Â    In lines 4-12, there are parameters regarding the solver and meshes used.   In line 51, a function is called that specifies the optical parameters.",,
b2858315-1141-4272-8de5-ac82998b97dc,duqtools,duqtools,*Duqtools* is a tool for **D**ynamic **U**ndertainty **Q**uantification for Tokamak reactor simulations modelling.  Features:  - Set up 100s of simulation runs from a single template - Launch stardard sets of sensitivy tests with minimal programming - Batch job submission and status tracking - Supports the Standardized Interface Data Structures (IDSs) data directory - Compare and visualize 100s of simulations in one overview - Display simulation results as confidence ranges and distributions,https://github.com/duqtools/duqtools,duqtools/duqtools
b2b5aeee-067a-4195-a7c8-bbc594923910,beyond-the-book,Beyond the book,"* Provides humanities scholars with a tool to mine Wikipedia. * Mine Wikipedia revision history. * Ease of use for humanities scholars * Can be used as a proxy to measure the cultural bias from a country towards a specific topic   The book translation market is a topic of interest in literary studies, but the reasons why a book is selected for translation are not well understood. The ""Beyond the Book"" project investigates whether web resources like Wikipedia can be used to establish the level of cultural bias. This work describes the eScience tools used to estimate the cultural appeal of a book: semantic linking is used to identify key words in the text of the book, and afterwards the revision information from corresponding Wikipedia articles is examined to identify countries that generated a more than average amount of contributions to those articles. Comparison between the number of contributions from two countries on the same set of articles may show with which knowledge the contributors are familiar. We assume a lack of contributions from a country may indicate a gap in the knowledge of readers from that country. We assume that a book dealing with that concept could be more exotic and therefore more appealing for certain readers, while others are therefore less interested in the book. An indication of the 'level of exoticness' thus could help a reader/publisher to decide to read/translate the book or not. Experimental results are presented for four selected books from a set of 564 books written in Dutch or translated into Dutch, assessing their potential appeal for a Canadian audience. A qualitative assessment of quantitative results provides insight into named entities that may indicate a high/low cultural bias towards a book.",https://github.com/c-martinez/BeyondTheBook,c-martinez/BeyondTheBook
b3e068b4-2ad5-4b22-a874-99c9f6b18cd8,self-hosted-runners,self-hosted runners,"- Set up different types of servers for continuous integration using Ansible playbooks - Documentation for remote systems such as SURF HPC-cloud, as well as local systems via Docker, VirtualBox, and Vagrant - Includes examples for continuous integration servers with GPU  Additional repositories: * https://github.com/ci-for-science/example-gpu-houston * https://github.com/ci-for-science/example-hello-world * https://github.com/ci-for-science/example-python-1 * https://github.com/ci-for-science/example-python-monte-carlo-pi",https://github.com/ci-for-science/self-hosted-runners,ci-for-science/self-hosted-runners
b4316272-4952-42fa-9f87-3d2b049d7c95,4tu-source-code-of-the-jeulynx-prototype-framework,Source code of the jEULYNX prototype framework,"A prototype version of the jEULYNX prototype framework, which uses an internal domain-specific language to capture SysML diagrams as digital models. It also contains several exports for the models, in particular to the process algebra mCRL2.",,
b4744231-1946-4ee9-b07d-d4e8d9f3e349,arctic-web-map,Arctic Web Map,"# About Artic Web Map AWM is a component in the Arctic Connect project. It is designed to provide a better mapping tool for researchers by displaying a more appropriate projection for Arctic research.  AWM is built on an open source mapping stack, using similar technologies to OpenStreetMap. Background map data is used under license from OpenStreetMap contributors. Tiles are based on the data in the OSM extract when the tile servers were provisioned. For v1.0 of the tiles, the tiles are based on 2017 OSM data. For v2.0 of the tiles, this is based on April 2019 OSM data.  If you have any questions or want to learn more about Arctic Web Map or Arctic Connect, please contact the project PI: Steve Liang.  If you have found a bug or the tiles/client library do not work as you expect, please contact the project technical manager: James Badger.  Arctic Web Map has two components: An Arctic-focused tile server, and a Leaflet-based client library. By providing tiles in multiple Arctic projections, data can be more accurately visualized compared to most Mercator projected map tiles.  The client library, PolarMap.js, is designed to be easy to use and easy to extend. It does this by providing a simple wrapper for building a typical Leaflet map, and also by providing base classes that can be customized to build a web map for your specific situation. Please see the PolarMap.js API for more information on how to get started.  # Service **Multi-projection web mapping service for Arctic regions:** An HTTP tile server that provides tiles in an Arctic specific projection. This means sea and land areas will not be distorted for the Arctic region, as they are in Mercator projections (Google, Bing, OpenStreetMap). - [Source code](https://github.com/GeoSensorWebLab/awm-styles)  Arctic Web Map offers other services - [Source code](https://github.com/orgs/GeoSensorWebLab/repositories)",https://github.com/GeoSensorWebLab/arctic-biomap-server,GeoSensorWebLab/arctic-biomap-server
b5050bc0-0015-4d57-bdef-76f0c1454307,4tu-a-public-ground-truth-dataset-for-handwritten-circuit-diagram-images,A Public Ground-Truth Dataset for Handwritten Circuit Diagram Images,"CGHD  This dataset contains images of hand-drawn electrical circuit diagrams as well as accompanying annotation and segmentation ground-truth files. It is intended to train (e.g. ANN) models for extracting electrical graphs from raster graphics.     Content  2.549 Raw Images (Annotated)25 Drafters (plus Images provided by TU Dresden)12 Circuits per Drafter2 Drawings per Circuit4 Photos per Drawing212.280 Bounding Box Annotations29.790 Rotation Annotations71.307 Text String Annotations264 Binary Segmentation Maps (Annotated)Strokes vs. BackgroundAccompanying Polygon Annotation Files20.060&nbsp;Polygon Annotations59 Object ClassesScripts for Data Loading, Statistics, Consistency Check and Training Preparation",https://data.4tu.nl/v3/datasets/a48923fe-2371-486d-80ba-5056fde13f3a.git,
b515d48b-cf8b-4bde-bd59-44c8b6a4ea6d,incompressiblenavierstokes,IncompressibleNavierStokes,,https://github.com/agdestein/IncompressibleNavierStokes.jl,agdestein/IncompressibleNavierStokes.jl
b539c338-2661-4882-869b-d0d9b21a12c8,sleplet,SLEPLET,"[SLEPLET](https://github.com/astro-informatics/sleplet) is a Python package for the construction of Slepian wavelets in the spherical and manifold (via meshes) settings. In contrast to other codes, `SLEPLET` handles any spherical region as well as the general manifold setting. The API is documented and easily extendible, designed in an object-orientated manner. Upon installation, `SLEPLET` comes with two command line interfaces - `sphere` and `mesh` - that allow one to easily generate plots on the sphere and a set of meshes using [plotly](https://github.com/plotly/plotly.py). Whilst these scripts are the primary intended use, `SLEPLET` may be used directly to generate the Slepian coefficients in the spherical/manifold setting and use methods to convert these into real space for visualisation or other intended purposes. The construction of the sifting convolution was required to create Slepian wavelets. As a result, there are also many examples of functions on the sphere in harmonic space (rather than Slepian) that were used to demonstrate its effectiveness. `SLEPLET` has been used in the development of several papers.  ## Background  Wavelets are widely used in various disciplines to analyse signals both in space and scale. Whilst many fields measure data on manifolds (i.e. the sphere), often data are only observed on a partial region of the manifold. Wavelets are a typical approach to data of this form, but the wavelet coefficients that overlap with the boundary become contaminated and must be removed for accurate analysis. Another approach is to estimate the region of missing data and to use existing whole-manifold methods for analysis. However, both approaches introduce uncertainty into any analysis. Slepian wavelets enable one to work directly with only the data present, thus avoiding the problems discussed above. Applications of Slepian wavelets to areas of research measuring data on the partial sphere include gravitational/magnetic fields in geodesy, ground-based measurements in astronomy, measurements of whole-planet properties in planetary science, geomagnetism of the Earth, and cosmic microwave background analyses.  ## Statement of Need  Many fields in science and engineering measure data that inherently live on non-Euclidean geometries, such as the sphere. Techniques developed in the Euclidean setting must be extended to other geometries. Due to recent interest in geometric deep learning, analogues of Euclidean techniques must also handle general manifolds or graphs. Often, data are only observed over partial regions of manifolds, and thus standard whole-manifold techniques may not yield accurate predictions. Slepian wavelets are designed for datasets like these. Slepian wavelets are built upon the eigenfunctions of the Slepian concentration problem of the manifold: a set of bandlimited functions that are maximally concentrated within a given region. Wavelets are constructed through a tiling of the Slepian harmonic line by leveraging the existing scale-discretised framework. Whilst these wavelets were inspired by spherical datasets, like in cosmology, the wavelet construction may be utilised for manifold or graph data.  To the author's knowledge, there is no public software that allows one to compute Slepian wavelets (or a similar approach) on the sphere or general manifolds/meshes. [SHTools](https://github.com/SHTOOLS/SHTOOLS) is a `Python` code used for spherical harmonic transforms, which allows one to compute the Slepian functions of the spherical polar cap. A series of `MATLAB` scripts exist in [slepian_alpha](https://github.com/csdms-contrib/slepian_alpha), which permits the calculation of the Slepian functions on the sphere. However, these scripts are very specialised and hard to generalise.  Whilst Slepian wavelets may be trivially computed from a set of Slepian functions, the computation of the spherical Slepian functions themselves are computationally complex, where the matrix scales as ð’ª(ð¿â´). Although symmetries of this matrix and the spherical harmonics have been exploited, filling in this matrix is inherently slow due to the many integrals performed. The matrix is filled in parallel in `Python` using [concurrent.futures](https://docs.python.org/3/library/concurrent.futures.html), and the spherical harmonic transforms are computed in `C` using [SSHT](https://github.com/astro-informatics/ssht). This may be sped up further by utilising the new [ducc0](https://github.com/mreineck/ducc) backend for `SSHT`, which may allow for a multithreaded solution. Ultimately, the eigenproblem must be solved to compute the Slepian functions, requiring sophisticated algorithms to balance speed and accuracy. Therefore, to work with high-resolution data such as these, one requires high-performance computing methods on supercomputers with massive memory and storage. To this end, Slepian wavelets may be exploited at present at low resolutions, but further work is required for them to be fully scalable.  ## Example Usage  `SLEPLET` may be interacted with via the API or the CLIs.  ### API Usage  The following demonstrates the first wavelet (ignoring the scaling function) of the South America region on the sphere.  ```python import sleplet  B, J, J_MIN, L = 3, 0, 2, 128  region = sleplet.slepian.Region(mask_name=""south_america"") f = sleplet.functions.SlepianWavelets(L, region=region, B=B, j_min=J_MIN, j=J) f_sphere = sleplet.slepian_methods.slepian_inverse(f.coefficients, f.L, f.slepian) sleplet.plotting.PlotSphere(     f_sphere,     f.L,     f""slepian_wavelets_south_america_{B}B_{J_MIN}jmin_{J_MIN+J}j_L{L}"",     normalise=False,     region=f.region, ).execute() ```  ![Slepian Wavelet j=2](https://github.com/astro-informatics/sleplet/blob/main/documentation/slepian_wavelets_south_america_3B_2jmin_2j_L128_res512_real.png?raw=true)  ### CLI Usage  The demonstrates the first wavelet (ignoring the scaling function) of the head region of a Homer Simpson mesh for a per-vertex normals field.  ```sh mesh homer -e 3 2 0 -m slepian_wavelet_coefficients -u -z ```  ![Slepian Mesh Wavelet Coefficients j=2](https://github.com/astro-informatics/sleplet/blob/main/documentation/slepian_wavelet_coefficients_homer_3B_2jmin_2j_zoom.png?raw=true)  ## Citing  If you use `SLEPLET` in your research, please cite the paper.  ```bibtex @article{Roddy2023,   title   = {%raw%}{{SLEPLET: Slepian Scale-Discretised Wavelets in Python}}{%endraw%},   author  = {Roddy, Patrick J.},   year    = 2023,   journal = {Journal of Open Source Software},   volume  = 8,   number  = 84,   pages   = 5221,   doi     = {10.21105/joss.05221}, } ```  Please also cite [S2LET](https://doi.org/10.1051/0004-6361/201220729) upon which `SLEPLET` is built, along with [SSHT](https://doi.org/10.1109/TSP.2011.2166394) in the spherical setting or [libigl](https://doi.org/10.1145/3134472.3134497) in the mesh setting.  ## Acknowledgements  The author would like to thank Jason D. McEwen for his advice and guidance on the mathematics behind `SLEPLET`. Further, the author would like to thank Zubair Khalid for providing his `MATLAB` implementation to compute the Slepian functions of a polar cap region, as well as the formulation for a limited colatitude-longitude region. `SLEPLET` makes use of several libraries the author would like to acknowledge, in particular, [libigl](https://github.com/libigl/libigl-python-bindings), [NumPy](https://github.com/numpy/numpy), `plotly`, `SSHT`, [S2LET](https://github.com/astro-informatics/s2let).",https://github.com/astro-informatics/sleplet,astro-informatics/sleplet
b54bea33-c7bf-4036-8805-8648fe7ac445,coeusai,CoeusAI,"Our QGIS plugin, CoeusAI, is designed for exploration of multiband geospatial datasets. It lets the user iteratively train and retrain classification models in seconds. Traditional machine learning approaches can lack intelligent predictions, while deep neural nets typically require vast amounts of training data and compute. Our tool combines a pretrained UNet for feature extraction with a RandomForest for classification. We therefore leverage strong points of both. The use of a UNet makes it possible to take into account complex patterns and encode these in a feature space. The RandomForest is then trained on top of those features with little labeled data, and only seconds (or minutes) of compute time required. This allows for an interactive and explorative user experience.  CoeusAI uses the functionalities of the command-line-tool and Python package, Pycoeus, it's main dependency, and makes them available with a user-friendly graphical user-interface to use in QGIS.",https://github.com/DroneML/CoeusAI,DroneML/CoeusAI
b58941ea-5882-4200-bff3-42af1a391e94,4tu-software-package-c1-belonging-to-the-paper-cold-pool-dynamics-shape-the-response-of-extreme-rainfall-events-to-climate-change,Software package C1 belonging to the paper: Cold pool dynamics shape the response of extreme rainfall events to climate change,"This software package, named celltrack, is used to cluster rain cells, cold pools and moist patches from the model output. A user documentation with instructions how to build and apply the software can be found in the doc folder: doc/celltrack_doc.pdf",,
b672697e-95c4-4707-83d5-a8c51ced25a8,4tu-etsi-qkd014-client, ETSI-QKD014-client,Quantum Key Distribution (QKD) hardware provides cryptographic keys that can be used to secure confidential data. This software allows to request keys exchanged using QKD from QKD hardware or a Key Management System by following the ETSI GS QKD 014 standard. It is then up to the user of this library to use these keys to encrypt data. This software can be used as a command-line program or as a library from either Rust or C code.,https://data.4tu.nl/v3/datasets/5b384f2e-4083-4dd4-b9d5-c85f1661d221.git,
b6d26ebd-3f39-451e-b9d9-92a2f3249297,4tu-mangrove-ch-agb-estimation,Mangrove_CH_AGB_estimation,"The dataset covers two long-term mangrove restoration areas in Bali Province, Indonesia, identified as Budeng-Perancak (BP) and Tahura Ngurah Rai (TA). Field observation data of canopy height (CH) and aboveground biomass (AGB) were gathered in 2024 through plot-level data collection and UAV-LiDAR flight campaigns.      Data 1. Plot-level field observations of canopy height and aboveground biomass vector dataset  Data 2. Dataset of plot-level UAV-LiDAR point clouds in Budeng-Perancak mangrove restoration area   Data 3. 15-m spatial resolution reference raster dataset of canopy height and aboveground biomass     The R code provides the regression machine learning model development for estimating canopy height and aboveground biomass using multisource remote sensing approach.",https://data.4tu.nl/v3/datasets/733f6e4f-f08a-44c7-a329-c9b66739454c.git,
b6d2c020-6845-4cfd-ade9-c5d4d8a33fe2,4tu-colorice-automated-in-plane-ice-thin-section-analysis-tool,ColorIce: Automated in-plane ice thin section analysis tool,"The ColorIce tool contains MATLAB scripts which accept a sequence of photographs of an ice thin section rotated between crossed-polarizers and 1) registers the images, 2) segments the grain boundaries, and 3) identifies the c-axis orientation of each grain in the images.       Â A full description of the methodology and application can be found here: Â https://doi.org/10.1016/j.coldregions.2022.103735. Â For questions about the tool, please contact Cody C. Owen (c.c.owen@tudelft.nl).",,
b774b0df-1bb4-4e2a-8748-2b237b5f4333,4tu-free-vortex-wake-with-discrete-adjoint-model-code,Free-Vortex Wake with Discrete Adjoint - Model Code,"Model code developed for the paper ""Adjoint Â Optimisation for Wind Farm Flow Control with a Free-Vortex Wake Model"", submitted to Renewable Energy.",,
b7c9ff67-dc8e-4fa2-a6e4-07ad321f2280,image-land-lue,IMAGE-LAND-LUE,,https://github.com/UtrechtUniversity/IMAGE-LAND-LUE,UtrechtUniversity/IMAGE-LAND-LUE
b8680d8a-b946-4d5a-8fc9-1c86184c4824,3d-e-chem-knime-pharmacophore,KNIME Pharmacophore,"* For cheminformations who are working with pharmacphores in KNIME workflows. * Adds pharmacophore data type to KNIME * Adds nodes to read, write and align pharmacophores   A pharmacophore is an abstract description of molecular features that are necessary for molecular recognition of a ligand by a biological macromolecule. This plugin for the KNIME workflow system adds the Pharmacophore (Phar) data type to KNIME, and adds nodes to read, write, and manipulate pharmacophores inside KNIME. It includes the [KNIME Silicos-it](https://www.research-software.nl/software/3d-e-chem-knime-silicos-it), [Kripo pharmacophore retrieval](https://www.research-software.nl/software/3d-e-chem-knime-kripodb) and [molviewer pharmacophore viewer](https://www.research-software.nl/software/knime-molviewer) nodes.",https://github.com/3D-e-Chem/knime-pharmacophore,3D-e-Chem/knime-pharmacophore
b87af0a2-d75d-4741-9e22-e2bc6aec2ad9,jasp-modules-app,JASP modules app,JASP (jasp-stats.org) is a free open-source statistics program with an intuitive graphical user interface that is used by students and researchers at hundreds of universities worldwide. JASP functionality is grouped into modules. The JASP modules app allows a user to see which modules are available and install or update them.,https://github.com/jasp-stats-modules/modules-app,jasp-stats-modules/modules-app
b98a11e5-75ef-4aff-aaa3-591a5860b9e1,openchrom,OpenChrom,"Data from different systems can be imported and analyzed. It runs under Windows, macOS and Linux. It started as a ChemStation alternative and therefore has a strong focus on chromatography and nominal mass spectrometry. Its main strength is to handle GC/MS and GC/FID measurements. Methods for peak detection, integration, identification, quantitation and reporting are supported. Using internal (ISTD) and external standards (ESTD) for quantitation purposes is supported as well. Additional filter help to optimize the measurements and classifier calculate key values of the chromatographic data and help to point out problems like shifted retention times or degraded columns. The modular design allows extending and recombining features for different purposes.  The base version is free for everyone. Commercial support is available as well as paid extensions that improve the workflow or add additional features. Students who register with a university mail can apply for discounts.",https://github.com/openchrom/openchrom,openchrom/openchrom
b997aae8-5e9b-4ad1-aae5-4e75dc06de2d,4tu-sam-java-and-matlab-code-for-running-and-analysis-the-experiments-in-the-phd-thesis-self-organizing-multi-agent-systems,SAM Java and Matlab code for running and analysis the experiments in the  PhD Thesis: Self-Organizing Multi-Agent Systems,"This archive contains a snapshot of the git repository containing the code for running and analysis of the experiments in the dissertation titled ""Self-Organizing Multi-Agent Systems"" by C.J. van Leeuwen. The original repositories are available at https://github.com/coenvl/jSAM and https://github.com/coenvl/mSAM",,
b9e83d0f-abc3-4f0c-9710-c5877f583099,4tu-code-supporting-the-paper-a-bayesian-finite-element-trained-machine-learning-approach-for-predicting-post-burn-contraction,Code supporting the paper: A Bayesian finite-element trained machine learning approach for predicting post-burn contraction,"This online resource shows two archived folders: Matlab and Python, that contain relevant code for the article: A Bayesian finite-element trained machine learning approach for predicting post-burn contraction.Â        One finds the codes used to generate the large dataset within the Matlab folder. Here, the file Main.m is the main file and from there, one can run the Monte Carlo simulation. There is a README file.       Within the Python folder, one finds the codes used for training the neural networks and creating the online application. The file Data.mat contains the data generated by the Matlab Monte Carlo simulation. The files run_bound.py, run_rsa.py, and run_tse.py train the neural networks, of which the best scoring ones are saved in the folder Training. The DashApp folder contains the code for the creation of the Application.",,
ba192940-b236-48ad-af61-b71b14924bad,4tu-data-and-optimisation-model-for-space-heating-and-committed-emissions-for-the-built-environment,Data and Optimisation model for space heating and committed emissions for the built environment,"This dataset is used to arrive to the results presented in the paper `Reducing committed emissions of heating towards 2050: Analysis of scenarios for the insulation of buildings and the decarbonisation of electricity generation' from Kaandorp et al. (2022).&nbsp;The dataset consists of a Python code together with the input data used to run the code. The code is used to compute which technology mix is to be applied in a neighbourhood to optimally minimise the carbon emissions associated with space heating between 2030 and 2050. The neighbourhoods used in this study are 'Felix Meritis', 'Prinses Irenebuurt', and 'Molenwijk'. The model is run for scenarios which represents different rates of the insulation of buildings and the decarbonisation of electricity production between 2020 and 2050.    The python code requires the following data files (provided in this collection):  -&nbsp;Address_Neigborhood_Heat_Demand.xlsx  - Heat_technology.xlsx (or Heat_teachnology_highEFhydrogen.xlsx to run change the input of the emission factors related to hydrogen).  - Scenario_Settings.xlsx    The data file 'Scenario_Setting.xlsx' is used for a first-order sensitivity analysis).    The code in 'post_processing.py' is used to process the output data from 'Address_Gurobi_scenario_loop_5y_timestep.py' (in this dataset) to facilitate analysis.&nbsp;",,
babc10b4-d832-4c26-9dd2-8427d48bd1fe,explainable-embeddings,Explainable embeddings,* Experiments to tune our explainable AI method in DIANNA.   Our method explains what parts of a data item causes it to come closer to or move further from some other data item in the same embedded space.,https://github.com/dianna-ai/explainable_embedding,dianna-ai/explainable_embedding
bac93316-068a-42f8-b154-6eb2894c646c,kmm,KMM,,https://github.com/NLeSC-COMPAS/kmm,NLeSC-COMPAS/kmm
baea78c5-f9fe-4682-aec4-ed3a50b4574a,democracy-scraping-tool,Democracy Scraping Tool,,https://github.com/backdem/scrape-tool,backdem/scrape-tool
bb3f2c02-d20b-4082-bd4b-f26937b040e8,twinl-website-code,Twiqs,"* search facility for tweets for users interested in tweet statistics * implements webserver and communication with Hadoop server with tweets * access to tweet statistics (more detailed than Twitter) * dozens of research papers and at least 1 PhD thesis used this service  Many researchers want to analyse tweets but find it to collect large numbers of messages and analyze these. The website twiqs.nl was created to support research based on Dutch tweets. The websites offers a search function that can be applied to tweets dating back to 2010. Twitter does not allow the text of tweets to be offered as a download.  Instead the website offers summaries of the tweets in graphs, on maps, word clouds and pie diagrams. Since it creation in 2012, the website has been used by many researchers and common people to test ideas about word usage in Dutch tweets.  This software package contains the software used by the website twiqs.nl.",https://github.com/twinl/website,twinl/website
bb6ff86b-f222-481b-a8a4-5146f7b2c9c9,sv-callers,sv-callers,"* Enables comprehensive detection of structural variants (SVs) by combining multiple tools * Supports both single-sample (germline) and paired-sample (somatic) SV analyses * Makes the analyses scalable and easily portable across HPC clusters (e.g., using GridEngine, Slurm etc.) or compute clouds * It's easy to use, deploy and extend with new tools  This [Snakemake](https://snakemake.readthedocs.io/)-based workflow combines several state-of-the-art tools (i.e. [Manta](https://github.com/Illumina/manta), [DELLY](https://github.com/dellytools/delly), [LUMPY](https://github.com/arq5x/lumpy-sv) and [GRIDSS](https://github.com/PapenfussLab/gridss)) for detecting structural variants (SVs) in whole genome sequencing data. The workflow is easy to use and to deploy on any Linux-based machine. In particular, the workflow supports automated software deployment, easy configuration and addition of new analysis tools as well as enables to scale from a single computer to different HPC clusters with minimal effort.",https://github.com/GooglingTheCancerGenome/sv-callers,GooglingTheCancerGenome/sv-callers
bb993a1a-24c2-485a-9fdf-121f87252b2a,4tu-sensession,sensession,"# Data and code for ""Same Signal, Different Story:  Demystifying Receiver Effects in Wi-Fi Channel State Information""     The dataset(s) here were collected with a Wi-Fi testbed of eight different receivers:     2x Asus RT-AC86U2x ESP32-S31x Intel IWL53001x Intel AX2101x Qualcomm Atheros QCA AR94621x USRP N2954-R   The transmitter for all the datasets is another USRP N2954-R with Wi-Fi frames generated using the MATLAB WLan Toolbox.     All data was collected using our python scripts, published on GitHub: https://github.com/nzqo/sensession.  Relevant subprojects for CSI extraction are found in `csi_tools` in sensession.  You can find the experiment design (e.g. frame precoding) by inspecting the `/experiments` directory.  Evaluation scripts of the data are also shared in the `/evaluation` directory.     ## Format     All data is stored as `.parquet` files, which is tabular data. Each line contains a single CSI recording. The `meta_id` column marks a group (a coherent recording), with further information linked in the corresponding `meta.parquet` files. These files can be easily loaded into dataframes using e.g. pandas or polars.     Every CSI value is represented as a nested list corresponding to shape `[num_antennas, num_streams, num_subcarrier]`.     ## HAR dataset     The HAR dataset comes with a dedicated README with more information on the contents.     ## Over-the-wire data     For the OTW data, all devices are connected to the transmitter using cables of equal length, feeding the signal through a waveguide powersplitter (ZN8PD1-63W-S+). Archive names describe the measurements we ran (e.g. single subcarrier modifications).",https://data.4tu.nl/v3/datasets/786f73fa-0073-4af8-826b-b3685a4c5ac6.git,
bbb61b0b-a5f7-4775-9767-3d910a71b8ae,kernel-float,Kernel Float,"![Kernel Float logo](https://raw.githubusercontent.com/KernelTuner/kernel_float/main/docs/logo.png)  CUDA natively offers several reduced precision floating-point types (`__half`, `__nv_bfloat16`, `__nv_fp8_e4m3`, `__nv_fp8_e5m2`) and vector types (e.g., `__half2`, `__nv_fp8x4_e4m3`, `float3`). However, working with these types is cumbersome: mathematical operations require intrinsics (e.g., `__hadd2` performs addition for `__half2`), type conversion is awkward (e.g., `__nv_cvt_halfraw2_to_fp8x2` converts float16 to float8), and some functionality is missing (e.g., one cannot convert a `__half` to `__nv_bfloat16`).  _Kernel Float_ resolves this by offering a single data type `kernel_float::vec<T, N>` that stores `N` elements of type `T`. Internally, the data is stored as a fixed-sized array of elements. Operator overloading (like `+`, `*`, `&&`) has been implemented such that the most optimal intrinsic for the available types is selected automatically. Many mathetical functions (like `log`, `exp`, `sin`) and common operations (such as `sum`, `range`, `for_each`) are also available.  By using this library, developers can avoid the complexity of working with reduced precision floating-point types in CUDA and focus on their applications.",https://github.com/KernelTuner/kernel_float,KernelTuner/kernel_float
bbdb2755-13cd-4ec3-a266-213d0d51f360,4tu-python-scripts-used-in-the-study-towards-affordable-3d-physics-based-river-flow-rating-application-over-luangwa-river-basin,Python Scripts used in the study: Towards Affordable 3D Physics-Based River Flow Rating: Application Over Luangwa River Basin,The data contains python scripts which were used in the study to do the following:   1. Compare rating curves prodused based on different GCPs   2. Compare 3D hydraulic model rating curve with traditional rating curves   3. Produce water level/width vs discharge curves,,
bc02e83c-c892-40d1-8b2b-154d22117711,4tu-particle-filter-code-with-an-example-of-subsidence-caused-by-a-compacting-reservoir,Particle filter code with an example of subsidence caused by a compacting reservoir,"A compacting reservoir gives a subsidence field at the Earth's surface. The subsidence field has a varying smoothness given the reservoir compaction field, with, for example a very smooth subsidence bowl or a very spatially uncorrelated subsidence field.  In this code, we implement a particle filter in a static case to estimate model states and illustrate its implementation with two subsidence models providing varying smoothness of the subsidence field.",https://data.4tu.nl/v3/datasets/c8dcca01-5773-4162-b5fd-0c66feb1200e.git,
bc0c710a-d6f3-45de-827a-e662540ce4df,openda,OpenDA,"* Provides researchers with a tool for experimentation with data-assimilation/calibration methods without the need for extensive programming * Enables quick implementation of data-assimilation and calibration for arbitrary numerical models * No need to learn and implement many different Data Assimilation and calibration algorithms/methods * Successfully used in many applications and publications: e.g., eWaterCycle I & II, OpenFOAM, Clever Monitoring.   OpenDA is an open interface standard for (and free implementation of) a set of tools to quickly implement data-assimilation and calibration for arbitrary numerical models. OpenDA wants to stimulate the use of data-assimilation and calibration by lowering the implementation costs and enhancing the exchange of software among researchers and end-users. A model that conforms to the OpenDA standard can use all the tools that are available in OpenDA. This allows experimentation with data-assimilation/calibration methods without the need for extensive programming. Reversely, developers of data-assimilation/calibration software that make their implementations compatible with the OpenDA interface will make their new methods usable for all OpenDA users (either for free or on a commercial basis). OpenDA has been designed for high performance. Hence, even large-scale models can use it. Also, OpenDA allows users to optimize the interaction between their model and the data-assimilation/calibration methods. Hence, data-assimilation with OpenDA can be as efficient as with custom-made implementations of data-assimilation methods.",https://github.com/OpenDA-Association/OpenDA,OpenDA-Association/OpenDA
bc1b8d6b-8e83-43e2-9e16-49f47dbf6258,2022-brughmans-001,Network structures,"A draft model with some useful code for creating different network structures using the Netlogo NW extension: small-world, preferential attachment, circular, star, wheel, lattice, random, nearest neighbours. This model is used for the following tutorial: Brughmans, T. (2018). Network structures and assembling code in NetLogo, Tutorial, https://archaeologicalnetworks.wordpress.com/resources/#structures .  By Tom Brughmans First version: Summer 2018 This version created 01/09/2018 Netlogo version used: 6.0.1 Extension used: nw (pre-packaged with Netlogo 6.0.1) https://ccl.northwestern.edu/netlogo/6.0-BETA1/docs/nw.html   Tutorial document available as a PDF in the [netlogo_implementation folder](netlogo_implementation/Netlogo_network-structures_v0.1.pdf)  Cite this tutorial as: Brughmans, T. (2018). Network structures and assembling code in Netlogo, Tutorial, https://archaeologicalnetworks.wordpress.com/resources/#structures  .  This tutorial provides an introduction to finding and assembling pre-existing code to quickly create complex models. It uses code and data linked to in the https://projectmercury.eu pages. We will create a Roman transport network by reusing existing code that draws on the open access ORBIS dataset (http://orbis.stanford.edu/), we will create alternative network structures by reusing existing code, and we will explore the impact these different network structures have in light of simple economic processes. This tutorial will reveal the importance of not reinventing the wheel, of searching for appropriate existing code and letting your model building be inspired by othersâ€™ previous work.  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |new-network-structure|string|The type of network structure you want to create, either small-world, preferential attachment, circular, star, wheel, lattice, random, nearest neighbours| |nodes|integer|The number of nodes in the network that will be created.| |probability|float|Value between 0 and 1, the probability with which a link will be created in the small-world and random network structures.| |nearest-neighbours|integer|The number of nearest nodes that a node will be connected to in the ""nearest neighbours"" network structure| |existing-network-structure|string|When keeping the nodes but creating new links between them (same-nodes/new-links), the type of network structure you will follow to create those links. Either nearest neighbour, random, star, circular.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |Network topology|network|The set of nodes and links created using the network creation algorithm selected.| |average-degree|float|the average number of links a node has| |clustering-coefficient|float|the proportion of closed triangles over all triads in the network; the degree of local clustering in the network.| |Av. shortest path length|float|the mean path length in the network; the mean number of steps to connect one node with another node| |degree distribution|frequency distribution|The frequency distribution of the number of links a node has (its degree)| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Brughmans-001,
bc60fc1e-06ae-465c-9ebd-8b2f86a61d3d,icos-data-repository,ICOS data repository,"A complete repository requires two separate services, meta and data.  Please also read https://github.com/ICOS-Carbon-Portal/data#information-for-developers",https://github.com/ICOS-Carbon-Portal/meta,ICOS-Carbon-Portal/meta
bcd4da49-2d1f-42ac-b352-6c44604f276f,4tu-chi2023-dke,CHI2023_DKE,"This repo contains all code and data associated with the paper ""Knowing About Knowing: An Illusion of Human Competence Can Hinder Appropriate Reliance on AI Systems."" In our study, we conducted a tutorial to mitigate the Dunning-Kruger effect. Our task is logical question answering. We provide statistical analysis to show the impact of tutorial interventions across two batches of tasks.",https://data.4tu.nl/v3/datasets/bb173377-8b33-434b-b005-0508490919cb.git,
bcf95ed4-800c-46c0-8d2e-e7a0dff96ba6,citation-js,citation.js,"Citation.js (GitHub, NPM) converts formats like BibTeX, BibJSON, DOI, and Wikidata to CSL-JSON to convert to styles like APA, Vancouver and to BibTeX and RIS.  ## Features  - Modular approach: select which formats to include - Convenient wrapper some of the most common modules - Preloaded styles and locales for formatted references - Support for Node.js and the browser - A command-line interface (CLI) - MIT licensed  ## Supported identifiers  - DOI - ISBN - PubMed - PubMed Central - Wikidata - GitHub repositories - npm packages  ## Supported file formats  - CSL-JSON - BibTeX, BibLaTeX - EndNote .enw files - Quickstatements V1 (output only) - RIS - refer - RefWorks - CFF - Wikipedia ""Citation Style 1"" templates",https://github.com/citation-js/citation-js,citation-js/citation-js
bd260a18-4dd4-4381-b009-c782f532c881,nplinker-escience-workshop-genmetmine-materials,NPLinker eScience Workshop GenMetMine Materials,"We hope that this initiative will help to further foster teaching the future generation of omics miners, to aid in setting up teaching materials for genome mining, metabolomics, and metabolome mining, and to promote discussions in between and bring together the fields of genome mining and metabolome mining.  Most materials are available in a format facilitating its reuse. When doing so, we kindly ask you to - where possible and appropriate - acknowledge and credit those that put effort in creating the slides and developing the methods. You will find information in the slide decks to do so. Please note that the materials also have a DOI through this Zenodo link that can be used to credit the workshop material. Thanks in advance on behalf of the instructors and organizers of the workshop.  Whilst natural products discovery was a prime application area for the workshop, we do note that both genome mining and especially metabolome mining do have other application areas for which - sometimes with some modifications - the materials could be reused.",https://doi.org/10.5281/zenodo.7801713,
bd33ea51-8331-4e97-b0dc-1fff5437a38f,4tu-code-underlying-the-publication-exploiting-learned-symmetries-in-group-equivariant-convolutions,Code underlying the publication: Exploiting Learned Symmetries in Group Equivariant Convolutions,"Code corresponding to ICIP 2021 submission ""Exploiting Learned Symmetries in Group Equivariant Convolutions"".     Abstract  Group Equivariant Convolutions (GConvs) enable convolutional neural networks to be equivariant to various transformation groups, but at an additional parameter and compute cost. We investigate the filter parameters learned by GConvs and find certain conditions under which they become highly redundant. We show that GConvs can be efficiently decomposed into depthwise separable convolutions while preserving equivariance properties and demonstrate improved performance and data efficiency on two datasets.",https://data.4tu.nl/v3/datasets/fb66f3b9-5eef-4af6-a0b8-8ed1e63a6f0f.git,
bd6cc297-d802-4ddb-8e3d-4edafa49ffa8,xarray-regrid,xarray-regrid,"xarray-regrid can regrid (geospatial) data to a new coordinate system.  Features:  - Linear, nearest-neighbor, cubic, conservative regridding methods.  - Fully implemented in Python, no conda or binaries required.  - Both lazy and parallelized computation with Dask,.  - Benchmarks to validate the results against standard methods are available.",https://github.com/xarray-contrib/xarray-regrid/,xarray-contrib/xarray-regrid
bd7ba5a1-5172-4e9b-b23b-52f8bacdbcfb,eossr,eOSSR ,"![eossr_logo](https://gitlab.in2p3.fr/escape2020/wp3/eossr/-/raw/eabcda5f58f33f4b01d600a54685ce581c478b3b/docs/images/eossr_logo_200x100.png)   # The ESCAPE OSSR library  The eOSSR is the Python library to programmatically manage the ESCAPE OSSR. In particular, it includes:  - an API to access the OSSR, retrieve records and publish content - functions to map and crosswalk metadata between the CodeMeta schema adopted for the OSSR and Zenodo internal schema - functions to help developers automatically contribute to the OSSR, in particular using their continuous integration (see also code snippets)  Code: [https://gitlab.in2p3.fr/escape2020/wp3/eossr](https://gitlab.in2p3.fr/escape2020/wp3/eossr) Documentation: [https://escape2020.pages.in2p3.fr/wp3/eossr/](https://escape2020.pages.in2p3.fr/wp3/eossr/)  [![pipeline_badge](https://gitlab.in2p3.fr/escape2020/wp3/eossr/badges/master/pipeline.svg)]( https://gitlab.in2p3.fr/escape2020/wp3/eossr/-/commits/master) [![coverage_badge](https://gitlab.in2p3.fr/escape2020/wp3/eossr/badges/master/coverage.svg)]( https://gitlab.in2p3.fr/escape2020/wp3/eossr/-/commits/master) [![CII Best Practices](https://bestpractices.coreinfrastructure.org/projects/5712/badge)](https://bestpractices.coreinfrastructure.org/projects/5712) [![SQAaaS badge shields.io](https://img.shields.io/badge/sqaaas%20software-silver-lightgrey)](https://api.eu.badgr.io/public/assertions/aiB2ndZOSL6IuVTOmljRCw ""SQAaaS silver badge achieved"") [![MIT_license_badge](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT) [![zenodo_badge](https://zenodo.org/badge/DOI/10.5281/zenodo.5524912.svg)](https://doi.org/10.5281/zenodo.5524912) [![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fescape2020%2Fwp3%2Feossr/HEAD?labpath=examples%2Fnotebooks%2Fossr_api-Explore_the_OSSR.ipynb)  ## Former stable versions  - v1.0: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7544514.svg)](https://doi.org/10.5281/zenodo.7544514) - v0.6: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6475946.svg)](https://doi.org/10.5281/zenodo.6475946) - v0.5: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6352039.svg)](https://doi.org/10.5281/zenodo.6352039) - v0.4: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.6326454.svg)](https://doi.org/10.5281/zenodo.6326454) - v0.3.3: [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5592584.svg)](https://doi.org/10.5281/zenodo.5592584) - v0.2 : [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.5524913.svg)](https://doi.org/10.5281/zenodo.5524913)  ## Install  Commands to be run in your terminal.  ### For users  ```bash cd eossr pip install . ```  You can also run it with docker:  ```bash docker run -it gitlab-registry.in2p3.fr/escape2020/wp3/eossr:latest ```  [Visit our registry](https://gitlab.in2p3.fr/escape2020/wp3/eossr/container_registry) to see the available docker containers.  Note that `latest` tag always point to the latest stable released container.  ### For developers  ```bash git clone https://gitlab.in2p3.fr/escape2020/wp3/eossr.git cd eossr pip install -e . ```  #### Running tests  To run tests locally, run:  ```bash pip install -e "".[tests]""  pytest eossr ```  Some tests will be skiped if `SANDBOX_ZENODO_TOKEN` is not defined in your environment variables. If you want to run these tests, you will need to create a [sandbox zenodo token](https://sandbox.zenodo.org/account/settings/applications/tokens/new/) and add it to your env:  ```bash export SANDBOX_ZENODO_TOKEN=""your_sandbox_token"" ```  ## Online CodeMeta validator for the OSSR  The eOSSR powers an online validator for your CodeMeta metadata and to convert it to Zenodo metadata.  [Just follow this link (running on mybinder)](https://mybinder.org/v2/git/https%3A%2F%2Fgitlab.in2p3.fr%2Fescape2020%2Fwp3%2Feossr/HEAD?urlpath=voila%2Frender%2Fdocs%2Fmetadata%2Fvalidate_codemeta.ipynb)  ## License  See [LICENSE](LICENSE)  ## Cite  To cite this library, please cite our ADASS proceedings:  ```latex @misc{https://doi.org/10.48550/arxiv.2212.00499,   doi = {10.48550/ARXIV.2212.00499},   url = {https://arxiv.org/abs/2212.00499},   author = {Vuillaume, Thomas and Garcia, Enrique and Tacke, Christian and Gal, Tamas},   keywords = {Instrumentation and Methods for Astrophysics (astro-ph.IM), FOS: Physical sciences, FOS: Physical sciences},   title = {The eOSSR library},   publisher = {arXiv},   year = {2022},   copyright = {arXiv.org perpetual, non-exclusive license} } ```  If you used the library in a workflow, please cite the version used as well, using the cite section in [the Zenodo page](https://zenodo.org/record/5592584#.YiALJRPMI-Q) (right column, below the `Versions` section).",https://gitlab.com/escape-ossr/eossr,
bd9af85e-969d-4793-bfca-413e5b3e887b,4tu-oraqle,oraqle,"This dataset does not contain data, only code.     The oraqle compiler lets you generate arithmetic circuits from high-level Python code. It also lets you generate code using HElib.     This repository uses a fork of fhegen as a dependency and adapts some of the code from [fhegen](https://github.com/Crypto-TII/fhegen), which was written by Johannes Mono, Chiara Marcolla, Georg Land, Tim GÃ¼neysu, and Najwa Aaraj.     Setting up  The best way to get things up and running is using a virtual environment:  - Set up a virtualenv using `python3 -m venv venv` in the directory.  - Enter the virtual environment using `source venv/bin/activate`.  - Install the requirements using `pip install requirements.txt`.  - *To overcome import problems*, run `pip install -e .`, which will create links to your files (so you do not need to re-install after every change).",https://data.4tu.nl/v3/datasets/e6194f77-0677-49d1-ae81-a725ca0a001e.git,
bd9d7d2b-1c82-4474-b9c6-f6d7ba20e3d3,4tu-mpcrl-for-ramp-metering,mpcrl-for-ramp-metering,"Source code for the implementation and simulation of a learning-based ramp metering control strategy with the goal of improving highway traffic flow management, where the proposed solution embeds model-based Reinforcement Learning methodologies in a Model Predictive Control framework, thus enabling the adaptation of the controller in order to improve automatically its performance based solely on observed closed-loop data. Simulations on a highway network benchmark demonstrate significant reduction in congestion and improved constraint satisfaction compared to an imprecise, non-learning initial controller, showcasing the efficacy of the proposed methodology.",https://data.4tu.nl/v3/datasets/f0c64b06-996a-4e02-a117-7f1c701decc3.git,
bdc2347e-f0a7-4465-b526-73cbd9271c01,4tu-trained-cnn-model-to-predict-performance-of-an-lpm-thruster-from-the-32-x-32-px-image-of-its-microchannel-cross-section,Trained CNN model to predict performance of an LPM thruster from the 32 x 32 px image of its microchannel cross-section,"Contains the trained CNN model that can predict performance parameters of an LPM thuster given the cross-sectional image (32 x 32 px) of the microchannel for the given conditions: Argon propellant, inlet pressure = 50 Pa, inlet temperature = 300 K, and heaterchip temperature = 600 K.",,
be7f5b7c-273a-4f94-842a-7d7f4edead20,4tu-brazilclim-script-to-gauge-calibrate-the-surfaces,BrazilClim: script to gauge-calibrate the surfaces,"A script that produces the monthly surfaces for each one of the covariates required to create the bioclimatic dataset: maximum and minimum temperatures, as well as precipitation. These surfaces are created applying several interpolation techniques, based on climatic information measured on-field (Supplementary Material A; DOI: 10.4121/14932947) and surfaces from GMTED2010 (digital elevation model), TRMM 3B43 v7 and NOAA (that must be downloaded from the respective sources). It also uses elevation and latitudes in the case of temperatures. Outputs are generated in GeoTiff format and 30 arc-sec resolution.",,
be9383c1-4531-4857-8ce9-36a92fc700ed,entangled,Entangled,"* Create **live documents** in Markdown * Program in **any language** you like * Use your **favourite editor** * Works well with **version control** * Powered by **Pandoc**  Wikipedia on literate programming: Literate programming is a programming paradigm introduced by Donald Knuth in which a program is given as an explanation of the program logic in a natural language, such as English, interspersed with snippets of macros and traditional source code, from which a compilable source code can be generated.  Literate programming is a method of developing software that is well suited for writing software manuals and library tutorials. More importantly, scientific literature can come to life by including samples of code that implement the topic under discussion. Literate programming takes this to the extreme: the manual, article or book becomes an executable piece of literature.  Entangled enables a method of literate programming in Markdown that is completely agnostic to the underlying programming language. The Markdown can be converted to HTML or LaTeX based targets using Pandoc, the universal document converter.",https://github.com/entangled/entangled.py,entangled/entangled.py
bee299ab-6f00-4344-8b10-ffd2a6763b36,motrainer,MOTrainer,"# MOTrainer: Measurement Operator Trainer Measurement Operator Trainer is a Python package training measurement operators (MO) for data assimilations.   Currently, the MOTrainer trains Deep Neuron Networks as measurement operators for Earth Observation (EO) applications. The trained predicts sensor measurement based on certain physical model states.  MOTrainer assumes the availability of the training dataset, i.e. input (model status) and output (sensor data).",https://github.com/VegeWaterDynamics/motrainer,VegeWaterDynamics/motrainer
bf5cc68c-a001-42ae-a8f3-88de8ca6d2c2,4tu-microdrill-rgp-file-processing-version-1,MicroDrill (.rgp) file processing / Version 1,"The purpose of these codes is to remove the noise and correct for shaft friction from the micro-drill measurements collected by resistograph in .rpd format. In the context of the research project (City of Amsterdam, Raamwerk palen Adam, project code: CS2B07, year: 2021) it was used on the provided data collected with IML-RESI PowerDrill on multiple wooden piles, which support bridge foundations in Amsterdam.",,
bf6cd417-7bd1-40f6-b8de-bcaa885d2264,4tu-script-to-register-figshare-dois-in-datacite,Script to register Figshare dois in Datacite,"Python script to register 4TU.ResearchData dois from Figshare in Datacite.   The script was originally made to run ad-hoc on a local computer and was later adapted to also run daily on a server. Apart from the daily update, there are options to registrate specific dois, e.g. based on a query.   The metadata sent to datacite contains custom metadata that are specific for 4TU.ResearchData.",,
bf705fd2-dfa8-44f3-b2d2-593824d23507,m4ma,m4ma,"An R package containing C++ implementations to speed up the simulation and parameter estimation of the Predictive Pedestrian model.  This package is currently not self-contained but should be used in combination with code from the [predped](https://github.com/CharlotteTanis/predped) repository. The m4ma package includes C++ implementations that can be used instead of R code from predped. The functions in m4ma have in most cases the same names as functions in predped, so that they can be easily substituted. Exceptions are functions to estimate parameters and likelihoods. Benchmarks that show the speed improvement of m4ma implementations compared to predped can be found [here](https://github.com/m4ma/m4ma-performance/tree/main/bench).",https://github.com/m4ma/m4ma,m4ma/m4ma
bfc22469-aaa5-44ca-bc70-f66fd07129c6,4tu-fit-a-double-logistic-function-to-a-sentinel-2-vegetation-index-vi-time-series,Fit a double logistic function to a Sentinel-2 vegetation index (VI) time series,Fit a double logistic function to a Sentinel-2 vegetation index (VI) time series. The first step is to fit separate double logistic functions to two (overlapping) S2 NDVI time series. Then both local functions are merged to obtain a single global function.,,
bffc5c3c-ce74-474a-bdac-7b065d86d65c,4tu-supplementary-materials-when-is-variable-importance-estimation-in-species-distribution-modelling-affected-by-spatial-correlation,Supplementary Materials: When is variable importance estimation in species distribution modelling affected by spatial correlation?,"Code and Data Metadata for Variable importance graphs using simulated variables    This data entry contains the code and data generated for creating the variable importance graphs as published in the article:     Harisena, N. V., Groen, T. A., Toxopeus, A. G., & Naimi, B. (2021). When is variable importance estimation in species distribution modelling affected by spatial correlation? Ecography, 44(5), 778â€“788. 10.1111/ecog.05534    Contents    1. A Readme.docx file    2. Main_script.R file; Includes the code to be run and the instructions for running the same     3. Functions.R file; Includes all the functions to be loaded in R before running the Main_script.R file    4. Generated_Data; folder with all .Rdata files for each code step as detailed in Main_script.R along with the output .csv and .jpg files that can be matched with the output images in the publication.  A CODECHECK certificate is available confirming that the computations underlying this article could be independently executed: https://doi.org/10.5281/zenodo.5574909",,
c001f415-9812-4e6d-90c8-82494f47fd76,baklava,Baklava,"* Deploy a Kubernetes cluster and big data services on the cloud  - Combination of scripts to deploy a kubernetes cluster  - Deploys big data services such as Apache Spark and Haddop - Currently, it supports OpenNebula platform - Automatically creates Virtual machines on SURF HPCCloud  - Deploys services to SURF HPCCloud",https://github.com/NLeSC/baklava,NLeSC/baklava
c074c7ab-44b6-4896-81f8-00c7b01c4064,4tu-counterfactualexplanations-jl,CounterfactualExplanations.jl,"Code and publication-relevant data for our JuliaCon (JCon) Proceedings paper, Explaining Black-Box Models through Counterfactuals, and the corresponding open-source software package: CounterfactualExplanations.jl (CE.jl).     Results (""results_jcon2023.zip"") for the paper include:  Images (PNG) that are included in the paper.Serialized Julia objects (.jls) for classification models (""*_classifier.jls"") and variational autoencoders (""*_vae*.jls"") used in the paper.   The code base (""CounterfactualExplanations.jl-juliacon-proceedings-jan-2023.tar.gz"") corresponds to the release of the CE.jl that was explicitly linked to the JCon publication in 2023. Just like current versions of the package, it contains the Julia code base and test suite (.jl files), along with the documentation (.qmd and .md files). The #paper branch of the repository contains a paper/ subfolder that was used to compile the paper in 2023. All code used to produce the results in the paper  leveraged the package itself at the time of publication and can be found in paper/_paper.qmd.     CE.jl is still actively maintained and therefore the most up-to-date reference for this work.",https://data.4tu.nl/v3/datasets/61a8469b-e56d-48b8-9bfc-1018b2a8ce1a.git,
c0a091f1-5391-489f-902b-19eb6ad23a5a,4tu-off-framework-code-underlying-the-publication-a-dynamic-open-source-model-to-investigate-wake-dynamics-in-response-to-wind-farm-flow-control-strategies,"OFF framework, code underlying the publication A dynamic open-source model to investigate wake dynamics in response to wind farm flow control strategies","The OFF toolbox provides one interface to dynamic parametric wake modeling. The goal is to enable testing of different approaches, comparisons using the same interface, and an environment to develop new techniques.     The OFF toolbox was used to predict the power and energy generated by a wind farm during ~24 hours of wind direction changes, parts of which were validated with LES. The findings can be found in the publication ""A dynamic open-source model to investigate wake dynamics in response to wind farm flow control strategies"", Marcus Becker, Maxime Lejeune, Philipe Chatelain, Dries Allaerts, Rafael Mudafort, and Jan-Willem van Wingerden, Wind Energy Science, 2024 (submitted).     The input files, as well as the results from the LES and FLORIS can be found in the data repository with the DOI 10.4121/29c209fa-f2a4-456d-9353-67cf81be1aaa     The code used in this repository is designed to work together with NREL's FLORIS toolbox and was tested with FLORIS v4.0.1:  https://github.com/NREL/floris/releases/tag/v4.0.1",https://data.4tu.nl/v3/datasets/edbd184d-dd9f-4d14-a569-d4487c48772b.git,
c0e256e5-bf13-45a7-898e-c448a8b707de,4tu-coordinates,coordinates,"To our knowledge no Python package is available for conversion between cartesian and polar coordinates, so we made one for convenience.  The package converts numpy arrays where the last dimension contains x, y or magnitude, orientation coordinates.  When working with cupy arrays, the package performs calculations on the GPU automatically.",https://data.4tu.nl/v3/datasets/30aba7b0-a5e1-4383-8f15-773d59f9c2f7.git,
c10aa350-cda4-4522-874a-3c35aabd7960,brei,Brei,"Brei is an alternative to GNU Make, designed to be minimal and easy to use. Workflow descriptions are written in a simple TOML file. Tasks can be written in any language. You specify your own runners.  - No new syntax: programmable workflows in TOML or JSON files. - Efficient: Runs tasks lazily and in parallel. - Feature complete: Supports templates, variables, includes and configurable runners. - Few dependencies: Only needs Python &ge;3.11. - Small codebase: Brei is around 1000 lines of Python.",https://github.com/entangled/brei,entangled/brei
c19b03b9-fe93-484d-aeef-42eb455f23d6,4tu-assets-for-joan-a-framework-for-human-automated-vehicle-interaction-experiments-in-a-virtual-reality-driving-simulator,Assets for JOAN: a framework for human-automated vehicle interaction experiments in a virtual reality driving simulator,These files are the assets needed to use the JOAN driving simulator. They can not be used as stand-alone data or software. Please read the documentation at https://JOAN.readthedocs.org or the accompanying software paper for more information on installing and using JOAN.,,
c1b9f81f-6d0d-47b9-a3ce-636f0d077298,4tu-dynamic-graph-decomposition,Dynamic-Graph-Decomposition,"Research Objective is to find out latent graphs which explain the evolution of dynamic graphs along with smooth graph signals. The data-sets are publicly available online, as indicated in the paper. The data is in .mat format. the file tgd_eff.m contains the main algorithm proposed.",https://data.4tu.nl/v3/datasets/abdcddca-1a2d-4dc5-b222-8c404ea18019.git,
c1d7728b-2050-4210-b842-6356cb58c3e3,lokum,Lokum,* Deploys Big data services  on bare-metal cluster.  - Services that can be deployed are:   - Deploy GlusterFS   - Docker   - Apache Spark   - Apache Hadoop   - JupyterHub,https://github.com/NLeSC/lokum,NLeSC/lokum
c1e34a3e-99df-4aeb-b125-a059ebe8b96b,fairdatapoint-client,FAIR Data Point Client,"Provides Python interface to read and write catalogs, datasets and distributions in an FDP server  fairdatapoint-client enable you to interact with your resources (catalogs/datasets/distributions) in a FDP using python. The supported APIs are listed below: FDP Layers	Path Endpoint	                                Specific Resource Endpoint fdp	                [baseURL] or [baseURL]/fdp	 catalog	        [baseURL]/catalog	                        [baseURL]/catalog/[catalogID] dataset	        [baseURL]/dataset	                        [baseURL]/dataset/[datasetID] distribution	[baseURL]/distribution	                [baseURL]/distribution/[distributionID]",https://github.com/fair-data/fairdatapoint-client,fair-data/fairdatapoint-client
c1e63fdb-f102-4e61-b486-ee6f03f5e754,4tu-code-to-generate-transit-times-as-input-for-the-material-accessibility-analysis,Code to generate transit times as input for the Material Accessibility Analysis,"Jupyter notebook containing the code used to generate the data used in the Material Access Stream of the thesis entitled ""A Socio-Spatial Analysis of the Urban Commons in Amsterdam"", submitted for the completion of the MSc. Engineering and Policy Analysis at the TU Delft.",,
c1f4c654-ecc9-4f39-bfb7-98e447db6826,4tu-wake-informed-lifting-line-theory-example-code,Wake-informed Lifting Line Theory: Example Code,The wake-informed lifting line method is a tool that can be used to obtain blade loading distributions from slipstream velocity measurements. This MATLAB code serves as an example implementation of the method and is not intended for direct use with external datasets.     Users must adapt the code to use it for their dataset.     Check out the README.md file for instructions on how to use the code.,,
c23c5063-88a4-4b56-88d3-0c2bc41e8957,4tu-r-code-underlying-the-analysis-reported-in-predicting-elbow-load-based-on-individualised-inter-segmental-rotation-in-fastball-pitching,R code underlying the analysis reported in Predicting elbow load based on individualised (inter)segmental rotation in fastball pitching,"The R code for external valgus torque prediction in baseball pitching. We fitted two-level varying-intercept, varying-slope Bayesian models with various combinations of observed kinematic predictors (pelvis peak angular velocity, trunk peak angular velocity and separation time). The weight and height of the pitchers were added to the kinematic predictors in all models to strengthen the individualization of the external valgus torque prediction. The purpose of the external valgus torque future prediction is to estimate elbow loading for every individual pitcher, not only ones included in this study. Therefore we used leave-one-group-out cross-validation to select the model with best predictive performance.",,
c27c2a27-3d6c-4dcd-833b-65e693e2f5c8,4tu-tu-e-microscopic-energy-consumption-prediction-tool-0-1-tu-e-mecpro-0-1,TU/e Microscopic Energy Consumption PRediction tOol 0.1   (TU/e MECPRO 0.1),"The files contained within this dataset describe a simulation tool that predicts the energy consumption of a battery electric vehicle. The tool is written in MATLAB-code and is connected to various API's to make use of up-to-date route information (Overpass OpenStreetMap API), height information (SRTM elevation map), and weather information (OpenWeatherMap API).   The prediction method relies on a physics-based interpretation of the energy consumption of the vehicle. Both the velocity profile prediction algorithm and the subsequent energy consumption model are based on data obtained from dedicated vehicle tests. In the supplied version of this tool, the parameters represent the Voltia eVan, which is a fully electric delivery van with a swappable traction battery.   The tool was developed within the Dynamics & Control research-group at Eindhoven University of Technology. This project has received funding from the European Unions Horizon 2020 research and innovation programme under grant agreement No. 713771 (EVERLASTING).  Version: 0.1.3  Date:     2021-08-23  Change Log Version 0.1.3------------------------ - Updated 'readhght' from FranÃ§ois Beauducel, which again omits the use of the NASA/EarthDATA account that was introduced in the previous version. SRTM elevation data can now again be obtained without account.  Change Log Version 0.1.2 ------------------------ - Changed the inputs to 'readhght' to use the NASA/EarthDATA server, because the original server hosting the SRTM DEM appears to be off-line. This new server requires a NASA/EarthDATA account. If road slope is to be included, 'settings.includeSlope' (l. 76, TUe_MECPRO.m) should be set to 'true' and NASA/EarthDATA account credentials should be entered in 'NASA_Earthdata_Login.txt'. - Corrected an error in velocityProfilePredicion.m where the prescribed maximum vehicle velocity was wrongly considered to be [m/s] instead of [km/h]. - Corrected an error in processOSMmap.m that resulted in the incorrect registration of the maximum legislated velocity, in case it is prescribed as 'none' (German Highways). Now, 200km/h is assumed in case the maximum legislated velocity is 'none'.",,
c2b1dba1-eca4-41c1-854e-deeaf60675d5,4tu-gsh-is-a-matlab-package-to-do-global-spherical-harmonic-analyses-gsha-and-synthesis-gshs-for-crust1-0,GSH is a MATLAB package to do Global Spherical Harmonic Analyses (GSHA) and Synthesis (GSHS) for Crust1.0.,"Global Spherical Harmonic package (MATLAB) that is able to analyse layered density structure of the Earth. It is also able to synthesise the resulting Stoke coefficients into potential fields, gravity fields, or gravity gradient fields at any height above the reference sphere. Some preliminary plotting of the fields has been made available.",,
c2da3d0c-5448-4de3-a2f7-af16dddede35,dcachefs,dCacheFS,"dCacheFS provides a file-system interface for a dCache storage system, such as the instance provided by SURF. dCacheFS builds on the Filesystem Spec (`fsspec`) library and it can be used as an independent library or via the more general `fsspec` functions.",https://github.com/NLeSC-GO-common-infrastructure/dcachefs,NLeSC-GO-common-infrastructure/dcachefs
c315835c-2dc6-4c7d-a020-dbedab00e435,nanomesh,nanomesh,"* Segment and mesh 2D or 3D image data * Mesh visualization * Easy-to-use Python API * Calculate cell metrics * Export to many mesh formats  Python workflow tool for generating meshes from 2D and 3D microscopy image data. It has an easy-to-use API that can help process and segment image data, generate quality meshes, and write the data to many mesh formats. Nanomesh also contains tools to inspect and visualize the meshes, and generate cell quality metrics.",https://github.com/hpgem/nanomesh,hpgem/nanomesh
c340b324-a0ba-4a52-a533-76e684ca187d,ochre,ochre,"* Train character-based language models/LSTMs for OCR post-correction * Ready to use workflows for data preprocessing, training correction models, doing the post-correction, and analyzing (remaining) errors * Compare (corrected) OCR text to the gold standard based on character error rate (CER), word error rate (WER), and order independent word error rate * Analyze OCR errors on the word level * Discover OCR post-correction data sets  Ochre is experimental software for cleaning up text with OCR mistakes. The software was developed to investigate whether character-based language models can be used to remove OCR mistakes. In addition, ochre provides functionality to analyze the kinds of OCR mistakes in a corpus. This enables researchers to compare different OCR post-correction methods and find out what kinds of mistakes they are good at solving.",https://github.com/KBNLresearch/ochre,KBNLresearch/ochre
c43b172d-6b0a-41b5-b0a8-fea9e908f560,pavics,PAVICS,"PAVICS is a virtual laboratory facilitating the analysis of climate data. It provides access to several data collections ranging from observations, climate projections and reanalyses. It also provides a Python programming environment to analyze this data without the need to download it. This working environment is constantly updated with the most efficient libraries for climate data analysis, in addition to ensuring quality control on the provided data and associated metadata.  * Data and files in netCDF format * Metadata following CF-conventions * Parallel computing environment (xarray + dask)  # Access Datasets PAVICS provides data access via a dedicated THREDDS server providing access to a growing variety of climate and weather related datasets including  * Future climate change projections * Observational data (station & gridded) * Reanalysis data * Weather forecast data   # Analyse Climate Data  PAVICS provides methods for finding and analysing climate data, offering tools that span the range of common data analysis steps including  * Finding and accessing datasets * Spatio-temporal subsetting * Climate indicator calculations * Ensemble statistics & metrics * Data visualization   # Run Hydrological Models  PAVICS offers a suite of tools to streamline the analysis of climate change's impacts on hydrology. It relies on Raven, a hydrological modeling framework that lets hydrologists define custom hydrological models or emulate existing ones. These models are minimally driven by daily time series of temperature and precipitation, and return series of flow and state variables such as snow pack and soil water.  * Simulate stream flow using one of four global models: HMETS, GR4J-CemaNeige, HBV-EC and MOHYSE * Run frequency analyses on flow series * Extract physiographic properties of watersheds * Estimate model parameters in ungauged basins * Make hydrological predictions from numerical weather forecasts  # Services **Climate indices service:** Users of climate data are interested in specific indices such as the number of freeze-thaw cycles, the number of degree-days of cooling, the duration of heatwaves, etc. This returns annual values of the most popular climate indices. - [Source code](https://github.com/Ouranosinc/xclim)  **High-resolution spatial gridded data renderer:** This service renders gridded data on the server and sends images to the client for display within mapping applications using Open Geospatial Consortium (OGC) Web Mappping Service (WMS) standard. - [Source code](https://ouranosinc.github.io/pavics-sdi/_sources/notebooks/rendering.ipynb.txt) - [Documentation](https://ouranosinc.github.io/pavics-sdi/notebooks/rendering.html)  **Node:** Nodes are data, compute and index endpoints accessed through the PAVICS platform or external clients. The Node service is the backend that allows: data storage, harvesting, indexation and discovery of local and federated data; authentication and authorization; server registration and management. Node service is therefore composed of several other services.  **Raven:** A suite of WPS processes to calibrate and run hydrological models, including geographicalinformation retrieval and processing as well as time series analysis. - [Source code](https://github.com/Ouranosinc/raven)",https://github.com/Ouranosinc/PAVICS-landing,Ouranosinc/PAVICS-landing
c43dab34-90db-439a-b26d-a40f7c2c2202,4tu-code-underlying-the-publication-exel-building-an-elaborator-using-extensible-constraints,Code underlying the publication: ExEl: Building an Elaborator Using Extensible Constraints,"This repository contains source code for the paper, slides, extended abstract and a demo implementation for the ExEl paper. This research is addressing some of the common problems in the implementations of dependently typed languages. In particular we present a modular architecture for implementing the transformation from user-friendly surface syntax into a small and well-behaved core language, also known as elaboration. Our architecture is made modular through the use of an open datatype of constraints and a plugin system for solvers that work on these constraints, which means that each new feature is contained in its own module. The implementation was developed by the authors of the paper in through 2022 and 2023.",https://data.4tu.nl/v3/datasets/2e75d19a-7ef8-445b-88d5-ea763cc2488e.git,
c49e5e78-d208-43f3-acce-5ccb25de196a,sv-channels,sv-channels,"- structural variant (SV) caller in short read alignments (BAM files)  using one-dimensional Convolutional Neural Networks - supports detection of major SV types: deletions (DEL), insertions (INS), inversions (INV), tandem duplications (DUP) and inter-chromosomal translocations (CTX)   The workflow includes the following key steps: 1) Transform read alignments into channels First, split read positions are extracted from the BAM files as candidate regions for SV breakpoints. For each pair of split read positions (rightmost position of the first split part and leftmost position of the second split part) a 2D Numpy array called window is constructed. The shape of a window is [window_size, number_of_channels], where the genomic interval encompassing the window is centered on the split read position with a context of [-100 bp, +100 bp) for a window_size of 200 bp. From all the reads overlapping this genomic interval and from the relative segment subsequence of the reference sequence 79 (number_of_channels) channels are constructed, where each channel encode a signal that can be used for SV calling. The list of channels can be found here. The two windows are joined as linked-windows with a zero padding 2D array of shape [10, number_of_channels] in between to avoid artifacts related to the CNN kernel in the part at the interface between the two windows. The linked-windows are labelled as SV when the split read positions overlap the SV callset used as the ground truth and noSV otherwise, where SV is either DEL,INS,INV,DUP or CTX according to the SV type.  2) Model training The labelled linked-windows are used to train a 1D CNN to learn to classify them as either SV or noSV. Two cross-validation strategies are possible: 10-fold cross-validation and cross-validation by chromosome, where one chromosome is used as the test set and the other chromosomes as the training set.  3) SV calling with a trained model Once a trained model is generated and the BAM file for the test set is converted into linked-windows, the SV calling is performed using the predict.py script.",https://github.com/GooglingTheCancerGenome/sv-channels/,GooglingTheCancerGenome/sv-channels
c50b9646-7f6b-4f77-af65-8f9a0f23ca30,4tu-structural-dynamics-model-to-find-the-dynamic-behaviour-as-a-response-of-the-wave-loading-of-the-tidal-bridge-in-indonesia,Structural dynamics model to find the dynamic behaviour as a response of the wave loading of the Tidal Bridge in Indonesia,This file consist of the numeric python model used to model the dynamic response of the BAM Tidal Bridge in Indonesia.  The model describes the wave particle kinematics to calculate the forcing on the floating structure. This file is supplementary to the Master Thesis:  Tidal Bridge Improvement of the operating reliability by reducing the dynamic response due to waves.,,
c51a6038-a08f-4483-a308-7441f0d7d4ff,dianna,DIANNA,"* Provides an easy-to-use interface for non (X)AI experts * Implements well-known XAI methods (LIME, RISE and Kernal SHAP) chosen by systematic and objective evaluation criteria * Supports the de-facto standard of neural network models - ONNX * Supports images, text, time series, and tabular data modalities and some support for embeddings in a related package * Comes with simple intuitive image, text, time series, and tabular benchmarks, so can help you with your XAI research  * Scientific use-cases tutorials * Easily extendable to other XAI methods   Modern scientific challenges are often tackled with (Deep) Neural Networks (DNN). Despite their high predictive accuracy, DNNs lack inherent explainability. Many scientists do not harvest DNNs power because of lack of trust and understanding of their working. Meanwhile, the eXplainable AI (XAI) research offers some post-hoc (after training) interpretability methods that provide insight into the DNN reasoning by quantifying the relevance of individual features (image pixels, words in text, etc.) concerning the prediction. These relevance heatmaps indicate how the network has reached its decision directly in the input modality (images, text, speech etc.) of the scientific data. Representing visually the captured knowledge by the AI system can become a source of scientific insights. There are many Open Source Software (OSS) implementations of these methods, alas, supporting a single DNN format, while standards like Open Neural Network eXchange (ONNX) exist. The libraries are known mostly by the AI experts. For the adoption by the wide scientific community, understanding of the XAI methods, and well-documented and standardized OSS are needed. The DIANNA library supports the best XAI methods in the context of scientific usage providing their OSS implementation based on the ONNX standard and demonstrations on benchmark datasets. DIANNA supports images, text, time-series, tabular data and some embeddings support. DIANNA also comes with scientific use-casse tutorials about the use of XAI in different scientific domains: astronomy, climate, social sciences (law) and soon in geo-sciences.",https://github.com/dianna-ai/dianna,dianna-ai/dianna
c5b6883d-4b55-4a8e-9278-02c0cb28329c,cleanx,cleanX,"* Workflow demos included as Jupyter notebooks * Enables data augmentation * Able to process metadata from csv, json or other formats * Command line interface available  Images can be extracted from DICOM files or used directly. The primary authors are Candace Makeda H. Moore, Oleg Sivokon, and Andrew Murphy. CleanX allows users to do many data exploration and preprocessing steps to prepare images for machine learning algorithms.",https://github.com/drcandacemakedamoore/cleanX,drcandacemakedamoore/cleanX
c6b46c4b-b7d8-4b37-8218-81a16f8a8754,simvascular,SimVascular,"Patient-specific cardiovascular simulation has become a paradigm in cardiovascular research and is emerging as a powerful tool in basic, translational and clinical research. In this paper we discuss the recent development of a fully open-source SimVascular software package, which provides a complete pipeline from medical image data segmentation to patient-specific blood flow simulation and analysis. This package serves as a research tool for cardiovascular modeling and simulation, and has contributed to numerous advances in personalized medicine, surgical planning and medical device design. The SimVascular software has recently been refactored and expanded to enhance functionality, usability, efficiency and accuracy of image-based patient-specific modeling tools. Moreover, SimVascular previously required several licensed components that hindered new user adoption and code management and our recent developments have replaced these commercial components to create a fully open source pipeline. These developments foster advances in cardiovascular modeling research, increased collaboration, standardization of methods, and a growing developer community.",https://github.com/SimVascular/SimVascular,SimVascular/SimVascular
c6dc60e9-c94e-45ce-9ccc-3838f41dd074,4tu-private-logic-and-mpso,private-logic-and-mpso,"This library implements private OR and AND operations, and provides protocols that use these subprotocols to achieve private set operations. Specifically, we support multi-party private set intersections and multi-party private set unions.     Build the code using `cargo build --release`, then follow the help from the command line interface when running it (e.g. using `cargo run --release`).     There is no data.",https://data.4tu.nl/v3/datasets/e762dd44-efc2-43c9-a53a-f5bb07036d39.git,
c70d5d52-7529-4c69-a8a8-ecd47aaa1ead,asreview-simulation,asreview-simulation,,https://github.com/asreview-simulation/asreview-simulation,asreview-simulation/asreview-simulation
c7279476-7f52-4f94-8395-6cb1c2cb1df0,collens,Collens,,https://github.com/collaite/collens,collaite/collens
c72881c7-aea3-49f3-b032-95ec45d7e57f,4tu-sse-akf-directional-waves-pem,sse-akf-directional-waves-pem,"This repository provides a structured methodology for estimating directional sea state parameters using an Adaptive Kalman Filter (AKF) based on measured ship motion responses. The method incorporates the effect of forward speed and encounter frequency transformation, enabling more accurate real-time sea state reconstruction.",https://data.4tu.nl/v3/datasets/22c0cb7c-a7d6-458a-a09c-d8a9112084c4.git,
c757ab53-ef18-44f4-afda-7b4f626d96fb,2022-romanowska-001,Place them on the map,"![Interface screenshot](netlogo_implementation/documentation/LoadWorldImageAndPlaceAgents%20interface.png)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |map (spatial input data)|png/bmp (grid or raster)|image file containing the colour representation of spatial data (e.g. elevation). The example file attached (""world.png"") was taken from the public repository associated to Romanowska, Wren, & Crabtree 2021 (https://github.com/SantaFeInstitute/ABMA/blob/master/ch1/ch1_map.png).| |numberOfTurtles|integer|number of agents (turtles) to be created| |inputX|integer|x coordinate of the desired/created agentsâ€™ location| |inputY|integer|y coordinate of the desired/created agentsâ€™ location| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |agents|object (agentset)|agents initialised at input location| |world|object (world)|worldâ€™s grid initialised with input spatial data| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Romanowska-001,
c7e41fc7-b7ea-49ed-9e92-cf79ddc1c542,4tu-mc-rb-em-1d,MC_RB_EM_1D,"The repository contains Python scripts to perform global and local inversion algorithms of frequency domain electromagnetic induction measurements of 2-layered and 3-layered electrical conductivity earth models. Additionally, the repository holds field data acquired using a DUALEM842s instrument and the Python scripts to perform the inversion algorithms of this field data. Finally, Jupyter Notebooks scripts to display the results.",https://data.4tu.nl/v3/datasets/95ffec64-0d4f-40cf-884e-8006ed31bb06.git,
c835e400-70f3-4997-af31-1829cba5332b,quafing,quafing,Questionnaire analysis using Fisher Information Non-Parametric Embedding enables users to explore the similarity of respondent groups by leveraging the information distance between the multi-dimensional probability density functions of each group - i.e. the most appropriate distance metric - to provide input for lower dimensional embedding and visualization. Quafing supports (mixed) continuous and discrete data responses. WIP,https://github.com/SDCCA/quafing/,SDCCA/quafing
c856a074-aefd-4442-97b8-42565c6574ab,heracalquantum,HERA_CAL_QUANTUM,,https://github.com/QuantumRadioAstronomy/hera_cal_quantum,QuantumRadioAstronomy/hera_cal_quantum
c89579c5-6c2c-4eaa-8145-15ddacf03a13,4tu-r-package-for-walrus-wageningen-lowland-runoff-simulator,R package for WALRUS (Wageningen Lowland Runoff Simulator),This is the R package containing the WALRUS model code. WALRUS is a rainfall-runoff model for catchments with shallow groundwater. This is version 1.11.,,
c89ec4e3-3c88-4c3b-8694-a68c9cc531d5,4tu-robustqo-data-and-codes-related-to-the-paper-extending-the-scope-of-robust-quadratic-optimization,"RobustQO, data and codes related to the paper: ""Extending the scope of robust quadratic optimization.""","Codes related to the paper  Marandi, Ahmadreza, et al. ""Extending the scope of robust quadratic optimization."" arXiv preprint arXiv:1909.01762 (2019)   You can see the documentation by looking at Readme file or clicking on the link below:https://github.com/ahmadreza-marandi/RobustQO",,
c8a816f9-1f89-4472-8720-3c54754ca30f,4tu-trip-planning,trip-planning,"This repository contains the interface developed to support a research project published in the following paper. The project involved an empirical study exploring how human decision-makers interact with AI systems when dealing with complex and uncertain tasks. The corresponding code for the interface and the full list of tasks are available here.  Sara Salimzadeh, Gaole He, and Ujwal Gadiraju. 2024. Dealing with Uncertainty: Understanding the Impact of Prognostic Versus Diagnostic Tasks on Trust and Reliance in Human-AI Decision Making. In Proceedings of the CHI Conference on Human Factors in Computing Systems (CHI '24). Association for Computing Machinery, New York, NY, USA, Article 25, 1â€“17.&nbsp;https://doi.org/10.1145/3613904.3641905",https://data.4tu.nl/v3/datasets/5693e16f-2ea4-4066-905d-560b80ff0b21.git,
c8ca92f2-495e-48dd-82ed-e69880615bcd,4tu-code-underlying-the-publication-restad-reconstruction-and-similarity-transformer-for-time-series-anomaly-detection,"Code underlying the publication: ""RESTAD: Reconstruction and Similarity Transformer for time series Anomaly Detection""","This repository contains the official implementation of RESTAD (REconstruction and Similarity-based Transformer for time series Anomaly Detection), a novel framework that integrates reconstruction error with Radial Basis Function (RBF) similarity scores to enhance sensitivity to subtle anomalies. RESTAD leverages a Transformer architecture with an embedded RBF layer to synergistically detect anomalies in time series data, outperforming existing baselines on multiple benchmark datasets.",https://data.4tu.nl/v3/datasets/0b854951-5345-48a8-8273-ba51a9c451be.git,
c8cf543d-5451-4bde-bc97-749d2d96bf28,4tu-scv-sparse-data,scv_sparse_data,"&nbsp;This code is part of the Ph.D. thesis of&nbsp;Isabelle M. van Schilt, Delft University of Technology. It calculates supply chain visibility for a given supply chain network with sparse data.",https://data.4tu.nl/v3/datasets/82d23d2c-fe0d-4ea6-b46f-725e18b084a1.git,
c8daee26-8595-494a-b7d2-e6b10fb3391e,dumux,DuMux,"# What is DuMux?  [DuMu<sup>x</sup>][0] is a simulation framework with a focus on finite volume discretization methods, model coupling for multi-physics applications, and flow and transport applications in porous media.  DuMu<sup>x</sup> is based on the [DUNE][1] framework from which it uses the versatile grid interface, vector and matrix types, geometry and local basis functions, and linear solvers. DuMu<sup>x</sup> then provides  * Finite volume discretizations (Tpfa, Mpfa, Staggered) and control-volume finite element discretization schemes * A flexible system matrix assembler and approximation of the Jacobian matrix by numeric differentation * A customizable Newton method implementation including line search and various stopping criteria * Many pre-implemented models (Darcy-scale porous media flow, Navier-Stokes, Geomechanics, Pore network models, Shallow water equations) and constitutive models * A multi-domain framework for model coupling suited to couple subproblems with different discretizations/domains/physics/dimensions/... and create monolithic solvers  DuMu<sup>x</sup> has been applied to model complex and non-linear phenomena, such as CO2 sequestration, soil remediation, reactive transport and precipitation phenomena, drug delivery in cancer therapy, flow in micro-fluidics, root-soil interaction, flow in fractured porous media, atmosphere-soil flow interaction, evaporation, and more. Please have a look at our journal publications (see below: [How to cite](#how-to-cite)) for a more detailed description of the goals the development history and motivations behind DuMu<sup>x</sup>.  [![Project Status: Active â€“ The project has reached a stable, usable state and is being actively developed.](https://www.repostatus.org/badges/latest/active.svg)](https://www.repostatus.org/#active) ![GitLab Last Commit](https://img.shields.io/gitlab/last-commit/dumux-repositories%2Fdumux?gitlab_url=https%3A%2F%2Fgit.iws.uni-stuttgart.de%2F) ![GitLab Release](https://img.shields.io/gitlab/v/release/dumux-repositories%2Fdumux?gitlab_url=https%3A%2F%2Fgit.iws.uni-stuttgart.de&label=latest%20DuMux%20release&color=orange)   # Overview  The following resources are useful to get started with DuMu<sup>x</sup>:  * [Installation guide][3] * [Getting started guide](https://dumux.org/docs/doxygen/master/getting-started.html) * [Documentation](https://dumux.org/docs/doxygen/master/), * [DuMu<sup>x</sup> course materials](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux-course/tree/master), * [Examples](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/tree/master/examples), with detailed description of code and results, * [Mailing list](https://listserv.uni-stuttgart.de/mailman/listinfo/dumux), * [Changelog](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/blob/master/CHANGELOG.md), where all changes between different release versions are listed and explained.  Some helpful code snippets are available in the [Wiki](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/wikis/home).  Automated testing of installation: [![installation testing pipeline](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux-test-installation/badges/main/pipeline.svg)](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux-test-installation/-/pipelines?page=1&scope=all&ref=main)  # License  [![REUSE status](https://api.reuse.software/badge/git.iws.uni-stuttgart.de/dumux-repositories/dumux)](https://api.reuse.software/info/git.iws.uni-stuttgart.de/dumux-repositories/dumux)  DuMu<sup>x</sup> is licensed under the terms and conditions of the GNU General Public License (GPL) version 3 or - at your option - any later version. The GPL can be [read online][4] or in the [LICENSE.md](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/-/blob/afb7f2296d84fd2367c612a1084d9b47ff85a260/LICENSE.md) file provided in the topmost directory of the DuMu<sup>x</sup> source code tree.  Please note that DuMu<sup>x</sup>' license, unlike DUNE's, does *not* feature a template exception to the GNU General Public License. This means that you must publish any source code that uses any of the DuMu<sup>x</sup> header files if you want to redistribute your program to third parties. If this is unacceptable, please [contact us][5] for a commercial license.  See the file [LICENSE.md](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/-/blob/afb7f2296d84fd2367c612a1084d9b47ff85a260/LICENSE.md) for copying permissions. For a curated list of contributors, see [AUTHORS.md](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/-/blob/afb7f2296d84fd2367c612a1084d9b47ff85a260/AUTHORS.md). If you notice that a contributor is missing on the list, please [contact us][5] or open a merge request adding the name.  # How to cite  DuMu<sup>x</sup> is research software and developed at research institutions. You can cite **specific releases** via [**DaRUS**](https://darus.uni-stuttgart.de/dataverse/iws_lh2_dumux) (from 3.6) or **Zenodo**: [![zenodo badge](https://zenodo.org/badge/DOI/10.5281/zenodo.2479594.svg)](https://doi.org/10.5281/zenodo.2479594). You can also cite individual code files or even lines via **Software Heritage**: [![sw](https://archive.softwareheritage.org/badge/origin/https://git.iws.uni-stuttgart.de/dumux-repositories/dumux.git/)](https://archive.softwareheritage.org/swh:1:dir:e947c9ac369afd90195080e4a06bbde2e1e150ca;origin=https://git.iws.uni-stuttgart.de/dumux-repositories/dumux.git;visit=swh:1:snp:3cf49b55de0218903103d428c378e356d7d4d082;anchor=swh:1:rev:11871e4abf619d4cb3f938aedd7a2dea47ce1e87)   If you are using DuMu<sup>x</sup> in scientific publications and in the academic context, please cite (at least one of) our publications:  **DuMux 3 â€“ an open-source simulator for solving flow and transport problems in porous media with a focus on model coupling.** *Computers & Mathematics with Applications*, 81, 423-443, (2021). [![dumuxCAMWAbadge](https://img.shields.io/badge/DOI-10.1016%2Fj.camwa.2020.02.012-blue)](https://doi.org/10.1016/j.camwa.2020.02.012) [PDF][6]  ```bib @article{Koch2021,     doi = {10.1016/j.camwa.2020.02.012},     year = {2021}, volume = {81}, pages = {423--443},     publisher = {Elsevier {BV}},     author = {Timo Koch and Dennis GlÃ¤ser and Kilian Weishaupt and others},     title = {{DuMux} 3 {\textendash} an open-source simulator for solving flow and transport problems in porous media with a focus on model coupling},     journal = {Computers \& Mathematics with Applications}} ```  **DuMux: DUNE for multi-{phase,component,scale,physics,â€¦} flow and transport in porous media.** *Advances in Water Resources*, 34(9), 1102â€“1112, (2011) [![dumuxAWRbadge](https://img.shields.io/badge/DOI-10.1016%2Fj.advwatres.2011.03.007-blue)](https://doi.org/10.1016/j.advwatres.2011.03.007) [PDF][2]  ```bib @article{Flemisch2011,     doi = {10.1016/j.advwatres.2011.03.007},     year = {2011}, volume = {34}, number = {9}, pages = {1102--1112},     publisher = {Elsevier {BV}},     author = {B. Flemisch and others},     title = {{DuMux}: {DUNE} for multi-$\lbrace$phase, component, scale, physics, {\ldots}$\rbrace$ flow and transport in porous media},     journal = {Advances in Water Resources}} ```  # Contributing  Contributions are highly welcome. Please ask questions over the [mailing list](mailto:dumux@listserv.uni-stuttgart.de). Please review the [contribution guidelines](https://git.iws.uni-stuttgart.de/dumux-repositories/dumux/blob/master/CONTRIBUTING.md) before opening issues and merge requests.  [0]: https://dumux.org [1]: https://dune-project.org/ [2]: https://dumux.org/documents/dumux_awrpaper.pdf [3]: https://dumux.org/docs/doxygen/master/installation.html [4]: https://www.gnu.org/licenses/gpl-3.0.en.html [5]: https://www.iws.uni-stuttgart.de/en/lh2/ [6]: https://doi.org/10.1016/j.camwa.2020.02.012",https://git.iws.uni-stuttgart.de/dumux-repositories/dumux,
c90572f6-97a3-4ed6-b0de-51d04593a5a6,ahn-pointcloud-viewer-ws,AHN point cloud viewer web service,"* Able to handle data sets of billions of points * Allows for the extraction of points from areas of interest to the user * Provides tools for measurement  This web application was developed in collaboration with Markus Schutz, the developer of potree.  If you want to set up your own web application with this kind of data, we highly recommend Massive-PotreeConverter, which you can find on this site.",https://github.com/NLeSC/ahn-pointcloud-viewer-ws,NLeSC/ahn-pointcloud-viewer-ws
c935eb85-6aff-4359-a5d8-60233b68e78e,4tu-landfill-emission,Landfill Emission,"Python 3 package for landfill emission modelling. This data set provides access to a git repository which provides the code and data sets to run the code for four different landfill cells in the Netherlands operated by Afvalzorg NV. The scripts run the code and generate the figures used for the corresponding paper.     The data have been downloaded from the iDS landfill data base hosted by the Delft University of Technology. As only registered users can access the database, compressed data files have been added to the repository.     The docs-html folder provides documentation to enable users to get started with the code.",https://data.4tu.nl/v3/datasets/bb3c2553-0c95-4635-8246-4f9adc4ad2b8.git,
c9828d41-caea-4edf-8176-d1f5c4f7cae1,bias,BIAS,,https://github.com/Dvermetten/BIAS,Dvermetten/BIAS
c989e144-90a4-467f-b0ab-d6777bf8e5d9,4tu-floridyn-off-toolbox-code-and-input-files-underlying-the-publication-wind-pattern-clustering-of-high-frequent-field-measurements-for-dynamic-wind-farm-flow-control,FLORIDyn (OFF toolbox) Code and input files underlying the publication: Wind pattern clustering of high frequent field measurements for dynamic wind farm flow control,"Python implementation of the FLORIDyn code [1] in the OFF toolbox, as well as the input files used for the publication ""Wind pattern clustering of high frequent field measurements for dynamic wind farm flow control"", M Becker, D Allaerts and JW van Wingerden, in preparation for the TORQUE 2024 conference.    The yaml files contain the baseline orientation (misalignment angle = 0 deg) and optimised angles. The C1 to C5 addition in the name refers to the wind direction change case that is investigated. The optimised angles have beed derived using FLORIS [2] and are given in separate files for convenience.    To run the code FLORIS [2] needs to be installed. In addition the DTU 10MW reference turbine [3] is used in the simulations. The required .yaml is provided with this code and needs to be added to the FLORIS turbine library. Go to 03_Code/main.py and run the selected case.    The farm power of all simulations in FLORIDyn in given in the files FarmPower*.csv  To use them in Matlab, call:    farm_pow = readmatrix(""FarmPowerNoYaw.csv"");  farm_pow = reshape(farm_pow, 5, length(1:2:359), [] );    Dimension 1 is along the cases, dimension 2 along the wind directions and dimension 3 along time (4:4:2000).  E.g. to plot the power for case C4 starting at 201deg call  plot(4:4:2000, farm_pow(4, (201+1)/2, :))    [1] https://iopscience.iop.org/article/10.1088/1742-6596/2265/3/032103/meta  [2] https://github.com/NREL/floris  [3] https://backend.orbit.dtu.dk/ws/portalfiles/portal/55645274/The_DTU_10MW_Reference_Turbine_Christian_Bak.pdf",https://data.4tu.nl/v3/datasets/51d3e0a5-61d4-40b7-905d-3a2a9ebc29b0.git,
c9984c7e-34d6-47c3-ba5e-8549847d14eb,addfeatures,addFeatures,,https://github.com/magnuspalmblad/addFeatures,magnuspalmblad/addFeatures
ca7b8bfc-daa9-4f3a-9094-3ea5c0981bf9,2021-angourakis-001,Angle-and-step random walk,"![Example of output: three random walk trajectories in continuous 2D space](python_implementation/documentation/randomWalk_v01_plot.png)  [`demonstration.ipynb`](python_implementation/demonstration.ipynb): Jupyter Notebook using the Python implementation that sets up a workflow for executing several random walks and plotting them. It also includes a demonstration of the test methods.  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |worldDimensions|dictionary (Dict)|The 2D world dimensions or limits. Specifically, the minimum and maximum coordinates in x and y that are considered valid positions in space. It is a dictionary with 'x' and 'y' keys, each associated to a two-item tupple of floats, representing minimum and maximum coordinates; e.g. {'x':(-50,50), 'y':(-50,50)}| |angleLeftMax, angleRightMax|float|Left and right angles in relation to an agent current angular direction that mark the maximum range of stochastic variation in direction for a step in a random walks. Both values must be set within the 0-180 interval. If both are set to 180, the direction of the previous step will have no impact on the direction of the next step (i.e. angle will be sampled from a 0-360 degrees interval).| |moveDistanceMin, moveDistanceMax|float|Minimum and maximum distance travelled after a step. When moveDistanceMin is less than moveDistanceMax, these mark the maximum range of stochastic variation in distance per step. If these are equal, distance travelled per step will be constant. MoveDistanceMin must be a positive number equal to or less than MoveDistanceMin. MoveDistanceMin must be a positive number equal to or greater than MoveDistanceMin, but less than the maximum world distance set by worldDimensions.| |(initial) x, y|float|Initial x and y coordinates of the agent's position.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |positionX, positionY|float|The agent's position in x and y coordinates within the 2D space set by worldDimensions.| |direction|float|The agent's angular direction in degrees.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2021-Angourakis-001,
caa72c6a-a3c1-40e4-8dba-8d990d2f3db1,horton,HORTON,"* HORTON is designed to be a helpful framework for rapidly prototyping methods and testing ideas, together with utilities that ensure the resulting implementation is not too inefficient. HORTON is not designed to achieve bleeding-edge performance: readability, extensibility, and modifiability are often preferred over computational efficiency and code compactness when trade-offs are essential.  * HORTON is, and always will be, open source, distributed under the GNU General Public License. The HORTON development team always welcomes new contributions.  * HORTON is a research tool for both its developers and users. As a result, the available functionality is naturally biased towards the interests of its developers. The current focus of HORTON is on low-cost ab initio electronic structure theory methods and post-processing tools for interpreting electronic structure calculations. Additional functionality can be provided by other developers, and through interfaces to various programs. If you are interested in joining the HORTON development team, please contact us through the the HORTON mailing list.  * HORTON can used in three ways: as part of a larger program, as a stand-alone problem, outside other programs. Retaining this flexibility is a fundamental design principle of HORTON. Horton can be run as a stand-alone program using either input files or Python scripts. However, when HORTONâ€™s functionality is limited or its computational requirements are too large, HORTON can be used in scripts for managing and post-processing calculations from other SchrÃ¶dinger solver software. HORTON can also be used, to provide specialized functionality to a larger program (e.g., to update the atomic charges in a molecular dynamics simulation). To facilitate the use of HORTON within other programs, we guarantee no major API changes for at least twelve months after each major release.",https://github.com/theochem/horton/,theochem/horton
cab4190e-23cd-47cd-8ea8-d95610a97dd5,4tu-code-on-sliding-mode-observer-based-detection-of-mitm-attacks-in-cvps,Code on Sliding Mode Observer Based Detection of MITM Attacks In CVPs,"Sliding mode observers (SMOs) have been proposed for exact anomaly estimation for a class of ideal systems without unmatched uncertainties and measurement noise. For such ideal systems anomaly detection is trivial, however for systems with unmatched uncertainties or measurement noise a dedicated detector is needed. In this code two of such robust anomaly detectors are implemented, which extend the anomaly detection capability of a large class of SMOs to include systems with unmatched uncertainties and measurement noise. The first detector is based on the so-called equivalent output injection (EOI), which is closely related to the anomaly estimate. The second detector is directly based on the SMO state estimation error. Doing so, the second detector bypasses the low-pass filter generating the EOI allowing for faster detection of anomalies and making it possible to detect smaller magnitude anomalies. The detectors are applied in this code to detect a man-in-the-middle (MITM) attack on a collaborative vehicle platoon (CVP).       The code used to generate results presented in chapter 2 of the PhD dissertation of Twan Keijzer: Advances in Safety and Security of Cyber-Physical Systems - Sliding Mode Observers, Coalitional Control and Homomorphic Encryption",,
cb15417b-d434-410e-a776-6f5a974bbda0,springtime,Springtime,"The Springtime Python package helps to streamline workflows for doing machine learning with phenology data.   Phenology is the scientific discipline in which we study the lifecycle of plants and animals. A common objective is to develop (Machine Learning) models for the occurrence of phenological events, such as the blooming of plants. Since there is a variety of data sources and existing tools to retrieve and analyse them, project folders and code organization can quickly get messy.   With Springtime, we aim to provide a more streamlined workflow for working with a variety of datasets and (ML) models. You can run Springtime as a command line tool in a terminal or use it as a Python library e.g. in a Jupyter notebook.",https://github.com/phenology/springtime/,phenology/springtime
cb4ff9a9-5ee6-4af0-88be-e454b28e96a7,4tu-code-supporting-the-paper-sensitivity-and-feasibility-of-a-one-dimensional-morphoelastic-model-for-post-burn-contraction,Code supporting the paper: Sensitivity and feasibility of a one-dimensional morphoelastic model for post-burn contraction,"This online resource shows an archived folder including Matlab files used for the paper Sensitivity of a oneâ€‘dimensional biomorphoelastic model for postâ€‘burn contraction. Within the archived folder, one finds two folders called Sensitivity and Feasibility. The sensitivity folder contains code for the sensitivity analysis and has a Matlab live script (Sensitivity.mlx). The feasibility folder contains code for the feasibility study and uses 8 processors. This folder also has a Matlab live script (Feasibility.mlx). Within the scripts, there are comments explaining the code.",,
cbe8a735-2119-453d-8362-00537b584bbf,ceiba-cli,ceiba-cli,"* provides a Command line interface * Allows to store, update and retrieve simulation data directly from the command line   Ceiba-cli is a command line interface to interact with the ceiba web service to store/retrieve simulation data from the command line. It provides the add, compute, report and query actions that represent all the possible interaction between the user and the web services that manage the data",https://github.com/nlesc-nano/ceiba-cli,nlesc-nano/ceiba-cli
cc87ef5b-638b-4ca6-826a-d8510e27311f,visor,visor,"<!-- README.md is generated from README.Rmd. Please edit that file -->  # visor  <!-- badges: start -->  [![R-CMD-check](https://github.com/CityRiverSpaces/visor/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/CityRiverSpaces/visor/actions/workflows/R-CMD-check.yaml) <!-- badges: end -->  The goal of visor is to provide a set of tools for visibility analysis.  ## Installation  You can install the development version of visor from [GitHub](https://github.com/) with:  ``` r # install.packages(""pak"") pak::pak(""CityRiverSpaces/visor"") ```  ## Example  This is a basic example which shows you how to solve a common problem:  ``` r library(visor) library(sf)  occluders_geom <- st_sfc(   create_occluder(1, 1, 1, 0.5),   create_occluder(4, 1, 1.5, 0.7),   create_occluder(7, 1, 0.8, 0.8),   create_occluder(2, 5, 2, 1),   create_occluder(5, 5, 1, 1.5),   create_occluder(1, 7, 1.2, 0.6),   create_occluder(7, 7, 1.8, 0.9)) occluders <- st_sf(id = 1:7, geometry = occluders_geom)  line_geom <- st_sfc(st_linestring(matrix(c(0, 3, 9, 3), ncol = 2, byrow = TRUE))) line <- st_sf(id = 1, geometry = line_geom)  vpoints <- get_viewpoints(line, 1)  isovist <- get_isovist(vpoints, occluders, ray_num = 160, ray_length = 5,                        remove_holes = FALSE)  plot(isovist, col = ""blue"") plot(occluders_geom, col = ""grey"", add = TRUE) plot(line_geom, col = ""lightblue"", add = TRUE) plot(vpoints, col = ""red"", add = TRUE) ```  ![](https://raw.githubusercontent.com/CityRiverSpaces/visor/main/man/figures/README-example-1.png)",https://github.com/CityRiverSpaces/visor,CityRiverSpaces/visor
ccad7109-b95b-4522-a8ee-f13e06921181,astroimagesjl,AstroImages.jl,,https://github.com/JuliaAstro/AstroImages.jl,JuliaAstro/AstroImages.jl
ccb05847-12c3-4063-be0b-1482cc9bd098,4tu-code-to-produce-the-results-of-the-publication-reducing-the-soleus-stretch-reflex-with-conditioning-exploring-game-and-impedance-based-biofeedback,Code to produce the results of the publication: â€œReducing the Soleus Stretch Reflex with Conditioning: Exploring Game- and Impedance-based Biofeedbackâ€,"This folder contains code to produce the results of ""R. C. van 't Veld, E. Flux, A. C. Schouten, M. M. van der Krogt, H. van der Kooij, E. H. F. van Asseldonk. Reducing the Soleus Stretch Reflex with Conditioning: Exploring Game- and Impedance-based Biofeedback. Frontiers in Rehabilitation Sciences. https://doi.org/10.3389/fresc.2021.742030""",,
ccb40022-333b-41d4-87a3-0eb6d2f5fd4f,4tu-python-code-of-hydrodynamic-model-underlying-the-master-thesis-the-delta-barrier-a-climate-robust-preliminary-design-of-the-delta21-storm-surge-barrier,"Python code of Hydrodynamic Model underlying the master thesis: ""The Delta Barrier - A climate robust preliminary design of the Delta21 storm surge barrier""",This data set includes the python code for the numerical analysis of the hydrodynamics regarding the impact of a (closeable) storm surge barrier on a tidal basin influenced by both the tide and river discharge for the Delta21 project. The code can be run via Python software.,,
cd2065ae-0928-45ab-8bbb-0f1ba89895a9,sfincs-bmi-server,SFINCS-BMI-server,,https://github.com/eWaterCycle/sfincs-bmi-server/,eWaterCycle/sfincs-bmi-server
cd600203-fc2e-4539-9562-e9f9488f9134,aiproteomics,aiproteomics,"This package contains various tools, datasets and ML model implementations from the field of (phospho-)proteomics. It is intended to facilitate the testing and comparison of different neural network architectures and existing models, using the same datasets. Both retention time and fragmentation (MSMS) models are included.  Implementations of existing models from the literature are intended to be modifiable/extendable. For example, so that tests may be carried out with different peptide input lengths etc.",https://github.com/aiproteomics/aiproteomics,aiproteomics/aiproteomics
cd6fcc99-afb7-4a6c-8df1-253b87df153c,4tu-task-aware-connectivity-learning-for-incoming-nodes-over-growing-graphs,Task-aware-connectivity-learning-for-incoming-nodes-over-growing-graphs,Code for the titled paper. Research objective is to find task-aware stochastic attachment models for cold start node inference. Data-sets used are available online.  Synthetic Data: Contains the code for the experimental setup shown in Section IV A of the paper  Movielens100k: Contains the code for the experimental setup shown in Section IV B of the paper (Data-Set used here to be found online)  Blog: Contains the code for the experimental setup shown in Section IV C of the paper (Data-Set used here to be found online),https://data.4tu.nl/v3/datasets/d2cc2e85-0c1d-41b1-9be4-2845bf879875.git,
cd6fea77-939c-40f1-8437-c176f050d0ce,4tu-code-for-evaluating-albedo-change-and-radiative-forcing-from-roof-pv-integration-in-delft,Code for evaluating albedo change and radiative forcing from roof PV integration in Delft,Data and MATLAB code for evaluating albedo change and radiative forcing from roof PV integration in Delft.  This dataset contains the data and scripts created for the project studying the impacts of urban photovoltaic (PV) integration on radiative forcing. The objective of this research is to employ a high-resolution approach to quantify both positive and negative RF effects of distributed urban rooftop PV systems with open-source LiDAR and geo-referenced material data. The city of Delft is used as a study case. The scripts are divided into three parts:  1. Albedo simulation before and after PV integration;  2. Annual PV energy calculation;  3. Positive and negative radiative forcing calculation.   Unzip the file and read 'README.txt'.,https://data.4tu.nl/v3/datasets/fe1b6865-08b7-456b-9bf8-af963b9175aa.git,
cd7dc41c-72d3-4c4e-bf23-abc1dfb0d096,4tu-learning-on-scs,Learning_on_SCs,"This implements the convolutional neural network models on simplicial complexes, including applications of simplex-prediction and trajectory prediction.",https://data.4tu.nl/v3/datasets/82579c3e-d7a1-46c4-b7ba-a5357a1c61dc.git,
cdd7672c-e7dd-4773-b04b-efabaa92aeb0,4tu-supplementary-data-for-the-paper-from-a-to-b-with-ease-user-centric-interfaces-for-shuttle-buses,Supplementary data for the paper 'From A to B with Ease: User-Centric Interfaces for Shuttle Buses',"User interfaces are crucial for easy travel. To understand user preferences for travel information during automated shuttle rides, we conducted an online survey with 51 participants from 8 countries. The survey focused on the information passengers wish to access and their preferences for using mobile, private, and public screens during boarding and travelling on the bus. It also gathered opinions on the usage of Near-Field Communication (NFC) for shuttle bus confirmation and viewing assistance to help passengers stand precisely where the shuttle will arrive, overcoming navigation and language barriers. Results showed that 72.6% of participants indicated a need for NFC and 82.4% for viewing assistance. There was a strong correlation between preferences for shuttle bus schedules, route details (r=0.55), and next-stop information (r=0.57) on mobile screens, suggesting that passengers who value one type of information are likely to value related kinds too.&nbsp;",,
ce40fc2b-aa4b-4be0-aa82-be91aa9b76e7,4tu-code-to-analyse-winch-sizing-for-ground-generator-airborne-wind-energy-systems,Code to analyse winch sizing for ground-generator airborne wind energy systems,"Airborne wind energy is an emerging technology that aims to capture the stronger and more steady higher altitude winds using tethered kites. The size of the winch, defined by its inertia and radius, has an important effect on the dynamics of the overall system for ground-generation concepts. This repository contains code to analyse these dynamics using a linearized simplified model of a ground-generation airborne wind energy system.     The publication related to this code is currently in review and will be linked on this page once published.",https://data.4tu.nl/v3/datasets/4c7e02fc-1b78-4eae-b9ca-f1a5a4ba641c.git,
ce5d6772-1cc4-4d29-8139-1fae4a120204,clindig,ClinDIG,"ClinDIG integrates two software platforms: CanDIG and CARDS to create a rich clinical and genomic data management solution that enables federated analysis while protecting patient privacy. CanDIG offers a suite of federation services to enable privacy-protected clinical and phenotypic data sharing and research across multiple sites. CARDS offers a data management platform that can be configured for your own needs, integrating a UI for questionnaire development, connection to numerous ontologies for data structure, and a patient-facing questionnaire UI for direct data capture.",https://github.com/CanDIG/CanDIGv2,CanDIG/CanDIGv2
ce94eaea-c173-4a11-a604-91307ad693e9,4tu-bluesky-software-underlying-the-publication-improving-algorithm-conflict-resolution-manoeuvres-with-reinforcement-learning,Bluesky software: underlying the publication â€œImproving Algorithm Conflict Resolution Manoeuvres with Reinforcement Learningâ€,"Bluesky scenarios and code files used in the work ""Improving Algorithm Conflict Resolution Manoeuvres with Reinforcement Learning"". The scenario files can be used with the Bluesky simulator tool implementation found at https://github.com/TUDelft-CNS-ATM/bluesky.",,
cf0299ea-ea50-41eb-85e1-a484542744e6,linkahead,LinkAhead,"LinkAhead simplifies structured access to all research data and thus reduces the hurdles for current and future researcher.  Storing new kinds of data or changing the structure of existing data is easy due to the flexible, semantic data model of LinkAhead.  A powerful search language provides access not only to the data and its contents, but also to interlinked data sets at any hierarchical depth.  LinkAhead offers an intuitive data access and visualization via a user friendly web interface.  You can rely on open APIs and clients in various programming languages like Python, C, C++ and Octave/Matlab as well as ready to use toolkits for automation.   LinkAhead adapts to your unique setup and mix of data sources. The software can integrate Electronic Lab Notebooks (ELNs), databases, repositories, machines or any other source that provides data in a structured manner. LinkAhead can be configured to automatically detect new and updated research data and reference it in the system. A precise versioning history and the flexibility of the underlying data model allows you to operate safely without putting your research data at risk. With your data set brought together in one place, you have new opportunities in terms of search, API access and collaboration.  - FAIR data with ease - Automated data integration and efficient searchability - Agile data model adaptable at any time",https://gitlab.com/linkahead/linkahead-server,
cfd3bd0a-cd1f-494f-8f9f-2d7a4ba79f4e,4tu-pystreed,pystreed,"STreeD is a framework for optimal binary decision trees with separable optimization tasks. A separable optimization task is a task that can be optimized separately for the left and right subtree. The current STreeD Framework implements a broad set of such optimization tasks, from group fairness constraints to prescriptive policy generation. For an explanation of each application, see below. For details on what tasks are separable and how the algorithm works, see our paper.",https://data.4tu.nl/v3/datasets/2aab49f2-a61b-456d-88d0-679b3e9103ac.git,
cfd4a3da-11df-4d4b-98ff-7d9905675ab4,4tu-code-supporting-the-paper-stability-of-a-two-dimensional-biomorphoelastic-model-for-post-burn-contraction,Code supporting the paper: Stability of a two-dimensional biomorphoelastic model for post-burn contraction,"This online resource shows an archived folder including Matlab files used for the paper Stability of a twoâ€‘dimensional morphoelastic model for postâ€‘burn contraction. Within the archived folder, one finds the Matlab live scripts Stability.mlx and Convergence.mlx, for the stability validation and convergence study, respectively. The code uses parallel computing with 5 processors.&nbsp;     Within the scripts, there are comments explaining the code.&nbsp;We note that the numerical study takes a few hours. The data that the codes creates can be found in the linked online resource Data supporting the paper: 2D stability. Within the subfolder Convergence, one finds a dataset containing the nodes on which we compute the convergence order. These are the nodes that exist in all the simulations where we used different resolutions.  &nbsp;",,
d030f1dc-582f-41d5-a1c7-a5af5feaed6a,4tu-aps,APS,This Python package contains the Python implementation of the empirical method from the Asses Pedestrian model Speed (APS) framework.,https://data.4tu.nl/v3/datasets/cad584e6-eeb5-4aac-97e6-53f87d6e578e.git,
d0965468-129a-4a47-ac20-6f6d5727173a,gamebus-fhir-layer,GameBus FHIR Layer,"Gamebus FHIR layer is a framework to enable GameBus platform to provide FHIR service.  It is built on a technology stack of open source software and consists of two main parts: a mapping engine and a FHIR web server (see the diagram). The two components are integrated into the FHIR layer in order for GameBus to provide FHIR compliant data.  Though it is developed for GameBus platform, it can be easily reused for other healthcare systems with some adaptations to the details.  ![diagram](https://raw.githubusercontent.com/nwo-strap/gamebus-fhir-layer/main/docs/source/image/architecture.png)   ## Check the doc and software for details:  - [Tutorials and documentation](https://fhir-layer.readthedocs.io) - [Docker image](https://hub.docker.com/r/nlesc/gamebus-fhir-layer) - [Source code](https://github.com/nwo-strap/gamebus-fhir-layer)  - [Poster](https://zenodo.org/record/7261350) for HealthRI 2022 conference",https://github.com/nwo-strap/gamebus-fhir-layer,nwo-strap/gamebus-fhir-layer
d09c42fe-02f1-4b59-b597-98756822bd53,4tu-rockin-webapp,rockin-webapp,"A simple webapp developed in Python to register rock samples collected from a well or borehole. This tool was developed to use the existing data-entry forms of an MS Access database (published here) as a front-end client connected to a remote MySQL database. It includes a Django data model for a MySQL database, as well as scripts to migrate data from Microsoft Access to MySQL. The data model was developed the package Pydantic, helps to describe relationships between entities without having to rely on a specific implementation, webapp or database. This data model can also be used to generate schemas.    Initially, this webapp would have been used to register samples from the geothermal well installed on TU Delft campus (https:www.tudelft.nl/geothermalwell) but drilling activities ended before the tool was completed. However, this sample registration tool could be of interest for other borehole drilling projects.     The webapp was developed by Jose Urra Llanusa, software developer from TU Delft Digital Competence Centre (DCC), and the pydantic data model is based on the data model from the MS Access database developed by Liliana Vargas Meleza, Data Manager of the geothermal well.",https://data.4tu.nl/v3/datasets/33ef5067-bfd7-46a0-aff8-edbe70773374.git,
d0cbdac9-eee0-4642-baed-3697273f2c2b,pdb2sql,pdb2sql,"* a powerful pdb2sql object to convert biomolecular structure data (in PDB format) in SQL database * strcuture transformation functions (rotations, translations...) * to calcualte structure interface (contact atoms and residues) * to calculate structure similarity (iRMSD, lRMSD, FNAT, DockQ...)  pdb2sql easily allows to load a PDB file in an object. Once loaded, the data can be parsed using SQL queries. To facilitate the adoption of the tool simple methods have been developped to wrap the SQL queries in simple methods.  Quick examples and documentation are available on https://pdb2sql.readthedocs.io.",https://github.com/DeepRank/pdb2sql,DeepRank/pdb2sql
d0f8c3cd-bcd1-4aaa-9633-33aed096f349,wearda,WEARDA,"We present WEARDA, the open source WEARable sensor Data acquisition software package. WEARDA facilitates the acquisition of human activity data with smartwatches and is primarily aimed at researchers who require transparency, full control, and access to raw sensor data. It provides functionality to simultaneously record raw data from four sensorsâ€”tri-axis accelerometer, tri-axis gyroscope, barometer, and GPSâ€”which should enable researchers to, for example, estimate energy expenditure and mine movement trajectories.  A Samsung smartwatch running the Tizen OS was chosen because of 1) the required functionalities of the smartwatch software API, 2) the availability of software development tools and accessible documentation, 3) having the required sensors, and 4) the requirements on case design for acceptance by the target user group.  WEARDA addresses five practical challenges concerning preparation, measurement, logistics, privacy preservation, and reproducibility to ensure efficient and errorless data collection. The software package was initially created for the project â€œDementia back at the heart of the communityâ€, and has been successfully used in that context.",https://github.com/LiacsProjects/Wearda,LiacsProjects/Wearda
d0fda92a-5296-47d0-9e0d-810961fd5662,4tu-software-for-smart-3d-super-resolution-microscopy-reveals-the-architecture-of-the-rna-scaffold-in-a-nuclear-body,"Software for ""Smart 3D super-resolution microscopy reveals the architecture of the RNA scaffold in a nuclear body""","This software repository contains the code accompanying the paper ""Smart 3D super-resolution microscopy reveals the architecture of the RNA scaffold in a nuclear body"". The repository includes a python algorithm to automatically detect and image paraspeckles using an Abberior STED microscope, as well as Matlab code to rotationally and translationally align the resulting paraspeckle images and perform shape and polarity analyses on them.",,
d1a7a0df-a7a8-43af-aded-4c93a80980cb,lilio,Lilio,"Lilio helps you to organize timeseries data in such a way that it can easily be used in a machine learning workflow. It is geared towards predictions of events with (annual) return periods. Lilio distinguishes between target and precursor periods (features) and stacks the timeseries such that targets from different years are treated as independent records (samples).   Concretely, Lilio helps you to:  - Define target and precursor periods - Resample and reshape input data  - Split data into train-test sets - Avoid information leakage between train/test sets",https://github.com/AI4S2S/lilio,AI4S2S/lilio
d1d6624a-06a3-4be8-b77e-7e4be2f9bd61,4tu-artifact-for-the-paper-efficient-formally-verified-maximal-end-component-decomposition-for-mdps,"Artifact for the paper ""Efficient Formally Verified Maximal End Component Decomposition for MDPs""","This artifact contains the proofs of correctness of an algorithm for maximal end component (MEC) decomposition as well as a modified version of mcsta from the Modest Toolset with models to reproduce the benchmarks. The high-level proofs are described in the paper ""Efficient Formally Verified Maximal End Component Decomposition for MDPs"" accepted at FM 2024. The proof is divided into a correctness proof for the abstract algorithm, an proof of correctness for the different data structures and a refinement to LLVM that is done using the Isabelle Refinement Framework. Running the proofs yields an LLVM implementation of the MEC algorithm. Once compiled into a library, it can directly be used by our modified version of mcsta to reproduce the experiments in our paper. To streamline the process, we have provided small scripts that perform basic tasks automatically (e.g. copying, moving, removing files and running mcsta).     This artifact is designed to run on a 64-bit Linux distribution (or Virtual Machine) with at least 16GB, although we recommend 32 GB or more to run the benchmarks. It installations of clang, Isabelle/HOL with the AFP and python3. If you use the Virtualbox virtual machine, these will be preinstalled. This virtual machine is based on the TACAS23-AEC, username and password are tacas23.     The artifact contains a link to a git repository to run the artifact directly on your machine. If you would like to run the artifact in a virtual machine, download the mec-artifact-vm.ova file.",https://data.4tu.nl/v3/datasets/a80bcdc5-8fc3-419f-897c-bfaf18fdd21f.git,
d22657c3-7958-471f-86cd-c9ed91383a11,4tu-conversion-tool-foam-to-nastran,Conversion tool: foam to Nastran,"This is the conversion tool that can be used to convert a Foam mesh into Nastran. There are some softwares such as COMSOL Multiphysics that imports mesh in the Nastran format. The meshing softwares designed for the OpenFOAM generally have the tool to convert into Fluent format but not in the Nastran format. With this tool, the conversion of Hexahedral, Tetrahedral, Pyramid and Prism elements into Nastran format is possible. The tool can be easily extended for other types of elements.",,
d22c72e7-64a8-4afd-b042-d8a99e3b1b6f,eegageprediction,EEG_age_prediction,This is a set of Jupyther notebooks for EEG signal processing. It allows the user to run ML and DL models to predict the chronological age of kids.,https://github.com/eegyolk-ai/EEG_age_prediction,eegyolk-ai/EEG_age_prediction
d2a49dcb-5127-4ab9-9baa-ac7f4b03274a,seahtrue,seahtrue,,https://github.com/vcjdeboer/seahtrue,vcjdeboer/seahtrue
d2ae3978-7d0f-48b2-9f04-c9bde2210c83,dune,DUNE (Distributed and Unified Numerics Environment),"**DUNE**, the *Distributed and Unified Numerics Environment* is a modular toolbox for solving partial differential equations (PDEs) with grid-based methods. It supports the easy, flexible, and efficient implementation of finite element and finite volume methods. DUNE is written in C++, but has interfaces to Python as well.  ![DUNE design](https://www.dune-project.org/img/dunedesign.png)  The underlying idea of DUNE is to create slim interfaces that allow the efficient use of legacy and/or new libraries. Modern C++ programming techniques enable very different implementations of the same concept using a common interface at a very low overhead. Thus, DUNE ensures efficiency in scientific computations and supports high-performance computing applications.  Particular highlights are:   * A generic interface for finite element grids.  This interface allows to use a wide range of very different [grid data structures](https://www.dune-project.org/doc/grids/) from the same code  * An interface for [finite element bases](https://www.dune-project.org/modules/dune-functions/), together with a generic mechanism to construct complicated bases from simpler ones  * The [Iterative Solver Template Library](https://www.dune-project.org/modules/dune-istl/), featuring vector and matrix data structures that allow for many general nesting patterns  DUNE is free software licensed under the GPL (version 2) with a so-called ""runtime exception"" (see [**license**](https://www.dune-project.org/about/license/)). This licence is similar to the one under which the libstdc++ libraries are distributed. Thus, it is possible to use DUNE even in proprietary software.  #### DUNE is based on the following main principles:  * Separation of data structures and algorithms by abstract interfaces: _This provides more functionality with less code and also ensures maintainability and extendability of the framework._  * Efficient implementation of these interfaces using generic programming techniques: _Static polymorphism allows the compiler to do more optimizations, particularly function inlining, allowing the interface to have very small functions (implemented by one or few machine instructions) without a severe performance penalty. In essence, the algorithms are parametrized with a particular data structure, and the interface is removed at compile time. Thus the resulting code is as efficient as if it would have been written for the special case._  * Reuse of existing finite element packages with a large body of functionality: _In particular, the finite element codes UG, ALBERTA, and ALUGrid have been adapted to the DUNE framework. Thus, parallel and adaptive meshes with multiple element types and refinement rules are available. All these packages can be linked together in one executable._   ### Modules  The framework consists of several modules, which are downloadable as separate packages. There is a set of [core modules](https://www.dune-project.org/groups/core/) used by most other packages. The [dune-grid](https://www.dune-project.org/modules/dune-grid/) core module already contains some grid implementation, and further [grid managers](https://www.dune-project.org/groups/grid/) are available as extra modules. Main [discretization modules](https://www.dune-project.org/groups/disc/) provide the infrastructure for solving partial differential equations using DUNE, and are available as separate modules. The modular structure of DUNE allows for using a small set of the modules (e.g., only the solver module [dune-istl](https://www.dune-project.org/modules/dune-istl/) or the module [dune-localfunctions](https://www.dune-project.org/modules/dune-localfunctions/) containing shape functions without, for example, using the [dune-grid](https://www.dune-project.org/modules/dune-grid/) or a full discretization module).  For further information, see the main [features](https://www.dune-project.org/about/features/), read the [documentation for application writers](/groups/tutorial/), or get in touch with the [people actively developing DUNE](https://www.dune-project.org/community/people/).",https://gitlab.dune-project.org/core/dune-common,
d2c37434-418b-4d5e-8d5f-9f3a96a18384,scriptcwl,scriptcwl,"* Create CWL workflows without learning CWL or YAML * Make workflows by programming  * Design workflows interactively in Jupyter notebooks  Scriptcwl is a Python package for creating Common Workflow Language (CWL) workflows by writing a Python script instead of manually typing YAML or using a GUI. To work with scriptcwl, you need to give it a bunch of CWL CommandLineTools. Then you subsequently specify the workflow inputs, steps, and workflow outputs, and save the result to a file. This can be done interactively in a Jupyter notebook. A scriptcwl script to generate a workflow provides a concise and transparent representation of your workflow; one that is much more readable than YAML.",https://github.com/nlesc/scriptcwl,nlesc/scriptcwl
d2c4aee5-4984-42ec-ae56-687029854fc7,grlc,grlc,* Builds a web API for users who need access to triple store data without requiring SPARQL  * Creates an API from SPARQL queries stored on a github repository  * No need to know how to write SPARQL queries,https://github.com/CLARIAH/grlc,CLARIAH/grlc
d2e33d09-42db-413a-9b2e-9e9aabea4ad7,datastream,DataStream,"DataStream Initiative is a registered Canadian charity dedicated to advancing freshwater protection through open data flows. The online DataStream hubs improve access to water quality data in Canada and are delivered in collaboration with regional monitoring networks. Mackenzie DataStream was developed through a unique collaboration between The Gordon Foundation and the Government of the Northwest Territories, Mackenzie DataStream's founding partner.",,
d3183c2f-dd78-4413-929d-35e0a9ce9923,4tu-code-underlying-the-bsc-thesis-efficient-auditory-coding-for-bat-vocalizations,Code underlying the BSc thesis: Efficient Auditory Coding for Bat Vocalizations,"This is the pipeline I used for my Bachelor's Thesis on Efficient Auditory Coding for Bat Vocalizations. It includes four Python notebooks. The first one, 'download', includes scripts for downloading bat recordings from the ChiroVox dataset, as bulk download functionality is not available. The second one, 'preprocessing',  contains analytics about the data distribution, as well as preprocessing steps, such as resampling, denoising and cropping the data into individual bat calls using a custom energy-based approach at bat call detection. Lastly, the third and fourth notebooks, namely 'analysis' and 'results' include tests about the performance of auditory kernels, their relative usage and activation patterns, as well as clustering of bat calls using low-fidelity reconstructions and sparsity experiments.",,
d38281f6-ca97-4c7e-a5e0-15227d7ac953,4tu-codan,CODaN,"Common Objects Day and Night (CODaN) is an image classification dataset for zero-shot day-night domain adaptation / generalization.     The CODaN dataset consists of 15,500 224x224 colour images in 10 classes, with 1,550 images per class. There are 10,000 training images, 500 validation images, 2,500 daytime test images and 2,500 nighttime test images.     The dataset is collected from the excellent&nbsp;COCO,&nbsp;ImageNet&nbsp;and&nbsp;ExDark&nbsp;datasets. All images are filtered and cropped such that they have the same dimensions and are completely mutually exclusive, i.e. do not contain objects of different classes, nor do belong objects to multiple classes.",https://data.4tu.nl/v3/datasets/96065c4f-3aca-4c60-897e-16c7f6a23530.git,
d38c2dd3-d257-453a-b9f3-72f8044b4106,lxcat-schema,LXCat schema,,https://gitlab.com/LXCat-project/lxcat/-/tree/main/packages/schema,
d3968223-a694-402e-84f1-51692fcaa04a,powersensor3,PowerSensor3,"PowerSensor is a tool that measures the instantaneous power consumption of PCIe cards and SoC development boards like GPUs, Xeon Phis, FPGAs, DSPs, and network cards, at sub-millisecond time scale. It consists of a commodity microcontroller, commodity current sensors, and (for PCIe devices) a PCIe riser card. The microcontroller reports measurements to the host via USB. A small host library allows an application to determine its own energy efficiency. The high time resolution provides much better insight into energy usage than low-resolution built-in power meters (if available at all), as PowerSensor enables analysis of individual compute kernels.",https://github.com/nlesc-recruit/PowerSensor3,nlesc-recruit/PowerSensor3
d3a6595d-1fb5-4a9a-a36c-d625e7c5148d,xenon-docker-images,Xenon Docker images,For developers of libraries that need to interact with different file systems and batch schedulers like Xenon. The Docker images in this repository can be used to write integration tests to make sure the library works against a real-life file system or batch scheduler.   For example a developer of a library that submits jobs to a [Slurm batch scheduler](https://slurm.schedmd.com/) can spin up a Slurm v20 Docker container and test if the commands the library sends to Slurm are actually working.   The spinning up of a Docker image in a test suite can be done using [testcontainers](https://testcontainers.org).,https://github.com/xenon-middleware/xenon-docker-images,xenon-middleware/xenon-docker-images
d4f62e8a-83f7-4d2e-b5c6-c6696578f494,orange3-argument-add-on,Orange3 Argument Mining Add-on,"This work is an open-source Python package that implements a pipeline of processing, analyzing, and visualizing an argument corpus and the attacking relationship inside the corpus. It also implements the corresponding GUIs on a scientific workflow platform named [Orange3](https://orangedatamining.com/), so that users with little knowledge of Python programming can also benefit from it.  The package contains three components that can be used to build the workflow:  - Chunker: Split arguments into smaller chunks, learn topics of chunks through topic modeling, measure sentiment and important of chunks within arguments. - Processor: Merge chunks and meta back to arguments, compute coherence and other potential measurements of arguments. - Miner: Build attack network of arguments, label supportive and defeated arguments based on the network structure.",https://github.com/EyeofBeholder-NLeSC/orange3-argument,EyeofBeholder-NLeSC/orange3-argument
d581314b-c2b3-4d3c-b51d-138b20697235,quantum-newton-raphson,Quantum Newton Raphson,,https://github.com/quantumapplicationlab/quantumnewtonraphson,quantumapplicationlab/quantumnewtonraphson
d5f97fb4-c491-4f70-bc4a-2bf6d7d0cb44,4tu-p1-data-combination-code,P1-data-combination-code,"Code to reproduce the figures of the paper ""Changing Sea Level, Changing Shorelines: Integration of Remote Sensing Observations at the Terschelling Barrier Island"". This research combines and compares different types of coastal remote sensing datasets in order to&nbsp;assess the potential of remote sensing techniques to detect a geometrical relationship between sea level rise and shoreline retreat for a case study at the Terschelling barrier island at the Northern Dutch coast. The code is written in python and organized in jupyter notebooks, relying on various open source python libraries.",https://data.4tu.nl/v3/datasets/b8cdd141-1c95-4472-9117-42216bf270f7.git,
d5fb092f-b738-483c-a3f7-52003874b2d5,bridgedb-java,BridgeDb Java,"BridgeDb Java is a library that maps identifiers from one source to another. For example, it can map NCBI Gene identifiers to Ensembl gene identifiers, and DOIs to PubMed identifiers.  For example, the following code shows how identifiers for the ChEBI data source can be used:  ```Java DataSource chebi = DataSource.getExistingByFullName(""ChEBI""); assertNotNull(chebi); assertEquals(""urn:miriam:chebi:1234"", chebi.getMiriamURN(""1234"")); assertEquals(""chebi"", chebi.getCompactIdentifierPrefix()); ```",https://github.com/bridgedb/bridgedb,bridgedb/bridgedb
d68cd53a-5692-4d7c-ab3e-25b664e1aa33,4tu-code-for-the-publication-vibration-induced-friction-modulation-for-a-general-frequency-of-excitation,Code for the publication: Vibration-induced friction modulation for a general frequency of excitation,"MATLAB script to reproduce results from:   E. Sulollari, K. N. van Dalen, and A. Cabboi  ""Vibration-induced friction modulation for a general frequency of excitation""  Journal of Sound and Vibration, Vol. 573, 118200, 2024.      This paper corresponds to Chapter 4 of the dissertation  ""Vibration-induced friction modulation""     This script generates the results shown in the figures of the paper.     Author: Enxhi Sulollari",,
d68fec23-e143-4a02-ac5c-8f405744f9a3,4tu-netlogo-model-agent-based-simulation-of-west-asian-urban-dynamics-impact-of-refugees,"NetLogo model: ""Agent-Based Simulation of West Asian Urban Dynamics: Impact of Refugees""","NetLogo model corresponding to the JASSS article ""Agent-Based Simulation of West Asian Urban Dynamics: Impact of Refugees""",,
d71feea9-090b-49c5-9d4f-4196ddede03e,case-law-app,Case Law App,"* Provides a graphical representation of Dutch case law for researchers * Allows exploring the network of citations between court decisions * Contains an example network about employer liability   The law is a text that describes what a person or legal entity can and cannot do. However, the law is somewhat open to interpretation. This interpretation is done by judges whenever a case is brought to court. Over time, the outcome of individual cases build what is called _case law_.  Because consistency is crucial for the fair application of the law, cases often reference other cases: if the reasoning behind a ruling in one case also applies to another case, then the ruling should be the same. To warrant consistency it is thus paramount that any relevant rulings from previous cases are identified. Conventionally, both the justice department and the defense depend on legal experts to make this identification when preparing a case. However, court rulings are often difficult to understand, there is only limited time available, and even experts are not aware of all cases that may be relevant.  To help mitigate this situation, the Netherlands eScience Center worked together with Maastricht University to develop an interactive visualization that assists the legal community at large (prosecutors, judges, lawyers, legal aids, but also researchers and students) in analyzing case law.  The visualization has been implemented as follows. The collective of cases that together make up case law can be thought of as a network, or _graph_. Each node in the graph represents a case, while the edges represent references to other cases. The graph is represented visually to make it easy for a non-technical user to discover which cases are related. By using metrics from graph theory, such as _in-degree_, _out-degree_, _betweenness_, etc., the nodes in the graph can be annotated with additional information. The additional information helps a researcher quickly assess a node's importance.",https://github.com/NLeSC/case-law-app,NLeSC/case-law-app
d753df97-ca8a-4314-87ba-03845d3809a6,4tu-momenttoolbox,MomentToolbox,"Repository containing several implementations for the calculation of Orthogonal Moments. This includes the most commonly used: discrete orthogonal moments, continuous orthogonal moments defined on a disk, and continuous orthogonal moments defined on a square cartesian grid. The majority of the moments are implemented recursively, enabling the computation of high orders.",https://data.4tu.nl/v3/datasets/c44c840a-7a12-4100-87b5-413c33385534.git,
d78ac9a7-8f34-48fa-a2e8-74869b2fc4f1,vantage6,Vantage6,"* Provides infrastructure for running analyses in a federated setting * Enables parties to set up trusted collaborations to allow federated analyses * Enables developers to create their own algorithms for privacy-preserving analysis  Answering many of the questions in health care often requires incorporating data that are located at different sources. Typically, data from different parties are analyzed by centralizing them. That is, data are brought to where the algorithms are. Unfortunately, this raises heavy concerns regarding the privacy and security of sensitive patient data.  Federated learning has emerged as a technological solution to address these concerns. In this novel paradigm, algorithms are brought to where the data are. This way, we can maximize the potential of multiple datasets while minimizing data leaks and privacy risks.  This led us to create vantage6, our open-source platform for supporting the development of federated learning projects.  Additional repositories: * https://github.com/IKNL/vantage6-server * https://github.com/IKNL/vantage6-node * https://github.com/IKNL/vantage6-client",https://github.com/vantage6/vantage6,vantage6/vantage6
d7f57f0e-6305-4342-a6d7-9f56a753e855,4tu-so-crystal,SO-CRYSTAL,"This code creates simulations of the impact of Na gamma rays and Muons onto a scintillator crystal based on the MTD-BTL (Minimum Ionizing Particle Detector - Barrel TIming Layer, https://cds.cern.ch/record/2667167/files/CMS-TDR-020.pdf) detector to be installed at the CMS (Compact Muon solenoid) at CERN to estimate its overall light collection depending on the crystal's shape and reflective surfaces. The code works with a NSGAII genetic algorithm to study the shape optimization of the crystals. To use the Gmsh integration use the branch from the git repository: https://github.com/grealesguti/SO-CRYSTAL/tree/g/f/G4asfunction/src/TierII-gmsh",https://data.4tu.nl/v3/datasets/3b871054-ca23-4e54-be1c-7ebe268223cb.git,
d82a2f05-a1e8-4a3a-9ad2-8a525e46a648,gemdat,gemdat,,https://github.com/GEMDAT-repos/GEMDAT,GEMDAT-repos/GEMDAT
d91192fe-2498-4335-8515-4988f10df04f,chemtools,ChemTools,"Conceptual tools (mostly density-based tools will be considered here, so this is often called â€œconceptual DFTâ€) can be divided into three main categories:  **Global tools:** There is one number for the entire molecule. Examples: Energy, ionization potential, electron affinity, chemical potential, chemical hardness, chemical softness, hyper-hardness, hyper-softness, electrophilicity, nucleophilicity. It provides a quantitative expression for the intrinsic reactivity of reagents in terms of the properties of the isolated molecules.  **Local tools:** Every point in space, r, has a value. Examples: electron density, electrostatic potential, Fukui function.  **Nonlocal tools:** There is a value for pairs (or triples, quadruples, etc.) of points. For example, the linear response function measure the change in electron density at r due to a change in external potential at râ€™.",https://github.com/theochem/chemtools,theochem/chemtools
d93ded1a-d4b7-467a-a80b-81b548c9bdce,4tu-software-underlying-the-publication-modelling-communication-enabled-traffic-interactions,Software underlying the publication: Modelling communication-enabled traffic interactions,"Simulation environment and implementation of the model as presented in the publication ""Modelling communication-enabled traffic interactions""",,
d9444eb9-7cd7-4feb-b784-5f9ba522f838,rivet,Rivet,"Rivet is a system for preservation of particle-collider analysis logic, analysis reinterpretation via MC simulations, and the validation and improvement of Monte Carlo event generator codes. It covers all aspects of collider physics, from unfolded precision measurements to reconstruction-level searches, and physics from the Standard Model to BSM theories, and from perturbative jet, boson and top-quarks to hadron decays, inclusive QCD, and Heavy Ion physics.  Rivet is the most widespread way by which analysis code from the LHC and other high-energy collider experiments is preserved for comparison to and development of future theory models. It is used by phenomenologists, MC generator developers, and experimentalists on the LHC and other facilities. Coding analyses in Rivet is a great way to publish executable code that extends the longevity, relevance, and impact of your publications!",https://gitlab.com/hepcedar/rivet,
d9599911-f0ef-401b-b63f-bf42f7b89765,tsdf,tsdf,"| Badges | | |:----:|----| | **Packages and Releases** | [![Latest release](https://img.shields.io/github/release/biomarkersparkinson/tsdf.svg)](https://github.com/biomarkersparkinson/tsdf/releases/latest) [![PyPI](https://img.shields.io/pypi/v/tsdf.svg)](https://pypi.python.org/pypi/tsdf/)  [![Static Badge](https://img.shields.io/badge/RSD-tsdf-lib)](https://research-software-directory.org/software/tsdf) | | **Build Status** | [![](https://img.shields.io/badge/python-3.9+-blue.svg)](https://www.python.org/downloads/) ![Python package](https://github.com/biomarkersparkinson/tsdf/workflows/Python%20package/badge.svg) [![pytype Type Check](https://github.com/biomarkersParkinson/tsdf/actions/workflows/pytype-checking.yml/badge.svg)](https://github.com/biomarkersParkinson/tsdf/actions/workflows/pytype-checking.yml) | | **DOI** | [![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.7867899.svg)](https://doi.org/10.5281/zenodo.7867899) | | **License** |  [![GitHub license](https://img.shields.io/github/license/biomarkersParkinson/tsdf)](https://github.com/biomarkersparkinson/tsdf/blob/main/LICENSE) | | **Fairness** |  [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/8083/badge)](https://www.bestpractices.dev/projects/8083) |  A package to work with TSDF data in Python. This implementation is based on the the TSDF format specification, which can be found in this [preprint](https://arxiv.org/abs/2211.11294).  ## What is TSDF data?  TSDF provides a unified, user-friendly format for both numerical sensor data and metadata, utilizing raw binary data and JSON-format text files for measurements/timestamps and metadata, respectively. It defines essential metadata fields to enhance data interpretability and exchangeability, aiming to bolster scientific reproducibility in studies reliant on digital biosensor data as a critical evidence base across various disease domains.   ## Example: TSDF Metadata  This example demonstrates a TSDF metadata JSON file, showcasing the structured format used to easily interpret and read the corresponding binary data. For more intricate examples and detailed specifications, the paper serves as a comprehensive reference.  ```json {     ""study_id"": ""voicedata"",     ""subject_id"": ""recruit089"",     ""device_id"": ""audiotechnica02"",     ""endianness"": ""little"",     ""metadata_version"": ""0.1"",     ""start_iso8601"": ""2016-08-09T10:31:00.000+00:00"",     ""end_iso8601"": ""2016-08-10T10:31:30.000+00:00"",     ""sampling_rate"": 44100,     ""rows"": 1323000,     ""channels"": [         ""left"",         ""right""     ],     ""units"": [         ""unitless"",         ""unitless""     ],     ""compression"": ""none"",     ""data_type"": ""int"",     ""bits"": 16,     ""file_name"": ""audio_voice_089.raw"" } ``` **Explanation:**  - `study_id`: Identifies the study as ""voicedata"".  - `subject_id`: Specifies the subject as ""recruit089"".  - `device_id`: Indicates the device used as ""audiotechnica02"".  - `endianness`: Specifies the byte order as ""little"".  - `metadata_version`: Denotes the metadata version as ""0.1"".  - `start_iso8601` and `end_iso8601`: Define the start and end timestamps of data collection in ISO 8601 format.  - `sampling_rate`: Represents the data sampling rate as 44,100 samples per second.  - `rows`: Specifies the number of data rows as 1,323,000.  - `channels`: Lists the data channels as ""left"" and ""right"".  - `units`: Specifies the units for each channel as ""unitless"".  - `compression`: Indicates that no compression has been applied to the data.  - `data_type`: Defines the data type as ""int"".  - `bits`: Specifies the bit length as 16.  - `file_name`: Names the binary file ""audio_voice_089.raw"" that contains the described data.    ## The python library - `tsdf` This Python library facilitates the manipulation of Time Series Data Format (TSDF) metadata and binary files, providing users with a familiar and structured interface. Leveraging the power of numpy arrays, it simplifies the process of working with TSDF data, allowing users to efficiently read, write, and manipulate both metadata and binary data. This approach enhances data management and analysis, making it a valuable tool for researchers and data scientists dealing with extensive physiological sensor data.   The package is available in [PyPI](https://pypi.org/project/tsdf/).",https://github.com/biomarkersParkinson/tsdf,biomarkersParkinson/tsdf
d9619161-88ae-40ef-b393-99d2e8223083,4tu-predictive-traffic-signal-control,Predictive_Traffic_Signal_Control,"This prototyping environment is made available to evaluate predictive traffic signal control applications in a 'controlled' real-time simulation environment to gain insights for development and design of these predictive control systems in real-life.    The prototyping environment is developed as part of a PhD project to study predictive traffic signal control systems under uncertainties. PhD thesis: M.C. Poelman, ""Predictive Traffic Signal Control under Uncertainty: Analyzing and Reducing the Impact of Prediction Errors,"" Delft University of Technology, 2024.    The research project is a cooperation between DiTTlab&nbsp;of Delft University of Technology and Royal HaskoningDHV. The project is part of the overall project MiRRORS - Multiscale integrated traffic observatory for large road networks, sponsored by the Dutch Foundation for Scientific Research NWO / Applied Sciences under grant number 16270.    Contact: Muriel Verkaik-Poelman (muriel.verkaik-poelman@rhdhv.com)",https://data.4tu.nl/v3/datasets/7397b7df-8b98-417c-97ec-0cce48a912b3.git,
d9e09a62-0cae-4d30-9d43-a0befa57be82,4tu-probabilistic-adequacy-constrained-planning-of-offshore-mmc-hvdc-based-wind-power-systems,Probabilistic-adequacy-constrained-planning-of-offshore-MMC-HVDC-based-wind-power-systems,"This repository contains MATLAB functions for the probabilistic adequacy-constrained planning of offshore MMC-HVDC-based wind power systems. The research objective is to develop models and evaluation tools that quantify reliability by integrating the stochastic nature of wind generation, component failures, and maintenance strategies. The work follows a probabilistic research approach, employing methods such as state enumeration, continuous time Markov chains, convolution of probability distributions, and reliability evaluation. Data are obtained through analytical probabilistic modelling of wind farms, Modular Multilevel Converters (MMCs), DC stations, and Multi-Terminal DC (MTDC) networks, producing capacity output probability distributions, transition matrices, and key system cost and reliability indices.",https://data.4tu.nl/v3/datasets/f6f4728b-6ab5-4c6b-9363-cfdfedf57ddc.git,
d9efa65d-d085-413e-ba08-0040b3772056,orange3-story-navigator,Orange3 Story Navigator,The Orange3 Story Navigator can help you with the analysis of digital texts with elements from narrative psychology theory like Burke's pentad.,https://github.com/navigating-stories/orange-story-navigator,navigating-stories/orange-story-navigator
da83789c-4b9c-40b6-9409-bb0b769a04ed,icos-cp-python-library,ICOS CP python library,"This library provides easy access to data hosted at the ICOS Carbon Portal (https://data.icos-cp.eu/). By using this library you can load data files directly into memory. The approach of this library is to free you from downloading and maintaining a local copy of data files and if you choose to use our Jupyter Hub services, you don't even need computational power.  If you would rather stick to the conventional ""download the data approach"", to store and use the data locally, we suggest you go to the data portal website to find and ""download"" the data.  Please be aware, that by either downloading data or accessing data directly through this library, you agree and accept, that all ICOS data is provided under a CC BY 4.0 licence  (licence.pdf)",https://github.com/ICOS-Carbon-Portal/pylib,ICOS-Carbon-Portal/pylib
dae8bd7d-224d-4913-81ed-99560a5b29a6,4tu-group-trip-planning,group-trip-planning,"This repository contains the interface developed to support a research project published in the following paper. The project involved an empirical study exploring how human decision-makers interact with AI systems when dealing with complex and uncertain tasks. The corresponding code for the interface and the full list of tasks are available here.  Sara Salimzadeh and Ujwal Gadiraju. 2024. When in Doubt! Understanding the Role of Task Characteristics on Peer Decision-Making with AI Assistance. In Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization (UMAP '24). Association for Computing Machinery, New York, NY, USA, 89â€“101.&nbsp;https://doi.org/10.1145/3627043.3659567",https://data.4tu.nl/v3/datasets/d2389b4b-ac15-4064-b331-6ebeb1087b36.git,
db4456b3-492b-4b3b-9e93-beba41d26fec,ibridges-gui,iBridges-GUI,,https://github.com/iBridges-for-iRods/iBridges-GUI,iBridges-for-iRods/iBridges-GUI
db926bd5-f069-42f6-acca-f65e3dba32c4,4tu-supplementary-materials-for-the-article-towards-future-pedestrian-vehicle-interactions-introducing-theoretically-supported-ar-prototypes,Supplementary materials for the article: Towards future pedestrian-vehicle interactions: Introducing theoretically-supported AR prototypes.,"Supplementary data for the paper Tabone, W., Lee, Y.M., Merat, N., Happee, R., & De Winter, J.C.F. (2021). Towards future pedestrian-vehicle interactions: Introducing theoretically-supported AR prototypes. Data includes the code used to build the AR concepts, the supplementary video demonstrating the concepts working in a real environment, and photographs of the concepts operating on an iPad in situ.",,
dbf69d7c-5764-4b0c-b46b-2be39b61c727,4tu-matlab-code-for-a-monte-carlo-simulation-underlying-the-master-thesis-new-breakwater-of-genoa,MATLAB code for a Monte-Carlo simulation underlying the master thesis: New breakwater of Genoa,"MATLAB code for a Monte-Carlo simulation of a vertical breakwater. 10 failure modes are included: shoreward sliding, seaward sliding, shoreward overturning, seaward overturning, bearing failure, toe instability, berm instability, bottom scour, wave overtopping and wave transmission.",,
dc20f5ad-230f-4d84-9acb-289b319d4c7a,teachbook-recombiner,Teachbook recombiner,"A TeachBook is a website with educational material. The material source can be Jupyter Notebooks or Markdown files. The website can be configured to run the notebooks interactively in the browser.  There are [dozens of TeachBooks](https://teachbooks.io/gallery/) out there, to make material for you own course material you can now use the [TeachBook recombiner](https://teachbooks.io/recombiner/) to pick and choose chapters from other TeachBooks.",https://github.com/TeachBooks/recombiner,TeachBooks/recombiner
dc7fcb6c-49bb-4b0c-a647-d275b7706bc2,couplednode,CoupledNODE,,https://github.com/DEEPDIP-project/CoupledNODE.jl,DEEPDIP-project/CoupledNODE.jl
dc8a1aed-ce50-4e87-81c2-2b1844099f3d,cffconvert-github-action,cffconvert GitHub Action,- validate and convert CITATION.cff files as part of your Continuous Integration on GitHub Actions,https://github.com/citation-file-format/cffconvert-github-action,citation-file-format/cffconvert-github-action
dc9461f8-474f-4dfa-94df-20aaef2e3530,virus-seq,Virus Seq,"**VirusSeq Data Portal:** The VirusSeq Data Portal was designed and developed by the Genome Informatics Team at the Ontario Institute for Cancer Research, led by Dr. MÃ©lanie Courtot, Director of Genome Informatics and Dr. Lincoln Stein, Head of Adaptive Oncology. The software is powered by Overture, an open-source software suite for managing and sharing data at scale on compute clouds.",https://github.com/cancogen-virus-seq/portal,cancogen-virus-seq/portal
dcba6a3a-58ff-4191-b715-bd0141e423af,pbi,PBI,"The Patent Breakthrough innovation (PBI) package can be used to identify breakthrough innovations in patents. The docembedding Python package contains a variety of methods for creating document embeddings. We have optimized and tested these methods for their ability to predict similarity between patents. This was done by maximizing the cosine similarity between patents that are classified into the same technology class, and minimizing cosine similarity between patents that fall into different technology classes. These methods with optimized parameters are then used to create document embeddings. From these embeddings, novelty scores are created using cosine similarities between the focal patent and patents in the previous n years and subsequent n years.",https://github.com/UtrechtUniversity/patent-breakthrough,UtrechtUniversity/patent-breakthrough
ddb1973f-d523-48d2-ae25-7efde464ac49,atlascine,Atlascine,"The main aim of Atlascine 4 is to streamline the process of transforming audiovisual material into a digital map and to enable a direct interface between audiovisual material, text and the map. With the latest version of the platform, researchers in many disciplines can use the map not only as a way to visually synthesize large volumes of annotated stories and to identify new patterns and structures in these stories, but also to use the map to navigate within the stories themselves and find specific events that link stories and places.  Atlascine 4 was designed in close collaboration with the Geomatics and Cartographic Research Center (GCRC) at Carleton University (Ottawa). The tool builds off of its previous versions by streamlining the process of transforming digital text (e.g. transcript of audiovisual material) to a digital map and to enable full interaction between these texts, the audio/video files from which they are derived, and their maps.",https://github.com/geomedialab/atlascine,geomedialab/atlascine
ddfc9768-ec1a-427b-afd9-c6ae94d3ea66,sv-gen,sv-gen,"- Generates artificial short-read alignments including different types of structural variants (SVs) - Enables scalable and portable genome simulations across HPC clusters (e.g., using GridEngine, Slurm etc.) or compute clouds - It's easy to use, deploy and extend with new tools    This [Snakemake](https://snakemake.readthedocs.io)-based workflow generates artificial short-read alignments based on a reference genome with(out) SVs. The workflow is easy to use and deploy on any Linux-based machine. In particular, the workflow supports automated software deployment, easy configuration and addition of new analysis tools as well as enables to scale from a single computer to different HPC clusters with minimal effort.",https://github.com/GooglingTheCancerGenome/sv-gen,GooglingTheCancerGenome/sv-gen
ddfd3168-40a7-4b34-a287-16910a007b24,cffconvert,cffconvert,"- command line tool written in Python - simple installation - read CFF files from GitHub URLs or local files - convert to APA-like, BibTeX, EndNote, RIS, codemeta, schema.org, or Zenodo JSON - validate the CFF against the schema",https://github.com/citation-file-format/cff-converter-python,citation-file-format/cff-converter-python
de08b1de-7f24-492a-9821-5253eabdd22e,pybacting,pybacting,"## Usage  Based on the example from the Bacting page, you can do:  ```python from pybacting import cdk  print(cdk.fromSMILES(""COC"")) ```  Or you can use some of the more pythonic functions that wrap the functions exposed through the `pybacting.cdk` object:  ```python import pybacting  print(pybacting.from_smiles(""COC"")) ```",https://github.com/BlueObelisk/pybacting,BlueObelisk/pybacting
de38bb8b-e5af-4beb-9304-89667ecf7296,limesurvey,LimeSurvey,"<div id=""top""></div> <p align=""center""> <img src=""https://www.limesurvey.org/images/limesurvey/svg/logo_limesurvey_head.svg"" width=""400"" alt='LimeSurvey Logo' /> </p>  # LimeSurvey: The worldâ€™s #1 open-source survey platform  It's what we love and do best since 2006...  ðŸŒ [Website](https://www.limesurvey.org) Â· ðŸ”® [Demo](https://demo.limesurvey.org/admin) Â· ðŸ“š [Documentation](https://www.limesurvey.org/manual) Â· âš™ï¸ [Request a feature](https://bugs.limesurvey.org) Â· ðŸ› [Report a bug](https://bugs.limesurvey.org) Â· ðŸ—¨ï¸ [Forums](https://forums.limesurvey.org) Â· ðŸ—¨ï¸ [Discord](https://discord.gg/DEjguXn)  [LimeSurvey](https://www.limesurvey.org) is a free and open-source online survey platfrom used by businesses of all sizes, professionals, academic institutions, teachers, students, governments, financial institutes, and [Anja](https://www.linkedin.com/in/anja-meinders-ba3b29213/) from HR (who is hiring and looking for a **DevOps** and **Technical Support Engineer/Developer**  â€“ [drop her a Limeâ€¦](https://www.linkedin.com/in/anja-meinders-ba3b29213/)) in 80+ countries worldwide. It offers features like conditional logic, question branching, customizable templates, multilingual support, and GDPR compliance.  ## ðŸŒŸ Why LimeSurvey?  LimeSurvey is perfect for you if you are...  - ðŸ“Š Gathering feedback from customers, employees, or stakeholders - ðŸŽ“ Conducting academic research or field studies - ðŸŒ Running multilingual surveys for global audiences - ðŸ” Prioritizing privacy and GDPR compliance in data collection - ðŸ›  Needing customizable, flexible survey platform for various projects - ðŸš€ Scaling surveys for startups, mid-sized businesses, or large organizations - ðŸŽ¯ Looking for an open-source solution with no vendor lock-in  ## ðŸš€ Features  - âœ… **Unlimited surveys** - âœ… **Unlimited questions** - âœ… **30+ question types** - âœ… **900+ survey templates** - âœ… **Easy LimeSurvey editor** - âœ… **On-brand surveys** (fonts, colors, logo, CSS, JavaScript) - âœ… **Multilingual surveys** (80+ languages) - âœ… **Skip logic and question branching** - âœ… **Easy survey sharing** (via public link, QR code, socials) - âœ… **Closed access mode** (via personal link, invite-only) - âœ… **Responses & statistics** - âœ… **Advanced data analysis tools** - âœ… **RemoteControl API** (via XML-RPC / JSON-RPC) - âœ… **Google Analytics** - âœ… **Data security & anonymization** - âœ… **Two-factor** authentication - âœ… **GDPR compliance** and strong data security - âœ… **WCAG 2.0 compliance** ðŸŒðŸ’š - âœ… **Plugins** (questions themes, survey themes, audit log, ExportR, ExportSTATAxml, AuthCAS ...) - âœ… **Integrations** (SAML, LDAP, SURFconext, Remote Control, REST API ...) - âœ… **And much more ...**",https://github.com/LimeSurvey/LimeSurvey,LimeSurvey/LimeSurvey
dee072c2-37ff-4c30-b834-0d8f5e70c3e4,storyboards,Storyboards,"* Showcase project outputs formatted as storyboards * Authoring stories in simple markdown format * Hosted for free on GitHub pages   Originally used by movie directors, a storyboard consists of a series of images that together form a story. These storyboards have nothing to do with filmmaking, but the term still seems appropriate to convey their essence.  The storyboards somehow hold the middle between a scientific poster and a traditional slide-deck presentation. The inspiration for this particular layout comes from R's flexdashboard. The main content is shown prominently, with room for annotations on the side. This makes it great as a stand-alone presentation format.   Moreover, the layout is optimized for (interactive) viewing on a computer screen. The top bar allows the user to navigate the different ""chapters"", and the static images that make up the main content can be replaced by interactive visualizations. This makes the storyboard a modern alternative to scientific posters or slide decks.",https://github.com/eucp-project/storyboards,eucp-project/storyboards
dee2283f-bd09-4e1d-8ba6-7642eaea97b1,fastmlc,FastMLC,* Provides an efficient and high precision clustering and embedding of large datasets followed by interactive 3D visualization * The only tool of this kind,https://github.com/FastMLC/fMLC,FastMLC/fMLC
dee48fbe-abe6-4105-8c64-9924c3a59003,4tu-ncs-sedimentanalysis,NCS_SedimentAnalysis,"This dataset contains all the code and data underlying the publication ""Subsurface Sediment Properties and Potential Impacts of Marine Sand Extraction on Sand Wave Occurrence on the Netherlands Continental Shelf"". In this study, we analysed existing sediment samples of the Netherlands Continental Shelf i) to find characteristic values for the median grain size and mud content of the current surface sediment; ii) to quantify the typical difference of these properties between the current surface layer, and a layer that might become exposed due to sediment extraction; and iii) to explore how the exposure of this layer mayimpact the occurrence of sand waves on the Netherlands Continental Shelf.  This folder contains all the input data (stored in ""Data/org"") as well as all analysis scripts and functions. For more details on how to use the scripts and code, as well as the included data, we refer to the Documentation (""AA Documentation/Documentation.pdf"").",https://data.4tu.nl/v3/datasets/f0205de4-b530-4cda-9469-333d019cd46a.git,
def1c6d9-c2c9-4429-b184-69d1f80d2fbc,4tu-propagation-model-and-pie-model-underlying-the-master-thesis-knowledge-in-building-with-nature-pilot-projects,"Propagation model and PIE model, underlying the master thesis:  Knowledge in Building with Nature pilot projects","Two Vensim System Dynamics model on the generation, use and propagation of knowledge within Building with Nature pilot projects. These model were used in the EPA Master thesis: Knowledge in Building with Nature pilot projects",,
df34293f-8ff2-476a-baac-ef964c564261,dumme,DumME,DumME (Mixed-Effects Dummy Model) is an adaptation of MERF (https://github.com/manifoldai/merf). The main difference is that this version is fully compliant with the scikit-learn API.  Other difference include:  * The name: MERF was renamed to the more general MixedEffectsModel * The default fixed-effects model: dummy model instead of random forest * The package structure: stripped down to its core and then upgraded to use modern standards * Test suite: using pytest instead of unittest,https://github.com/phenology/dumme,phenology/dumme
df399af2-8186-40f6-bf5c-d7c48d2d786b,ireceptor,iReceptor Gateway,"# What is iReceptor? iReceptor is a data discovery platform that facilitates the curation, analysis and sharing of antibody/B-cell and T-cell receptor repertoires (Adaptive Immune Receptor Repertoire or AIRR-seq data) from multiple labs and institutions. We are committed to providing a platform for researchers to increase the value of their data through sharing with the community. This will greatly increase the amount of data available to answer complex questions about the adaptive immune response, accelerating the development of vaccines, therapeutic antibodies against autoimmune diseases, and cancer immunotherapies. Visit the COVID-19 page for a list of curated COVID-19 AIRR-seq studies.  # Our Approach The iReceptor Gateway integrates large, distributed, AIRR-seq data repositories, following standards for sharing and interoperability developed by the Adaptive Immune Receptor Repertoire (AIRR) Community. The main goal of iReceptor is to connect this distributed network of AIRR-seq repositories into an AIRR Data Commons, allowing queries across multiple projects, labs, and institutions.  Present functionalities include: * Search for repertoires satisfying certain metadata (e.g. find all AIRR-seq repertoires from ovarian cancer studies) * Search for all repertoires that contain specific CDR3 sequences * Search identified repertoires for sequences derived from particular V, D, and J genes and alleles * Download sequences from these repertoires in AIRR.tsv format, easily importable to other AIRR-seq analysis tools  iReceptor is a member of the iReceptor Plus Consortium. This project will expand the analysis tools available on the Gateway, including for single cell or systems immunology approaches, and add security for iReceptor Plus repositories.  Users of the iReceptor Gateway can search AIRR-seq data from studies on ovarian cancer, autoimmune diseases such as MS and SLE, and infectious diseases such as COVID-19 and HIV.  Visit the iReceptor Studies pageÂ for a list of curated studies.  # Services iReceptor Turnkey - [Source code](https://github.com/sfu-ireceptor/turnkey-service-php) iReceptor Service (PHP/MongoDB) - [Source code](https://github.com/sfu-ireceptor/service-php-mongodb) iReceptor Data Migration Service - [Source code](https://github.com/sfu-ireceptor/dataloading-mongo) iReceptor Database Service - [Source code](https://github.com/sfu-ireceptor/repository-mongodb) iReceptor AIRR Mapping Configuration File - [Source code](https://github.com/sfu-ireceptor/config) iReceptor Data Curation - [Source code](https://github.com/sfu-ireceptor/dataloading-curation) ireceptor Monitor - [Source code](https://github.com/sfu-ireceptor/ireceptor-monitor)",https://github.com/sfu-ireceptor/gateway,sfu-ireceptor/gateway
df82de99-b556-41ef-a46b-ffd4161dab08,orbitn-symplectic-integrator-for-near-keplerian-planetary-systems,orbitN,"orbitN is a second order symplectic integrator developed with the primary goal of generating accurate and reproducible long-term orbital solutions for near-Keplerian planetary systems with a dominant mass M0. Among other features, orbitN includes M0's quadrupole moment, a lunar contribution, and post-Newtonian corrections (1PN) due to M0 (fast symplectic implementation). To reduce numerical roundoff errors, orbitN features Kahan compensated summation. orbitN version 1.0 focuses on hierarchical systems without lose encounters but can be extended to include additional features in future versions. orbitN version 1.0 uses Gaussian units, i.e., length, time, and mass are expressed in units of au, days, and fractions of M0.",https://github.com/rezeebe/orbitN,rezeebe/orbitN
dfa2c76f-bd53-4cd5-a683-4ea06898f8fe,4tu-code-and-data-underlying-the-publication-city3d-large-scale-building-reconstruction-from-airborne-lidar-point-clouds,Code and data underlying the publication: City3D: Large-scale Building Reconstruction from Airborne LiDAR Point Clouds,"It implements the hypothesis-and-selection-based building reconstruction method. In this research, we generate a new dataset consisting of the point clouds and reconstructed surface models of 20k real-world buildings.",https://data.4tu.nl/v3/datasets/7c0e1c11-b9c9-485a-af43-16b0288c17e6.git,
e00591f9-6feb-41cf-b6d7-f10c76122a29,pyspextools,Pyspextools,The package contains tools to create and manipulate spectra in SPEX format and tools to create user defined models.,https://github.com/spex-xray/pyspextools,spex-xray/pyspextools
e0288ca1-f17e-4c92-919b-e522b3893c98,measurementsjl,Measurements.jl,,https://github.com/JuliaPhysics/Measurements.jl,JuliaPhysics/Measurements.jl
e02e043e-3b94-4c5c-9517-5f0c132bbc61,2024-jarigsma-001,Preferential Attachment Network,"This model is an algorithm implemented in NetLogo.  Users can choose how many (unattached) nodes to begin the algorithm with.  Node size is proportional to the number of links the node has.  <img width=""622"" alt=""Screenshot 2024-11-15 at 22 28 25"" src=""https://github.com/user-attachments/assets/b7b7b9fa-aeb2-49ce-bd3d-ea9482c05f2f"">    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |initial-number-nodes|integer|the number of unconnected nodes existing before preferential attachment is applied.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |network|object (agent and link sets)|nodes (turtles, breed nodes) and edges (links) between nodes, formed with iterations of preferential attachment.| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2024-Jarigsma-001,
e09f2f9b-b0aa-4286-b51a-41ee06c29d44,validsense,ValidSense,"## Description The ValidSense toolbox aims to assess the agreement between two quantitative methods or devices measuring the same quantity  using the Limits of Agreement analysis (LoA analysis), also known as the Bland-Altman analysis, using four existing variants.  Moreover, a Longitudinal analysis is developed to assess agreement over time.  ## Usage ValidSense is a webbased application, consisting of five pages. Follow the instructions on these pages sequentially.  Detailed instructions are given in the web interface. This is a summary of the usage of the five pages.  1. Introduction: Information about the ValidSense software and the statistical methods is given here 2. Loading: Files must be uploaded here, and some filtering settings can be made. Use CSV or XLSX files 3. Preprocessing: The analysis variant is chosen, as well as variables and settings. Variants available:    - Limits of Agreement:       - Classic: Assess agreement in single pair of measurements per patient, [Bland & Altman 1986](https://pubmed.ncbi.nlm.nih.gov/2868172/)      - Repeated measurements: Assess agreement in multiple measurements per patient,  	   [Bland & Altman 1999 section 5.2](https://pubmed.ncbi.nlm.nih.gov/10501650/), [Bland & Altman 2007 section 3](https://pubmed.ncbi.nlm.nih.gov/17613642/)       - Mixed-effects: Assess agreement based on the mixed-effects LoA analysis, allowing to correct, for example,  	   multiple measurements per patient or systematic relationship between the difference and mean, [Parker et al. 2016](https://pubmed.ncbi.nlm.nih.gov/27973556/)      - Regression of difference: Assess agreement in a single measurement per patient, with a linear relationship between difference and mean for bias  	   and/or LoA, [Bland & Altman 1999 section 3.2](https://pubmed.ncbi.nlm.nih.gov/10501650/)    - Longitudinal analysis: a new method to assess agreement over time. 5. Limits of Agreement Analysis: The Bland-Altman plot is shown, along with information helping to check assumptions of the analysis 6. Longitudinal Analysis: The newly developed Longitudinal analysis can assess non-constant agreement over time.    The longitudinal analysis involves breaking down a dataset into smaller parts over time and applying existing LoA analysis to each part.    A moving time window is applied, and based on the data included in the window, the bias and 95% LoA are calculated for each time window.    The agreement plot graphical visualises the results of the longitudinal analysis. A window can be selected using the time slider,     shown in the Bland-Altman plot of a selected window, and allows the user to navigate through the time domain.    In addition traditional Time series plots are also available (independent from the Longitudinal analysis),     to show the datapoints and trendlines of each cluster.",https://github.com/umcu/ValidSense,umcu/ValidSense
e0a3c2ae-ae3c-4f97-ad59-a4a9a30b4178,qubops,qubops,,https://github.com/quantumapplicationlab/qubops,quantumapplicationlab/qubops
e0a64f62-62e3-4668-8291-f842021eaad6,ienvironment,iEnvironment,,,
e0ae3961-8738-4933-9638-d73211b9e79c,harmony,Harmony,"# Harmony  Do you need to compare questionnaire items across studies? Do you want to find the best match for a set of items? Are there are different versions of the same questionnaire floating around and you want to make sure how compatible they are? Are the questionnaires [written in different languages](https://harmonydata.ac.uk/harmony-supports-over-8-languages/) that you would like to compare?  _Here's a walkthrough video on how you can use Harmony online at harmonydata.ac.uk. Click to view:_  [![Harmonising questionnaires](https://github.com/harmonydata/.github/raw/main/profile/video.jpg)](https://www.youtube.com/watch?v=cEZppTBj1NI ""Harmonising questionnaires"")  The Harmony project is a data harmonisation project that uses [Natural Language Processing](https://fastdatascience.com/guide-natural-language-processing-nlp/) to help researchers make better use of existing data from different studies by supporting them with the harmonisation of various measures and items used in different studies. Harmony is a collaboration project between [Ulster University](https://ulster.ac.uk/), [University College London](https://ucl.ac.uk/), the [Universidade Federal de Santa Maria](https://www.ufsm.br/), and [Fast Data Science](http://fastdatascience.com/). Harmony is funded by [Wellcome](https://wellcome.org/) as part of the [Wellcome Data Prize in Mental Health](https://wellcome.org/grant-funding/schemes/wellcome-mental-health-data-prize).  Harmony is a project in active development and you can [contribute](https://github.com/harmonydata/harmony).  If you have found a bug or would like a new feature, you can [raise an issue here for issues with Harmony's natural language understanding functionality](https://github.com/harmonydata/harmony/issues), or alternatively [here for issues with Harmony's user interface and graphics](https://github.com/harmonydata/app/issues). You can also join [our Discord server](https://discord.gg/harmonydata)!  # What does Harmony do?  * Psychologists and social scientists often have to match items in different questionnaires, such as ""I often feel anxious"" and ""Feeling nervous, anxious or afraid"".  * This is called **harmonisation**. * Harmonisation is a time consuming and subjective process. * Going through long PDFs of questionnaires and putting the questions into Excel is no fun. * Enter [Harmony](https://harmonydata.ac.uk/app), a tool that uses [natural language processing](naturallanguageprocessing.com) and generative AI models to help researchers harmonise questionnaire items, even in different languages.  # Quick start with the code  [Read our guide to contributing to Harmony here](https://harmonydata.ac.uk/contributing-to-harmony/) or read [CONTRIBUTING.md](./CONTRIBUTING.md).  You can run the walkthrough Python notebook in [Google Colab](https://colab.research.google.com/github/harmonydata/harmony/blob/main/Harmony_example_walkthrough.ipynb) with a single click: <a href=""https://colab.research.google.com/github/harmonydata/harmony/blob/main/Harmony_example_walkthrough.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>  You can also download an R markdown notebook to run in R Studio: <a href=""https://harmonydata.ac.uk/harmony_r_example.nb.html"" target=""_parent""><img src=""https://img.shields.io/badge/RStudio-4285F4"" alt=""Open In R Studio""/></a>  You can run the walkthrough R notebook in Google Colab with a single click: <a href=""https://colab.research.google.com/github/harmonydata/experiments/blob/main/Harmony_R_example.ipynb"" target=""_parent""><img src=""https://colab.research.google.com/assets/colab-badge.svg"" alt=""Open In Colab""/></a>   # The Harmony Project  Harmony is a tool using AI which allows you to compare items from questionnaires and identify similar content. You can try Harmony at https://harmonydata.ac.uk/app and you can read our blog at https://harmonydata.ac.uk/blog/.  ## Who to contact?  You can contact Harmony team at https://harmonydata.ac.uk/, or Thomas Wood at https://fastdatascience.com/.  ## ðŸ–¥ Installation instructions (video)  [![Installing Harmony](https://raw.githubusercontent.com/harmonydata/.github/main/profile/installation_video.jpg)](https://www.youtube.com/watch?v=enWh0-4I0Sg ""Installing Harmony"")  ## ðŸ–± Looking to try Harmony in the browser?  Visit: https://harmonydata.ac.uk/app/  You can also visit our blog at https://harmonydata.ac.uk/  ## âœ… You need Tika if you want to extract instruments from PDFs  Download and install Java if you don't have it already. Download and install Apache Tika and run it on your computer https://tika.apache.org/download.html  ``` java -jar tika-server-standard-2.3.0.jar ```  ## Requirements  You need a Windows, Linux or Mac system with  * Python 3.8 or above * the requirements in [requirements.txt](./requirements.txt) * Java (if you want to extract items from PDFs) * [Apache Tika](https://tika.apache.org/download.html) (if you want to extract items from PDFs)  ## ðŸ–¥ Installing Harmony Python package  You can install from [PyPI](https://pypi.org/project/harmonydata/).  ``` pip install harmonydata ```  ## Loading all models  Harmony uses spaCy to help with text extraction from PDFs. spaCy models can be downloaded with the following command in Python:  ``` import harmony harmony.download_models() ```  ## Matching example instruments  ``` instruments = harmony.example_instruments[""CES_D English""], harmony.example_instruments[""GAD-7 Portuguese""] questions, similarity, query_similarity, new_vectors_dict = harmony.match_instruments(instruments) ```  ## How to load a PDF, Excel or Word into an instrument  ``` harmony.load_instruments_from_local_file(""gad-7.pdf"") ```",https://github.com/harmonydata/harmony,harmonydata/harmony
e0b7ddaf-76ec-4789-a6ff-0db4db988df0,spot,SPOT,"* Visualize and interact with multidimensional data using a variety of linked charts * Drag and drop, no need for any programming * Load your own data, or connect to a database * Store and share your sessions * Can be used online as a web app or as a standalone desktop application  With SPOT, you can gain insight into multi-dimensional datasets in just a few clicks. It requires no programming at all, everything is drag and drop. SPOT is designed to feel quick and responsive, even for data sets containing millions of records, and it works across a wide variety of platforms (PC, smartphone, tablet), operating systems (Windows, Linux, Mac), and browsers (Firefox, Google Chrome, Safari).  Additional repositories: * https://github.com/NLeSC/spot-desktop-app",https://github.com/NLeSC/spot,NLeSC/spot
e0b8dd33-e7db-4cff-b29b-aeca1158d1f4,nplinker,NPLinker,"NPLinker is a python framework for data mining microbial natural products by integrating genomics and metabolomics data.  # A quick look ```python  from nplinker import NPLinker  # create an instance of NPLinker npl = NPLinker(""nplinker.toml"")   # load data npl.load_data()  # check loaded data print(npl.bgcs) print(npl.gcfs) print(npl.spectra) print(npl.mfs) print(npl.strains)  # compute the links for the first 3 GCFs using metcalf scoring method link_graph = npl.get_links(npl.gcfs[:3], ""metcalf"")    # get links as a list of tuples link_graph.links   # get the link data between two objects or entities link_graph.get_link_data(npl.gcfs[0], npl.spectra[0])   # Save data to a pickle file npl.save_data(""npl.pkl"", link_graph) ```",https://github.com/NPLinker/nplinker,NPLinker/nplinker
e0de74b3-2684-4f61-89c9-d2062ead5a48,4tu-valuemonitor-software,ValueMonitor software,"ValueMonitor is a tool that can be used by all to analyse how moral values have changed over time. Analyses are done based on text corpora, using a text-mining method called probabilistic topic models. Specific to topic models is that values are not defined as keywords, but as distribution of words. This is useful to better capture the idea of (latent) values.",,
e166b777-334e-4a48-b490-35248ebf9aa7,4tu-model-code-of-anisotropic-electrical-conduction-in-layered-3d-prints-with-fused-deposition-modeling,Model Code of Anisotropic Electrical Conduction in Layered 3D-Prints with Fused Deposition Modeling,Matlab code from manuscript:Modelling of Anisotropic Electrical Conduction in Layered Structures 3D-Printed with Fused Deposition Modelling,,
e182cef1-3baf-4850-ac16-ac4597987402,4tu-summation-of-currents-electromagnetic-solver,Summation of currents electromagnetic solver,"This solver can be used to predict the self-inductance and mutual  inductance of coils, as well as the inductance change due to the  presence of a conductive object near the coil. To do so the solver can  create object that only have mesh elements on the surface in order to be  able to model the skin effect.",,
e1f96b2e-d03f-465e-b657-ffe7b2b4e69e,4tu-crsutil-coordinate-and-time-reference-system-matlab-toolbox,CRSUTIL Coordinate and Time Reference System Matlab Toolbox,"CRSUTIL is a Matlab toolbox for GPS and GNSS related Coordinate and time Reference System utility functions.       It contains  basic functions for Geodesy, coordinate transformations,  Global Navigation Satellite Systems (GNSS), Earth Centered and Inertial Reference Frames and Keplerian elements. This toolbox is a prerequisite for several other Matlab toolboxes.      The  most recent version can be found on Github (https://github.com/hvandermarel/crsutil-matlab-toolbox).",https://data.4tu.nl/v3/datasets/5d5bca3c-8fe4-4382-9614-12e72b784485.git,
e23669eb-6e5e-4454-b4c3-a2d9415a1234,4tu-matlab-scripts-created-during-the-work-on-design-of-experiments-a-statistical-tool-for-piv-uncertainty-quantification,"MATLAB scripts created during the work on ""Design of experiments: a statistical tool for PIV uncertainty quantification""","This dataset contains MATLAB scripts created during the work on ""Design of experiments: a statistical tool for PIV uncertainty quantification"". The proposed UQ approach is applied to estimate the uncertainties in time-averaged velocity and Reynold normal stresses in planar PIV measurements of the flow over a NACA0012 airfoil. The approach is also used to the investigation by stereoscopic PIV of the flow at the outlet of a ducted Boundary Layer Ingesting (BLI) propulsor. The codes in this dataset are used for these two experimental cases.",,
e23cc543-ba97-4913-98e5-5ddd6fcd1d34,dtbase,DTBase,,https://github.com/alan-turing-institute/DTBase,alan-turing-institute/DTBase
e24ee378-a664-4612-9338-394994fa642a,4tu-cost-efficient-gnss-basestation-sd-card-image-for-raspberry-pi,Cost-efficient GNSS basestation SD card image for Raspberry Pi,"Consists of the full SD card image to run the cost-efficient GNSS unit in the accompanied paper ""Cost-effective GNSS as a tool for monitoring volcanic deformation: A case study on Saba in the Lesser Antilles"" (submitted)",,
e28d324a-e16c-4ebb-bb6b-5b252791a079,reaxpro-framework-ontology,ReaxPro framework ontology,"This library provides an EMMO-complaiant semantic description of basic electronic-atomistic, mesoscopic and continuum terms so the user can build multiscale simulation (semantic-based) workflows.",https://github.com/simphony/reaxpro-framework-ontology,simphony/reaxpro-framework-ontology
e2a4a61f-d337-4cef-84a6-5b59e8aad4a8,4tu-code-related-to-loudspeaker-beamforming-to-enhance-speech-recognition-performance-of-voice-driven-applications,"Code related to ""Loudspeaker Beamforming to Enhance Speech Recognition Performance of Voice Driven Applications""","This repository contains MATLAB code implementing the loudspeaker spotformer proposed in ""Loudspeaker Beamforming to Enhance Speech Recognition Performance of Voice Driven Applications"", see also https://github.com/D1mme/LoudspeakerBeamformingForVoiceDrivenApplications. The article was accepted but has not yet been published. A DOI will be added once available. Note that some parts of the data and code in this repository are not my own and are published under different but permissive licenses. To see them, please refer to the corresponding directories.     Abstract:  In this paper we propose a robust loudspeaker beamforming algorithm which is used to enhance the performance of voice driven applications in scenarios where the loudspeakers introduce the majority of the noise, e.g. when music is playing loudly. The loudspeaker beamformer modifies the loudspeaker playback signals to create a low-acoustic-energy region around the device that implements automatic speech recognition for a voice driven application (VDA). The algorithm utilises a distortion measure based on human auditory perception to limit the distortion perceived by human listeners. Simulations and real-world experiments show that the proposed loudspeaker beamformer improves the speech recognition performance in all tested scenarios. Moreover, the algorithm allows to further reduce the acoustic energy around the VDA device at the expense of reduced objective audio quality at the listenerâ€™s location.",https://data.4tu.nl/v3/datasets/fbc606b8-548e-40ca-ab39-50f5145c3db3.git,
e2b5b46c-edd6-4e01-9355-db69faba326c,salient-detector-matlab,SalientDetector-matlab,"* Provides MATLAB implementation of a competitive Data-driven Morphological Salient Region (DMSR) image detector for computer vision, biodiversity and other researchers who need to analyzable large amount of images for discovering or identifying objects or scenes * Highly repetitive detection  and visualization of salient regions in multiple structured images of the same scene or object of interest  * Open-source code for researchers who are used to MATLAB * Successfully applied to various tasks: automated determination whether images are from the same scene, marine mammals photo-identification, wood species classification from microscopy images, etc.  * DMSR shows invariance to affine geometric image transformations as well as to photo-metric transformations such as blur and lighting  * DMSR gives better results than the popular MSER region detector  E. Ranguelova, ""A Salient Region Detector for structured images,""  2016 IEEE/ACS 13th International Conference of Computer Systems and Applications (AICCSA), Agadir, 2016, pp. 1-8. doi: 10.1109/AICCSA.2016.7945643",https://github.com/NLeSC/SalientDetector-matlab,NLeSC/SalientDetector-matlab
e2f19823-4bc6-4c98-8b98-86c5de8ff5d8,4tu-data-underlying-the-publication-thesis-chapter-learning-to-adapt-to-position-bias-in-vision-transformer-classifiers,Data underlying the publication/thesis chapter: Learning to Adapt to Position Bias in Vision Transformer Classifiers,"This article proposes a novel method to measure learned position bias in Vision Transformers, and investigates which model design choices affect learned equivariance.",https://data.4tu.nl/v3/datasets/9c507af0-7860-45e7-b9bf-1ff14cdfe4a3.git,
e2f58f47-4e11-4de0-bef5-9c2a18f77be9,panva,PanVA,This repository contains both the Frontend code and the API code for the PanVA application.,https://github.com/PanBrowse/PanVA,PanBrowse/PanVA
e313bdcd-4cdd-4329-87f1-6a2b57bf72e9,4tu-hydrodynamic-analysis-of-marine-structures-marine-renewable-energies-lab-hams-mrel,Hydrodynamic Analysis of Marine Structures-Marine Renewable Energies Lab (HAMS-MREL),"HAMS-MREL is Boundary integral Element Method (BIEM) based solver for performing the hydrodynamic analysis of multiple body system, considering interactions between them. The solver is based on Linear Potential Flow theory, and can be used for preliminary analysis of floating (diffraction and radiation) and fixed structures (diffraction), all within a computationally efficient parallelized framework. This builds on the work on the HAMS solver, that was developed for single body fluid structure interaction problems. 								     The zip file contains the executable for the solver, along with python script for pre- and post processing. Instructions for running the solver are also added in a word file. The current version is v1.1.",,
e32738b6-d3f7-4c00-b329-ed907bc808a0,4tu-sars-cov-2-variant-quantification-using-kallisto-code,SARS-CoV-2 variant quantification using kallisto code,"Code used together with its results for the paper as part of my Bachelor's Thesis project. The research consists of optimizing the kallisto algorithm for predicting the abundances of SARS-CoV-2 variants in wastewater samples. Specifically, I look at how only sequencing certain regions of the genome influences the prediction accuracy of this pipeline.",,
e354f4c2-e8c9-4d8f-9cfc-b0c0a5c34551,4tu-multi-component-mrf-reconstruction,Multi-component MRF reconstruction,This repository can be used to reconstruct MR Fingerprinting data and perform single and multi-component matching with the newly proposed k-SPIJN reconstruction.,,
e3b7a77e-93d5-40d3-940c-f25c4fe8cf41,4tu-automated-xy-calibration-for-3d-printers-using-a-scanner,Automated xy calibration for 3D printers using a scanner,This program can be used to calibration the x and y offset of a multi-material 3D printer by printing a calibration pattern on a piece of paper and scanning it using a digital scanner.,,
e3ca905f-2d5c-49b7-9750-6b287406f467,muscle3,MUSCLE3,"* Create multiscale simulations by coupling existing or new models together via the network * Temporal and spatial scale separation, ensembles * Central parameter settings with in-simulation override for e.g. Uncertainty Quantification, surrogate modelling * YAML-based configuration language describing connections and settings   MUSCLE3 is the third incarnation of the MUSCLE Multiscale Coupling Library and Environment. It is developed by the e-MUSC project of the University of Amsterdam and the Netherlands eScience Center.  MUSCLE3 scales all the way from a simple model in a single Python file running on your laptop to a complex multiscale simulation comprising multiple models written in different programming languages running on an HPC machine. Existing models can be connected with minimal modification.  Additional repositories: * https://github.com/multiscale/ymmsl-python",https://github.com/multiscale/muscle3,multiscale/muscle3
e3e0191c-7fe3-4f11-860b-6dc928cbf6c8,bacting,Bacting,,https://github.com/egonw/bacting,egonw/bacting
e3fa67af-c85e-41e5-8ec7-d3129c97ff7e,wrf-runner,WRF-runner,- Run a complete WPS/WRF experiment in one go - Make a dedicated directory for each experiment - The original WRF source code is untouched - Namelists under version control so you can always go back,https://github.com/Urban-M4/wrf-runner,Urban-M4/wrf-runner
e4a0a6e9-710e-41af-97f3-97220f1eac96,gh-action-set-up-grpc,GitHub action to set up gRPC,"* For developer writing a software development workflow in GitHub Actions and need to build against gRPC C++ library * Tell workflow what you want not how you want it * No need to learn how to install gRPC yourself, just copy/paste configuration",https://github.com/eWaterCycle/setup-grpc,eWaterCycle/setup-grpc
e4a9a709-0ca7-48c4-9463-5a8794a3c9b4,4tu-pydsol-model,pydsol-model,"pydsol-model is a package that includes standard model objects suitable for developing a discrete event simulation model. Standard model objects are source, server using resources, sink, node, link, entity and vehicle. This makes it easy and fast to design discrete event simulation models using queueing theory, useful for teaching, academic research, and commercial use. You can use the standard model objects as is, or use it to make your own objects for more complex simulation models.&nbsp;",https://data.4tu.nl/v3/datasets/788435c3-00cb-4858-b933-1a4af41528b8.git,
e4b0cb71-7276-4f1d-b0a5-b0e0d220d3e7,4tu-software-used-in-the-publication-a-non-parametric-bayesian-network-for-multivariate-probabilistic-modelling-of-weigh-in-motion-system-data,"Software used in the publication ""A Non-parametric Bayesian Network for multivariate probabilistic modelling of Weigh-in-Motion System Dataâ€","The sofware is called Â WIM NPBN, it is a Graphical User Interface to compute synthetic Weigh-in-Motion (WIM) axle loads and inter-axle distances of 26 vehicle types using a Non-Parametric Bayesian Network (NPBN). Â The model is based on Â measurements that were taken in Dutch highways A12 (km 42) Woerden, A15 (km 92) Â Gorinchem, and A16 (km 41) Gravendeel.",,
e4d4ea8a-605b-4f6a-8a6d-5f8d18c55b0d,4tu-data-and-code-underlying-the-publication-deep-learning-reveals-key-predictor-of-thermal-conductivity-of-covalent-organic-frameworks,Data and code underlying the publication: Deep learning reveals key predictor of thermal conductivity of covalent organic frameworks,"Abstract  The thermal conductivity of covalent organic frameworks (COFs), an emerging class of nanoporous polymeric materials, is crucial for many applications, yet the link between their structure and thermal properties remains poorly understood. Analysis of a dataset containing over 2,400 COFs reveals that conventional features such as density, pore size, void fraction, and surface area do not reliably predict thermal conductivity. To address this, an attention-based machine learning model was trained, accurately predicting thermal conductivities even for structures outside the training set. The attention mechanism was then utilized to investigate the model's success. The analysis identified dangling molecular branches as a key predictor of thermal conductivity, leading us to define the dangling mass ratio (DMR), a descriptor that quantifies the fraction of atomic mass in dangling branches relative to the total COF mass. Feature importance assessments on regression models confirm the significance of DMR in predicting thermal conductivity. These findings indicate that COFs with dangling functional groups exhibit lower thermal transfer capabilities. Molecular dynamics simulations support this observation, revealing significant mismatches in the vibrational density of states due to the presence of dangling branches.     Please refer to the README.md file for more information on the code and data.",,
e53b3492-acbf-4ddb-bd24-c43dbe68dfb6,4tu-code-for-chapter-4-of-dissertation-vibration-induced-friction-force-modulation,Code for Chapter 4 of dissertation: Vibration-induced friction force modulation,"MATLAB script to reproduce results from:  Chapter 4 of ""Vibration-induced friction modulation"" Dissertation     This script generates results shown in different figures of the chapter  for various system parameters. Users can modify the parameters below  (v_b, r_w, damping, excitation amplitudes, frequencies, etc.) to explore  different scenarios.      Author: Enxhi Sulollari",,
e5853172-9e7b-4595-8bdc-abc6b7f0a846,scholy-core,Scholy â€“ Core,"## What Scholy Can Do For You  - **Create and manage researcher profiles** - Easily maintain up-to-date information about researchers, their work, and their achievements in one central location - **Showcase academic publications and projects** - Display research outputs, publications, and ongoing projects in an organized, accessible format - **Connect researchers and their work** - Help people discover researchers at your institution and find relevant expertise for collaboration - **Support institutional visibility** - Provide a public-facing platform that highlights the research excellence and academic contributions of your organization  ## Overview  Scholy is a web-based software platform designed to help universities and research institutions create and maintain directories of their researchers and scholars. Think of it as a specialized system for building researcher profile pages similar to what you might see on Google Scholar or university faculty pages, but customized for your institution's needs.  The software helps institutions organize and present information about their researchers in a structured way, making it easier for:  - **Academic administrators** to showcase their institution's research strengths - **Researchers** to share their work and connect with collaborators - **Students and the public** to discover expertise and learn about ongoing research  Built using modern web technologies and semantic web standards, Scholy enables institutions to create rich, interconnected researcher profiles that can integrate with other academic platforms and databases. The system can be customized to match institutional needs and branding.",https://cau-git.rz.uni-kiel.de/scholy/scholy-core/-/tree/master,
e5975efc-6175-4377-b70c-847b267b7739,snakemake-wrf,snakemake-wrf,Reproduce a set of experiments with the Weather Research and Forecasting (WRF) model with the click of a button.,https://github.com/Urban-M4/snakemake-wrf/,Urban-M4/snakemake-wrf
e5a4509d-8486-4c0c-a53b-48f71a16bd93,cptm,Cross-perspective Topic Modeling,"* Experiment with different settings * Generate iPython notebooks to visualize results * Downloading and preprocessing of the Dutch parliamentary proceedings  cptm is an implementation of cross-perspective topic modeling, which is is a topic modeling method that simultaneously extract topics and opinions from text. Topics are learned from nouns and opinions from adjectives. To use this tool, you need a corpus that is divided into perspectives. In the DiLiPaD project a perspective refers to a political party. We used this tool to extract topics and opinions from parliamentary proceedings.",https://github.com/NLeSC/cptm,NLeSC/cptm
e5b178e1-1388-4a97-9e65-67848639461d,fortrandavidson,Davidson diagonalization in Fortran,"* Follows modern Fortran practices  * Allows both dense and matrix-free calculations.  * Implements different correction methods    The Davidson method is suitable for **diagonal-dominant symmetric matrices**, that are quite common in certain scientific problems like [electronic structure](https://en.wikipedia.org/wiki/Electronic_structure). The Davidson method could be not practical for other kinds of symmetric matrices.",https://github.com/NLESC-JCER/Fortran_Davidson,NLESC-JCER/Fortran_Davidson
e5c27c13-5179-405f-9aea-ad8330c99a69,geoflow-visualizer,Geoflow visualizer,,https://github.com/geoflow-visualizer/geoflow-visualizer,geoflow-visualizer/geoflow-visualizer
e61dfac9-0465-4281-bf71-935ec9ff4b96,4tu-totem-m,TOTEM_M,"This code provides an FEM code in MATLAB to perform topology optimization (TO) of systems with thermal, voltage, and mechanical degrees of freedom. In particular the code provides examples to run TO for a thermocouple for cooling purposes with decreased temperature objectives using power and stress constraints together with a variable voltage gradient.",https://data.4tu.nl/v3/datasets/058eb2e5-060e-4176-9601-9a5af3ac60d5.git,
e622531e-2f4a-4a36-95ba-51395393b759,doc2sentences,doc2sentences,,https://github.com/backdem/doc2sentences,backdem/doc2sentences
e62379be-1bcb-435c-9c18-39cb016e51af,3d-slicer,3D Slicer,"3D Slicer is an open-source platform for the analysis and display of information derived from medical imaging and similar data sets. Such advanced software environments are in daily use by researchers and clinicians and in many nonmedical applications. 3D Slicer is unique through serving clinical users, multidisciplinary clinical research terms, and software architects within a single technology structure and user community. Functions such as interactive visualization, image registration, and model-based analysis are now being complemented by more advanced capabilities, most notably in neurological imaging and intervention. These functions, originally limited to offline use by technical factors, are integral to large scale, rapidly developing research studies, and they are being increasingly integrated into the management and delivery of care. This activity has been led by a community of basic, applied, and clinical scientists and engineers, from both academic and commercial perspectives. 3D Slicer, a free open-source software package, is based in this community; 3D Slicer provides a set of interactive tools and a stable platform that can quickly incorporate new analysis techniques and evolve to serve more sophisticated real-time applications while remaining compatible with the latest hardware and software generations of host computer systems.",https://github.com/Slicer/Slicer,Slicer/Slicer
e6450709-b78d-46f2-8b97-b4f83cccb696,4tu-unity-vr-package-of-a-plausible-future-of-bicycle-assembly-scenario-in-samxl,UNITY: VR Package of a plausible future of bicycle assembly scenario in SAMXL,"""Futures is a topic that is inherently fuzzy, as everything beyond this moment can potentially happen. The uncertainty that is related to futures makes it not only difficult to imagine and convey, but also challenging to study.â€  The software includes two folders. The â€œSAMXL_FowGame_V1â€ folder contains all the assets that was initially build by TUDelft. This game consists out of a short general introduction, the Future of Work scenario as imagined by the RoboFiets project, and a short ending.   The build was used for an explorative study, where the potential for using Virtual Reality as a foresight tool and research tool is discussed and evaluated. The â€œExtendedIntro_ShortenedGame_V1.6fâ€ folder contains all the assets of the final version of the game, created and used during this explorative study:  A degree of presence, immersion or flow, prompted by (perceived) interactivity, was concluded to be required in order for VR to be used as a tool for futures studies. In order to achieve this, a tutorial has been added in the introduction; refined and reiterated through iterations, in order to ease players into virtual reality more comfortably and enhance their feeling of immersion and presence (in the mediated environment). In the (re)developed introduction, basic interaction controls are explained interactively and in detail. Furthermore, the context of play is set so the future of work scenario story-line can be conveyed more effectively.   The software runs best on Unity version 2019.4.10, played on a HTC Vive system with two controllers. A brief explanation of how scenes and interactions can be added into the game can be found here.",,
e6a9770b-d298-4b0f-9cdb-3c9401e701e1,overture-data-management-system,Overture Data Management System,"## Genome informatics first: You can use our solutions for anything, but here at Overture we focus on Genome Informatics. With rapidly expanding datasets at the heart, we've built our bioinformatics bundle to track, transfer, and secure genomic data in distributed cloud environments.  ## Swappable: There is no such thing as perfection. Our tools are implemented to be interchangeable from the get-go. You can pick-and-choose from our software stack and simply use the components that best match your use case.  ## An open world: We are strong believers in open-source software, open science, and open communication. Donâ€™t hesitate to follow our team activities on GitHub by taking a look at upcoming or in-progress tickets, or even be the first to test out a feature detailed in a Pull Request.  ## Closing the loop: Our team has established a strong foundation in building software solutions for genomic projects, from data generation and submission all the way to data dissemination and analysis, leading to a deep understanding of the genomic data lifecycle. The Overture stack contains a wide array of components for cloud infrastructure, data shepherding, and analysis.   **The Overture Data Management system** is comprised of five components of software that can function and be deployed independently: * Ego - [Source code](https://github.com/overture-stack/ego) * Song - [Source code](https://github.com/overture-stack/SONG) * Score - [Source code](https://github.com/overture-stack/score) * Maestro - [Source code](https://github.com/overture-stack/maestro) * Arranger - [Source code](https://github.com/overture-stack/arranger)",https://github.com/overture-stack/dms,overture-stack/dms
e6fada92-c851-4bd5-8a97-6f0bcc70ce61,obiba-mica,OBiBa Mica,"# What is Mica?  Mica is a powerful software application used to create data web portals for large-scale epidemiological studies or multiple-study consortia. Mica2 is the successor of Mica.  Mica helps studies to provide scientifically robust data visibility and web presence without significant information technology effort. Mica provides a structured description of consortia, studies, annotated and searchable data dictionaries, and data access request management.  # Features Using Mica2, studies and consortia can administer and display information about themselves and about their constituants, this is:  ## Network Identification and Listing Mica2 provides a standardized way to describe both networks and the studies that constitute them.  ## Study Catalogue Mica2 study catalogue includes:  * A list of important study description default characteristics that can be customized to reflect specific requirements, * Support for longitudinal study design with data collection events, sub-populations and recruitment timelines.  ## Datasets Studies makes available different datasets to the research community. Mica2 provides: * A standard menu for its data resources, * Advanced variable search functionality using facets, * A detailed description of a dataset including a comprehensive dictionary of all the variables provided by the dataset.  Importantly, Mica2 does not store any record-level data itself. Instead, Mica2 includes for studies using the OBiBa Opal software as main secured database solution, a distributed query engine that enables authorized researchers to obtain real-time aggregated reports on the exact number of participant with specific characteristics or phenotypes. The same engine is used to obtain summary statistics on each variable. Using this approach, no individual-level data passes through Mica2, yet researchers can assess the suitability of a dataset for their needs.  ## Data Access Management Mica2 provides a way to edit forms that allows a study to handle access requests electronically. Using this feature, single studies or consortia can: * Present their Data Access Policy, * Manage Data Access Requests from submission to approval, * Extend default Data Access Request form to match their needs, * Present the Data Access Committee members.  ## Administration With Mica2, you can: * Administer your Consortium web portal from the website itself, * Define permissions on any kind of content.",https://github.com/obiba/mica2,obiba/mica2
e701d760-1f57-414a-835a-87e90a635da0,mapreader,MapReader,,https://github.com/Living-with-machines/MapReader,Living-with-machines/MapReader
e7de278a-a8d0-44bf-b26c-cce33f49aefa,psrdada-python,PSRDADA python,"* python library to communicate with an astrophysics pipeline * connects as a data reader, writer, or monitor to a PSRDADA ringbuffer * written to be fast; no memory copies * and to be simple  It is hard to keep up with the real-time datastreams originating from a radio telescope. The processing pipeline needs to efficiently pass data around between very different components: network, cpu, gpu, disks. The problem is typically solved using a ringbuffer, with an often used implementation in PSRDADA.  This package provides a python interface to the PSRDADA ringbuffer, allowing you to easily test and develop code and algorithms in python. On real-time data. On the real system.",https://github.com/TRASAL/psrdada-python,TRASAL/psrdada-python
e80643b9-df33-4c5f-9582-930979e5f19d,4tu-global-analysis-of-dna-methylation-in-hepatocellular-carcinoma-via-a-whole-genome-bisulfite-sequencing-approach,Global analysis of DNA methylation in hepatocellular carcinoma via a whole-genome bisulfite sequencing approach,"We compared the patterns of global epigenomic DNA methylation during the development of HCC, focusing on the role of DNA methylation in the early occurrence and development of HCC, providing a direction for future research on its epigenetic mechanism.",,
e812bda2-aa96-4c0e-a276-66a5299f1342,cronian,Cronian,,https://gitlab.tudelft.nl/demoses/cronian,
e86a214b-dc03-457e-bc94-a2e7d62fb5b3,slurm-time-on-jupyter,Slurm time on Jupyter,# jupyterlab_slurm_time  ![Github Actions Status](https://github.com/nesi/jupyterlab_slurm_time/workflows/Build/badge.svg)  Display Slurm job remaining time in JupyterLab.   This extension is composed of a Python package named `jupyterlab_slurm_time` for the server extension and a NPM package named `jupyterlab-slurm-time` for the frontend extension.   ## Requirements  * JupyterLab >= 3.0,https://github.com/nesi/jupyterlab_slurm_time,nesi/jupyterlab_slurm_time
e8735ad3-3c01-4f33-83fb-3184faf3f961,3d-e-chem-vm,3D-e-Chem Virtual Machine,"* Gives cheminformaticians that don't want to install lots of software a single command to get all the software they need to perform ligand binding site comparisons and GPCR research * The VM contains a fully functional cheminformatics infrastructure * The VM was used in several workshops, giving all participants the same environment and quick start",https://github.com/3D-e-Chem/3D-e-Chem-VM,3D-e-Chem/3D-e-Chem-VM
e87f4a06-a66c-413d-8631-21bbed038bbe,magnumpi,MagnumPI - Potential Integrator,"* For physicists that need to calculate scattering angles or cross sections using a simple web interface* For C++ wanting to make their code available on the web * Example of how to make a C++ library available in web browser with WebAssembly, without any compute on server * Example of how to make a C++ library available in a Python and JavaScript package. * Example formatting standard for potentials.   Several blogs where written about the technology used, see the mentions below. Makes use of emscripten to make a WASM and pybind11 to call C++ library from Python.",https://gitlab.com/magnumpi/magnumpi,
e89b28ca-a50e-4b79-9a79-7a363fec8e07,attentionlayerjl,AttentionLayer.jl,,https://github.com/DEEPDIP-project/AttentionLayer.jl,DEEPDIP-project/AttentionLayer.jl
e90f4708-6ae2-4d4d-a2c7-3e3a1d55607d,4tu-mo-tas,mo-tas,"This repository contains the code developed for the research work of Ilias Parmaksizoglou (2025), specifically for the studies related to Truck Appointment Systems:     A Multi-Agent Optimization Approach for Multimodal Collaboration in Marine Terminals   The repository is made public to serve as supplementary material for the above publication and the PhD thesis of Ilias Parmaksizoglou, and to allow other researchers to use and build upon this work in their own studies.",https://data.4tu.nl/v3/datasets/0a1738ef-f14e-4a17-b954-572be1e0f7b3.git,
e9aaa62b-f542-4916-bd31-453a1935b088,ssiml2021,SSIML2021,,https://github.com/SSIML2021/SSIML2021,SSIML2021/SSIML2021
e9be86b2-4f4f-43aa-b93a-cccd9785a95b,deeprankgnn,DeepRank GNN,"* Creates graphs on protein-protein interface * Train graph neural network on protein-protein interface     Gaining structural insights into the protein-protein interactome is essential to understand biological phenomena and extract knowledge for rational drug design or protein engineering. We have previously developed DeepRank, a deep-learning framework to facilitate pattern learning from protein-protein interfaces using Convolutional Neural Network (CNN) approaches. However, CNN is not rotation invariant and data augmentation is required to desensitize the network to the input data orientation which dramatically impairs the computation performance. Representing protein-protein complexes as atomic- or residue-scale rotation invariant graphs instead enables using graph neural networks (GNN) approaches, bypassing those limitations.  We have developed DeepRank-GNN, a framework that converts protein-protein interfaces from PDB 3D coordinates files into graphs that are further provided to a pre-defined or user-defined GNN architecture to learn problem-specific interaction patterns. DeepRank-GNN is designed to be highly modularizable, easily customized, and is wrapped into a user-friendly python3 package. Here, we showcase DeepRank-GNNâ€™s performance for scoring docking models using a dedicated graph interaction neural network (GINet). We show that this graph-based model performs better than DeepRank, DOVE and HADDOCK scores and competes with iScore on the CAPRI score set. We show a significant gain in speed and storage requirement using DeepRank-GNN as compared to DeepRank.  https://www.biorxiv.org/content/10.1101/2021.12.08.471762v1",https://github.com/DeepRank/DeepRank-GNN,DeepRank/DeepRank-GNN
e9ed554e-fe94-4772-a2b4-113b88b19ff3,4tu-artifact-for-paper-alpinist-an-annotation-aware-gpu-program-optimizer,Artifact for paper (Alpinist: an Annotation-Aware GPU Program Optimizer),"Artifact for paper (Alpinist: an Annotation-Aware GPU Program Optimizer) submitted to TACAS '22 conference. For a full description on how to use the artifact, please see the README.txt file. The artifact contains the Alpinist tool, all its dependencies and documentation for the VerCors tool.  Abstract of the paper: GPU programs are widely used in industry. To obtain the best  performance, a typical development process involves the manual or  semi-automatic application of optimizations prior to compiling the code.  To avoid the introduction of errors, we can augment GPU programs with  (pre- and postcondition-style) annotations to capture functional  properties. However, keeping these annotations correct when optimizing  GPU programs is labor-intensive and error-prone. This paper  introduces Alpinist, an annotation-aware GPU program optimizer. It  applies frequently-used GPU optimizations, but besides transforming  code, it also transforms the annotations. We evaluate Alpinist, in  combination with the VerCors program verifier, to automatically optimize  a collection of verified programs and reverify them.",,
ea737017-9fd0-489c-becf-a4f710d3fff1,isaac-chrome-extension,ISAAC Chrome extension,"# Install  The extension is [available in the Chrome Web Store](https://chrome.google.com/webstore/detail/isaac-chrome-extension/kiljfbiapahlahhilgcgfkfjnkgggode).  # Usage    1. Click ""Projects"", select your project and go to the ""Product"" tab. There, click ""Add"".   2. Select the type of product. The extension can do this automatically, but this process is fallible.   3. Click the icon of the extension in the top right.  ![Screenshot of the popup of the extension over the ISAAC forms](https://github.com/citation-js/isaac-chrome-extension/raw/main/assets/screenshot.png)    4. Fill in your DOI, PubMed identifer, or ISBN and click ""Search"". This takes you to the information of the first author.   5. Review the author information, optionally add additional info such as the gender and DAI, and click ""Next"". Repeat until all authors are added.   6. Review the product information, optionally add additional info such as Open Access status.   7. Submit.",https://github.com/citation-js/isaac-chrome-extension,citation-js/isaac-chrome-extension
ea8d52cb-bd30-4143-9deb-38ac75e2482b,protein-quest,protein-quest,Python package to search/retrieve/filter proteins and protein structures.  It uses  - [Uniprot Sparql endpoint](https://sparql.uniprot.org/) to search for proteins and their measured or predicted 3D structures. - [Uniprot taxonomy](https://www.uniprot.org/taxonomy?query=*) to search for taxonomy. - [QuickGO](https://www.ebi.ac.uk/QuickGO/api/index.html) to search for Gene Ontology terms. - [gemmi](https://project-gemmi.github.io/) to work with macromolecular models. - [dask-distributed](https://docs.dask.org/en/latest/) to compute in parallel.  An example command:  ```shell protein-quest search uniprot \     --taxon-id 9606 \     --reviewed \     --subcellular-location-uniprot nucleus \     --subcellular-location-go GO:0005634 \     --molecular-function-go GO:0003677 \     --limit 100 \     uniprot_accs.txt ```,https://github.com/haddocking/protein-quest,haddocking/protein-quest
eab23fbf-7eab-4561-a64c-2aaf003d94f2,knime-gpcrdb,KNIME GPCRdb nodes,"* For cheminformaticians who are working with GPCR proteins in KNIME workflows. * KNIME nodes to interact with GPCRdb webservice * You don't have to download csv files from the GPCRdb website and load them into a KNIME workflow, but can use nodes which fetch for example a protein sequence alignment for you",https://github.com/3D-e-Chem/knime-gpcrdb,3D-e-Chem/knime-gpcrdb
eaef1ed1-9137-4c73-a185-9ed20224997e,4tu-code-for-the-master-thesis-exploring-the-effect-of-automation-failure-on-the-human-s-trustworthiness-in-human-agent-teamwork,"Code for the master thesis ""Exploring the Effect of Automation Failure on the Human's Trustworthiness in Human-Agent Teamwork""","The game is based on the game ""moving out"". It is programmed using MATRX.Â        Run main.py after meeting the requirements on your pc (see requirements.txt).       Scenario A is a game without automation failure. Scenario B is a game with automation failure.",,
eaf6453a-8c03-44d8-ac19-8003becb8faf,canadian-writing-research-collaboratory,Canadian Writing Research Collaboratory (CWRC),"CWRCâ€™s key is integration: of system components; of content whose value increases exponentially when combined and subjected to new modes of inquiry; of scholarly materials with the massive archive of digital texts; of scholars themselves.   # Content  ## An online repository The repository or database houses born-digital scholarly materials, digitized texts, images, audio files, video, prosopographic records, and metadata (indices, annotations, cross-references).  ## Open access content CWRC content is made freely available wherever possible. Although it contains some copyright materials, and some materials that are restricted for reasons to do with rights or work in progress, most CWRC content is licensed under a Creative Commons CC-BY-NC license for sharing and reuse, although other licenses are possible as is the application of Traditional Knowledge Labels for Indigenous Content.  ## Interoperable content Materials are stored in standard, preservation-friendly and interchange-friendly formats.  ## Existing content CWRC is seeded with a substantial body of existing materials. The Early Canadian Cultural Journals Index, published on CD-ROM, is now available again through CRWC. The Canadian Early Women Writers significantly updates and expands the work originally published as Canadaâ€™s Early Women Writers, positioning it for ongoing enhancement by the community. The Orlando Project will have CWRC as its new home and share its Canadian content.  ## Content in progress CWRC supports the production of born-digital content, from inception to dissemination, enabling scholars to explore the possibilities of collaboration at every stage of the research process. Individuals and teams have the option to develop their work in the open, to engage citizen scholars, within the walled garden of the Collaboratory in conversation with their peers, or to develop it behind the scenes and release their work in stages or as a whole once it is complete.  ## Dynamic content The results of digital scholarship need not repose. Rather, they can be updated, enhanced, revised, remixed, and redistributed as part of an ongoing process of knowledge production. CWRC provides the means for scholarly communities to support the knowledge and resources that matter to them.  ## Tools CWRC supports the creation, development, management, use, revision, enhancement, and publication of scholarly content through an extensive suite of tools that are accessible through the main CWRC interface.  ## Standards-oriented content creation The uploading of existing materials, and creation of metadata, text, and other content is designed to promote adherence to best practices for interoperability and preservation. The centerpiece of this strategy is the CWRC-Writer browser-based editor that allows the creation of structured text, linked metadata, and interoperable annotations.  ## Content management CWRC helps scholars keep track of the research process by allowing them to apply workflow stamps to record what has been done to materials, assign them to others, and draw on reports of sets of materials. This information is visualized in the forthcoming Credit Visualization (CV) tool that shows the contributions to individual items or collections of materials.  ## Content publication Content can be published at any stage. Alternatively, CWRC can also be used as a production environment and content published elsewhere, or within a separate interface that draws on the repository.   ## Open source tools CWRC builds on the open-source Islandora framework for a Fedora Commons repository. All code developed by the project is open-source in the projectâ€™s Github repository.   ## Programmatic access and extensibility CWRC is built on a RESTful service-oriented architecture with Application Programming Interfaces (APIs). The modular interface supports the addition of new components.   ## Customizable interfaces CWRCâ€™s architecture allows projects to have their own customizable homepages as first points of access to their collections. It also allows projects to build and maintain their own Drupal multisite interfaces that draw on materials in the Collaboratory but provide specialized theming or functionality.     ## CWRC is upgrading From 2021 we are developing a new version of the platform based on the newest version of Islandora, with a more flexible interface and upgraded Fedora repository layer.",https://github.com/cwrc/beta.cwrc.ca,cwrc/beta.cwrc.ca
eb0206f3-4eb3-4483-8e27-c594ef656e40,pytams,pyTAMS,"Provided an implementation of your dynamic problem core functionalities, pyTAMS uses the TAMS algorithm to evaluate the transition probility between two stables point in your system. TAMS is particularly adapted to rare transition probability prediction, where brute force Monte-Carlo method is computationally too expensive.",https://github.com/nlesc-eTAOC/pyTAMS,nlesc-eTAOC/pyTAMS
eb1aedc0-5740-4c26-9492-8c7d72a84bbd,4tu-news-is-more-than-a-collection-of-facts-moral-frame-preserving-news-summarization-code,News is More than a Collection of Facts: Moral Frame Preserving News Summarization - code,"Code used to generate summaries that preserve the moral framing of the original news article. We leverage the zero-shot summarization ability of Large Language Models, shown to produce results on par with human-generated summaries. We compare three language models and five prompting methods. Leveraging the intuition that journalists intentionally use or report moral-laden words in the article text, we propose approaches that first identify moral-laden words in the article (e.g., through Chain-of-Thought or supervised classification) and then guide the language model in preserving such words in the summary.",https://data.4tu.nl/v3/datasets/4727197d-5819-4871-9432-ec3bba1260e6.git,
eb612d1d-4ddf-4516-abcc-41d9a9f74515,4tu-fluvgan-python-scripts-to-test-generating-and-inverting-fluvial-deposits-using-gans,FluvGAN: Python scripts to test generating and inverting fluvial deposits using GANs,"FluvGAN is a set of Python scripts to test generating and inverting fluvial deposits using generative adversarial networks (GANs) in the context of predicting the physical properties of the subsurface. It builds upon FluvDepoSet (https://doi.org/10.25919/4fyq-q291) - a dataset of 3D representations of fluvial deposits simulated with CHILD (https://github.com/childmodel/child), a landscape evolution model - for training, and upon voxgan (https://github.com/grongier/voxgan), a Python package making it easier to define and train GANs with PyTorch.     Installation  You can install all the packages needed to run all the scripts using conda and the file environment.yml included in this repository:  				conda env create -f environment.yml  In addition, the file environment_cluster.yml contains the full list of packages with which the scripts were run on a cluster with a Linux system:  				conda env create -f environment_cluster.yml  This environment misses some packages to visualize the results, because those steps were not done on the cluster, and it misses voxgan, because it was used during voxgan&#x27;s development (now you can install it using pip, see https://github.com/grongier/voxgan).     Usage  FluvGAN is subdivided into three compressed folders:  scripts.zip contains all the Python scripts and Jupyter notebooks to train and test the GANs and to generate some figures showcasing the results.data.zip contains the well &amp; seismic data used for testing GAN inversion; the training and testing samples need to be downloaded from FluvDepoSet at https://doi.org/10.25919/4fyq-q291, which can be done using the script fluvgan_0_data.py.outputs.zip contains all the outputs from all the scripts, including the pretrained GAN models; the outputs&#x27; names follow the same convention as the scripts, and the scripts showcase how to reuse those outputs using Python.You can find more information in the file README.md.     License  Copyright notice: Technische Universiteit Delft hereby disclaims all copyright interest in the program fluvgan written by the Author(s). Prof.dr.ir. S.G.J. Aarninkhof, Dean of the Faculty of Civil Engineering and Geosciences  Â© 2025, Guillaume Rongier, Luk Peeters   This work is licensed under a MIT OSS licence, see the file LICENSE for more information.",,
ebc7e7c7-7982-4d7f-bf88-3ab5f2120ca7,4tu-set-up-files-for-the-determination-of-homogenised-elastic-properties-of-bi-axial-ncfs-using-periodic-homogenisation-of-an-rve,Set-up files for the determination of homogenised elastic properties of bi-axial NCFs using periodic homogenisation of an RVE,"The main file is the â€™RVE set-up fileâ€™. This is the file where input data is given for the creation of the Abaqus input file and the script file used to run the simulations. The user can choose which of the elastic properties they are interested in and provide dimensions and material properties for the tows and stitches. Stitches are created by providing the start and end-coordinate for each segment. This main file uses the â€™RVE functions fileâ€™ and the â€™scriptfileâ€™ to get all the information required to write the output files.Â    It is also responsible for setting the interactions and contact between the different set and surfaces. The â€™RVE functions fileâ€™ also creates the reference points that are used to apply the strains on the RVE. Finally. this file imports the constraint functions from the â€™Constraint functions fileâ€™ to create the boundary conditions necessary to ensure the deformed surfaces of the RVE stay periodic.   Next is the â€™Constraint functions fileâ€™. This file writes all the constraints for the nodes based on the different sets that are defined by Omairey et al. [1]. Omairey et al. [1] illustrate these sets in Figure 6. The linear constraints and load boundary conditions are given in Table 1 in their work [1].   The final file is the â€™script fileâ€™. The basis of this file comes directly from the work by Omairey et al. [1]. It has been adapted to work with the other files and with the model as created for the current research. The â€™script fileâ€™ writes the script file that is imported in Abaqus to run the simulations based on the Â elastic properties that have been chosen in the â€™RVE set-up fileâ€™. This is the part of the script where the strains to be applied to the RVE are set. It is also the part where the reaction forces are obtained that are used to calculate the Poison ratioâ€™s, Youngâ€™s moduli and Shear moduli. Finally the script prints the desired results for the elastic properties and writes these to a text file.   [1] Sadik L Omairey, Peter D Dunning, and Srinivas Sriramula. â€œDevelopment of an ABAQUS plugin tool for periodic RVE homogenisationâ€. In: Engineering with Computers 35.2 (2019), pp. 567â€“577.",,
ebf7f5d3-e591-420c-8d2f-861b2391ed5b,magma,MAGMa,- Supports mass spectrometrists to chemically annotate fragmentation data - Retrieves and ranks candidate molecules and substructures for parent and product ions - Provides metabolic reaction rules to generate candidate molecules - Provides a powerful user interface to interpret the results,https://github.com/NLeSC/MAGMa,NLeSC/MAGMa
ebfa65c4-d8ec-4280-a0cd-5b05c8362089,cerulean,cerulean,"* Python 3 library for talking to HPC clusters and supercomputers * Copy files to and from remote machines * Start processes locally or through SSH * Manage jobs on a local or remote scheduler * Python 3 / SSH / SFTP / Slurm / Torque  Cerulean is a Python 3 library for talking to HPC clusters and supercomputers. It lets you copy files between local and SFTP filesystems, it lets you start processes locally and remotely via SSH, and it lets you submit jobs to schedulers such as Slurm and Torque/PBS. The file access functions of Cerulean use a pathlib-like API, but unlike in pathlib, Cerulean supports remote file systems. That means that there is no longer just the local file system, but multiple file systems, and that Path objects have a particular file system that they are on. On High-Performance Computing machines, you donâ€™t run commands directly. Instead, you submit batch jobs to a scheduler, which will place them in a queue, and run them when everyone else in line before you is done. With Cerulean, you can submit jobs to a scheduler and track their progress, using a simple Python API.",https://github.com/NaturalHPC/cerulean,NaturalHPC/cerulean
ec7a05be-b4de-4d76-9308-92d9e98d5453,unrollingaverages,UnrollingAverages.jl,"## Overview   UnrollingAverages is a Julia package aimed at deconvolving simple moving averages of time series to get the original ones back.  UnrollingAverages currently assumes that the moving average is a [simple moving average](https://en.wikipedia.org/wiki/Moving_average#Simple_moving_average). Further relaxations and extensions may come in the future, see [Future Developments](#Future-Developments) section.  ## Installation  Press `]` in the Julia REPL and then  ```julia pkg> add UnrollingAverages ```  ## Usage  The package exports a single function called `unroll`: it returns a `Vector` whose elements are the possible original time series.  ```julia unroll(moving_average::Vector{Float64}, window::Int64; initial_conditions::U = nothing, assert_natural::Bool = false) where { U <: Union{ Tuple{Vararg{Union{Int64,Float64}}},Nothing} } ```  ### Arguments  - `moving_average`: the time series representing the moving average to unroll ; - `window`: the width of the moving average ; - `initial_conditions`: the initial values of the original time series to be recovered. It may be a `Tuple` of `window-1` positive integer values, or `nothing` if initial conditions are unknown. Currently it is not possible to specify values in the middle of the time series, this may be a feature to be added in the future ; - `assert_natural` default boolean argument. If true, the pipeline will try to recover a time series of natural numbers only. More then one acceptable time series (where ""acceptable"" means that it reproduces `moving_average`) may be found and all will be returned.  A few remarks:  1. If `isnothing(initial_conditions)`:    - `if assert_natural`, then an internal `unroll_iterative` method is called, which tries to exactly recover the whole time series, initial conditions included. Enter `?UnrollingAverages.unroll_iterative` in a Julia  to read further details;    - `if !assert_natural`, then an internal `unroll_linear_approximation` method is called. See this [StackExchange post](https://stats.stackexchange.com/a/68002). NB: this is an approximated method, it will generally not return the exact original time series; 2. If `typeof(initial_conditions) <: Ntuple{window-1, <:Union{Int64,Float64}}`, then an internal `unroll_recursive` method is called, which exactly recovers the time series. Mathematical details about this function are reported in the [documentation](https://InPhyT.github.io/UnrollingAverages.jl/stable), and you may read more by entering `?UnrollingAverages.unroll_recursive`.  ## Future Developments  - Modify `initial_conditions` argument of `unroll` so that it accepts known values throughout the series ; - Implement reversing methods for other types of moving averages .  ## How to Contribute  If you wish to change or add some functionality, please file an [issue](https://github.com/InPhyT/UnrollingAverages.jl/issues). Some suggestions may be found in the [Future Developments](#Future-Developments) section.  ## How to Cite   If you use this package in your work, please cite this repository using the metadata in [`CITATION.bib`](https://github.com/InPhyT/UnrollingAverages.jl/blob/main/CITATION.bib).  ## Announcements   - [Twitter](https://twitter.com/In_Phy_T/status/1461358804730204168?s=20&t=OzJAPLw6gd2WsGjiqlYeWA) - [Discourse](https://discourse.julialang.org/t/ann-unrollingaverages-jl-a-package-to-deconvolve-time-series-data/81684) - [Forem](https://forem.julialang.org/inphyt/ann-unrollingaveragesjl-a-package-to-deconvolve-time-series-data-5bbp)",https://github.com/InPhyT/UnrollingAverages.jl,InPhyT/UnrollingAverages.jl
ecef4689-11ea-4719-8a79-1c4a90d65f95,4tu-generics-library-for-datatype-generic-programming-in-agda,Generics library for datatype-generic programming in Agda,"This archive contains the source code for the Generics Agda library for safe, typed datatype-generic programming.     This archive is intended for use as an artifact in a journal submission.     To follow the latest development of the Generics library, please go to the Github repository.",,
ed34b59f-86d0-445b-929b-08d5a8bac41b,cat,CAT,"* Offers interface to ADF and CP2K * Build automatically the ligand-core geometry for a given ligand-quantum dot pair * Optimize automatically the ligand * Compute quantum chemistry properties for the ligand * Allows to provide the configuration input as a YAML file * Stores the results in a HDF5 file  For a more detailed description of the CAT compound builder read the documentation. The documentation is divided into three parts: The basics, further details about the input cores and ligands and finally a more detailed look into the customization of the various jobs.",https://github.com/nlesc-nano/CAT,nlesc-nano/CAT
ed35d184-a69e-4dba-91c5-9b226c24ca64,4tu-code-to-produce-the-results-of-the-publication-disentangling-acceleration-velocity-and-duration-dependency-of-the-short-and-medium-latency-stretch-reflexes-in-the-ankle-plantarflexors,"Code to produce the results of the publication: â€œDisentangling acceleration-, velocity- and duration-dependency of the short- and medium-latency stretch reflexes in the ankle plantarflexorsâ€","This folder contains code to produce the results of ""R. C. van â€™t Veld, E. H. F. van Asseldonk, H. van der Kooij and A. C. Schouten. Disentangling acceleration-, velocity- and duration-dependency of the short- and medium-latency stretch reflexes in the ankle plantarflexors. Journal of Neurophysiology. https://doi.org/10.1152/jn.00704.2020""",,
ed63311c-17af-4cd2-a63f-74f44ecc4f3e,4tu-code-underlying-the-publication-turbulent-exchange-of-co2-in-the-lower-tropical-troposphere-across-clear-to-cloudy-conditions,Code underlying the publication: Turbulent Exchange of CO2 in the Lower Tropical Troposphere Across Clear-to-Cloudy Conditions,"In this repository we present the code and/or necessary data to reproduce the results of the study: ""Turbulent Exchange of CO2 in the Lower Tropical Troposphere Across Clear-to-Cloudy Conditions"". Additional data on the campaign can be requested via https://www.cloudroots.wur.nl. A dedicated data paper with the required data is currently being developed and can be accessed at a later stage through this website.",,
ed7278c4-812a-466f-ab84-84bc91d7ef1e,check-deps,check-deps,"Sometimes, when you have a project that uses many components there are no easy ways to systematically check if dependencies are installed. If you stick to a single programming language, this problem is better handled by a package manager. When you step outside the bounds of a single language however, there is no easy to use tool. Maybe you need some UNIX tools to be present along with utilities from Python and some scripts that run on top of Node.  The goal of this script is to check software dependencies for you. If you have some complicated setup which requires a combination of executables, libraries for different languages etc., this script can check if those are in order.  You specify the dependencies in a `dependencies.ini` file, then this script checks them. You only need Python installed, nothing else for this script to work. You simply ship this stand-alone script with your distribution.",https://github.com/jhidding/check-deps,jhidding/check-deps
ede744f3-7d0a-4a5f-9c9d-e5f8b38bc79c,fm128-radar,fm128-radar,* Provides a library to write ascii based fm128_radar radar files to be used in WRFDA * Example for KNMI radar data available at https://github.com/ERA-URBAN/fm128_radar_knmi  fm128-radar provides a python library to write ascii based fm128_radar files to be used in WRFDA. Ascii based fm128_radar files is the only accepted file format for data assimilation of radar data through WRFDA.,https://github.com/ERA-URBAN/fm128_radar,ERA-URBAN/fm128_radar
ee357db4-6d3f-4647-8176-860b2716a422,neuroanalyzer,NeuroAnalyzer,"Welcome fellow researcher!  NeuroAnalyzer is a [Julia](https://julialang.org) toolbox for analyzing neurophysiological data. Currently, it covers importing, editing, processing, visualizing, and analyzing EEG, MEP and EDA data. Preliminary functionality is also available for MEG, NIRS, ECoG, SEEG and iEEG recordings.  Various methods for modeling non-invasive brain stimulation protocols (tDCS/tACS/tRNS/tPCS/TMS/TUS/INS) will also be implemented (NeuroStim submodule). Another submodule, NeuroTester, will allow designing and running psychological studies. Certain neurophysiological data can be recorded using NeuroRecorder submodule.  NeuroAnalyzer contains a set of separate (high- and low-level) functions. Some interactive graphical user interface (GUI) functions are also available. NeuroAnalyzer functions can be combined into an analysis pipeline, i.e. a Julia script containing all steps of your analysis. This, combined with processing power of Julia language and easiness of distributing calculations across computing cluster, will make NeuroAnalyzer particularly useful for processing large amounts of neurophysiological data.  NeuroAnalyzer is a collaborative, non-commercial project, developed for researchers in psychiatry, neurology and neuroscience.  Every contribution (bug reports, fixes, new ideas, feature requests or additions, documentation improvements, etc.) to the project is highly welcomed.  NeuroAnalyzer website is located at [https://neuroanalyzer.org](https://neuroanalyzer.org).  You may also follow NeuroAnalyzer on [Mastodon](https://fediscience.org/web/tags/neuroanalyzer).",https://codeberg.org/AdamWysokinski/NeuroAnalyzer.jl,
ee3cbcdd-5508-451f-8448-1b9c0e8755dc,4tu-microscopy-data-code-and-analysis-underlying-the-publication-optical-stem-detection-for-scanning-electron-microscopy,"Microscopy data, code and analysis underlying the publication ""Optical STEM detection for scanning electron microscopy""","Raw microscopy data underlying the publication, structured per experiment (figure). The imaging setting that is varied in the experiments (exposure/dwell time, beam current, landing energy) is indicated by the folder or image name. The images may have a data bar indicating the relevant acquisition settings.  For more information on the sample preparation and image acquisition, see the Methodology section in the publication.The code and data analysis notebooks are included in the accompanying Github repository: https://github.com/hoogenboom-group/Kievits-OSTEM-2023   Data set structure:  1_Optimization-OSTEM: contains the images for the landing energy optimization (Figure 2 in the article). Images have a pixel size of 1 nm and where acquired with increments of 500 eV (electron volts). Dwell times are indicated in the folder names. The 'Qualitative' folder contains a subset of these data for ease of plotting Figure 2A.2_Qualitative-comparison: contains qualitative images of rat pancreas and zebrafish larva (Figure 3). Both raw images (with metadata in TIFF header) and cropped images are included (used for publication). Images have a pixel size of 4 nm.3_SNR-comparison-detectors: contains images used for the quantitative comparison of detectors (Figure 4). Contains a folder per detection technique with all the images. The 'Qualitative' folder contains a subset of these data for ease of plotting Figure 4A.4_Image-resolution-detectors: contains raw images and extracted edge widths from FEI Image (Figure 5). The acquisition settings are indicated in the folder names, with one folder per detection technique.5_Current-SNR-relation: contains the images with increasing beam current / fixed dwell time and increasing dwell time / fixed current (Figure 6). The 'Qualitative' folder contains a subset of the data for ease of plotting Figure 6A and 6B.6_Supplementary: contains the supplementary data, used for the background texture analysis. The results in Figure S4 and Table S1 are based on these images.   General acquisition parameters  Pixel size = 1 nm (unless stated otherwise)  Beam current = 0.4nA (unless stated otherwise)  BSD, SE Landing energy = 1.5keV (typically)  OSTEM landing energy = 4keV (typically)  ADF-STEM landing energy = 25keV",https://data.4tu.nl/v3/datasets/e9adfc91-018d-4156-a7c1-189643e4c3b7.git,
ee486ecc-0f17-41b2-835b-9a72b2612dda,4tu-code-underlying-the-publication-integrity-based-explanations-for-fostering-appropriate-trust-in-ai-agents,Code underlying the publication: Integrity-based Explanations for Fostering Appropriate Trust in AI Agents,"This repository includes the software code that was developed for the publication titled ""Integrity-based Explanations for Fostering Appropriate Trust in AI Agents"".    Research Objective: How does the expression of different principles of integrity through explanation affect the appropriateness of humanâ€™s trust in the AI agent?Type of research: EmpiricalCode Environment: Microsoft Power Platform - Power AppsType of Code: Power Apps Solution. Solutions are the mechanism for implementing application lifecycle management (ALM) in Power Apps.Method of data collection: Participants were provided guest credentials to login in Power Apps platform and interact with an AI agent to estimate calories of a food plate. The AI agent provided different types of integrity-laden explanations to help the participant in estimating the calories. Data was collected regarding reliance of the participant on the AI agent in form of 0 (not relied) and 1 (relied).",,
eec6c4b1-d274-4e3a-aa66-521dfc3b6c84,4tu-specification-translator-artifact-for-ifm-2023-paper-joining-forces-reusing-contracts-for-deductive-verifiers-through-automatic-translation,"Specification Translator: Artifact for iFM 2023 paper ""Joining Forces! Reusing Contracts for Deductive Verifiers through Automatic Translation""","We present the Specification Translator, a tool that has been implemented as part of our research titled ""Joining Forces! Reusing Contracts for Deductive Verifiers through Automatic Translation"".  hen using deductive verifiers to prove a programâ€™s correctness, users have to write formal specifications.  Unfortunately, each tool uses its own specification language for this, which makes it difficult to reuse the specifications.  This tool makes this process quicker &amp; easier by automatically translating such specifications in verified Java programs from one specification language to another. It supports the tools Krakatoa, OpenJML and VerCors.  The tool takes an annotated Java program and a target tool as input. It will then generate an annotated Java program where the annotations have been translated.  &nbsp;  This artifact allows you to reproduce the evaluation discussed in the paper.  For more information, we refer to the ReadMe inside the artifact.",,
eeeb5890-70fd-4f92-b9d2-d57453dd8599,4tu-artifact-for-the-paper-fast-verified-sccs-for-probabilistic-model-checking,"Artifact for the Paper ""Fast Verified SCCs for Probabilistic Model Checking""","The artifact that backs up the data in our ATVA 2023 Paper titled ""Fast Verified SCCs for Probabilistic Model Checking"". This artifact contains the proof files for Isabelle/HOL. Running the proofs yields a LLVM implementation of Gabow's algorithm that can be compiled to a dynamic library. This artifact also contains a version of the Modest toolset that can use this library to replace the built-in SCC algorithm. Lastly, the artifact contains scripts that reproduce the results in the paper. The additional reference contains a link to browsable version of the artifact that allows proofs to be inspected without downloading anything.",,
ef4802b7-1162-4d0d-9501-1baa6bde9c77,4tu-applywood-a-calculation-tool-supporting-the-design-of-plywood-based-seismic-retrofitting-interventions-on-existing-timber-floors,ApPlyWood: a calculation tool supporting the design of plywood-based seismic retrofitting interventions on existing timber floors,The purpose of this software is to estimate the in-plane seismic response of timber diaphragms retrofitted with plywood panels fastened along their perimeter to the existing sheathing. The program can support professional engineers in the preliminary design and dimensioning of plywood-based seismic retrofitting interventions on timber floors in existing buildings.,,
f003f909-7c23-432d-a740-9dd969de0165,mpijl,MPI.jl,,https://github.com/JuliaParallel/MPI.jl,JuliaParallel/MPI.jl
f01aebbb-35d3-450a-ac87-f85ee7c8c594,4tu-hodge-edge-gp,Hodge-Edge-GP,"This implements the Gaussian processes on the edge space of a simplicial 2-complex, including applications in forex prediction, ocean current interpolation and water networks state estimation. Our way of constructing the edge Gaussian processes allows separate modeling of the different parts of the edge signals, e.g., divergence-free part or curl-free part. This code shows how we can use Gaussian process regression to interpolate or predict signals on the edge space while respecting their intrinsic properties.",https://data.4tu.nl/v3/datasets/08af2aa7-c1f1-47d6-9f43-b397250ba8f5.git,
f030d10d-9b87-4d7d-a416-9cd0827e1beb,4tu-python-models-underlying-the-master-thesis-managing-water-and-energy-on-small-touristic-islands-study-case-caye-chapel,Python Models underlying the master thesis: Managing Water and Energy on Small Touristic Islands: study case Caye Chapel,"The collection contains twelve different models. There is one model for each of the twelve (12) alternatives for the water-energy system described in the thesis ""Managing Water and Energy on Small Touristic Islands: study case Caye Chapel"". The input data for each model is contained in the .txt files.",,
f07cccb2-286c-40cc-a6a2-d3831fc17bce,trex2champ,trex2champ,,https://github.com/neelravi/trex2champ,neelravi/trex2champ
f09bd44a-0bf3-4be0-afab-7f118857d31f,shifting-concepts-through-time,ShiCo,* Provides historians with a tool for exploring concept shifts  * Explores changes on words related to a seed word  * It provides a unique way of using word2vec technology over different time intervals   ShiCo is a tool for visualizing time shifting concepts. We refer to a concept as the set of words which are related to a given seed word. ShiCo uses a set of semantic models (word2vec) spanning a number of years to explore how concepts change over time -- words related to a given concept at time t=0 may differ from the words related to the same concept at time t=n.,https://github.com/NLeSC/ShiCo,NLeSC/ShiCo
f09bfad2-9ab5-49c4-bf7f-b1d7b1ad2a1a,votca-xtp,votca-xtp,"* excited state properties via GW-BSE  * spectra using QM/MM in conjunction with excited state treatment via GW-BSE  * energetic disorder for electrons/holes and excitions via QM/MM or fast MM/MM multipole methods  * electron and hole mobilities using electronic couplings and kinetic Monte-Carlo  * electron, hole, singlet and triplet diffusion constants using electronic couplings and kinetic Monte-Carlo   VOTCA is a software package which focuses on the analysis of molecular dynamics data, the development of systematic coarse-graining techniques as well as methods used for simulating microscopic charge (and exciton) transport in disordered semiconductors.VOTCA contains the packages CSG, which provides coarse graining routines and XTP, which allows to extract electronic properties from MD calculations",https://github.com/votca/xtp,votca/xtp
f129be93-aeba-46a5-b066-91123447d0c7,tweetcollector,Tweet_collector,,https://github.com/UtrechtUniversity/tweet_collector,UtrechtUniversity/tweet_collector
f1560c36-09b4-47fc-9225-8ee085c2116e,hadrianus-scripts,Hadrianus scripts,"* Gives examples of how to define a common ontology for very different databases * Combine cultural heritage heterogeneous database infrastructures using linked data tools and ontologies * Every cultural heritage scholar has a unique perspective on their subject; this is reflected in their databases * The Humanities treasure and stimulate such richness of perspectives; this is a challenge for Digital Humanities and a great opportunity for eScience  In the Hadrianus project, we aimed to enrich the existing [Hadrianus website](http://www.hadrianus.it)  with data from various other sources. The Hadrianus website revolves around the connection between Dutch artists and the city of Rome. While the current manually curated database offers a very rich picture of this connection already, many more discoveries could be made by adding data on the same or related artists and works of art from other sources. The challenge of this project was to figure out how to do this in an automated way that would enable some uniform way of linking and comparing data, but still retain all the richness of detail that each database brings to the table. In this set of scripts, we have explored one way to do so, namely by using the linked data Virtuoso platform, that can transform relational databases to linked databases. The biggest challenge in this was to define a common ontology.",https://github.com/NLeSC/hadrianus-scripts,NLeSC/hadrianus-scripts
f17e1133-23a5-4463-b552-9b0a155515c2,jupyterlab-thredds,JupyterLab dataset browser for THREDDS,* Provides an easy way to select and use a dataset from a THREDDS catalog in a Jupyter notebook * Offers several ways to open a dataset in Python using graphical user interface instead of copy/pasting urls and having to figure out the right code snippet,https://github.com/eWaterCycle/jupyterlab_thredds,eWaterCycle/jupyterlab_thredds
f1a3281e-df70-4949-9cf4-60053b6ae3bb,4tu-musician,MUSICiAn,"This repository consists of code to reproduce the results of the paper ""MUSICiAn: Genome-wide Identification of Genes Involved in DNA Repair via Control-Free Mutational Spectra Analysis"".     The code to reproduce the paper is shared at: https://github.com/joanagoncalveslab/MUSICiAn     No experimental data was collected as part of this project, this project only utilised previously publicly available datasets. Details on accessing this data are fully described in the Methods section; ""Data and preprocessing"", ""Mutational outcome data"".",,
f1a94b86-ca6c-403a-b294-40377242352e,astronomical,AstronomicAL,,https://github.com/grant-m-s/AstronomicAL,grant-m-s/AstronomicAL
f1c8ae2e-7085-45be-97d9-46aaabe9259a,4tu-mrf-undersampling-optimization-software-and-code-for-the-publication-mitigating-undersampling-errors-in-magnetic-resonance-fingerprinting-by-sequence-optimization,MRF undersampling optimization: Software and code for the publication: Mitigating undersampling errors in Magnetic Resonance Fingerprinting by sequence optimization,"Repository to perform sequence optimization as proposed in the following paper:   Mitigating undersampling errors in Magnetic Resonance Fingerprinting by sequence optimization  D.G.J. Heesterbeek, K. Koolstra, M.J.P. van Osch, M.B. van Gijzen, F.M. Vos and M.A. NagtegaalÂ    Abstract:   Magnetic Resonance Fingerprinting (MRF) is  an imaging technique for simultaneous estimation of multiple quantitative parameters in a single acquisition. The efficiency of MRF results from strong k-space undersampling, but this can go at the expense of strong aliasing artifacts. The errors due to undersampling can be predicted based on a mathematical model leveraging on perturbation  theory. We exploited this model to perform MRF sequence optimization by adjusting the MRF flip angle train. Numerical simulations showed that the undersampling errors can be suppressed by sequence optimization. This was further corroborated in eight in vivo scans. The results were compared to sequences with a conventionally shaped flip angle pattern and an optimized pattern based on the Cramer-Rao lower bound (CRB). A sequence optimized for improved  robustness against undersampling with a flip angle train of length 400 yielded significantly lower median absolute errors T1 (5.6%Â±2.9%) and T2 (7.9%Â±2.3%) compared to the conventional (T1: 8.0% Â± 1.9%, T2: 14.5% Â± 2.6%) and CRB-based (T1: 21.6% Â± 4.1%, T2: 31.4% Â± 4.4%) sequences. The proposed optimization scheme can be adapted to different  scan settings, reference maps and optimization parameters in a straightforward manner.",,
f1f34961-69d0-435b-8240-e096d4601f1d,crowded,CrowdED,"CrowdED is a two-staged statistical guideline for optimal crowdsourcing experimental design to a-priori estimate optimal workers and tasks' assignment to obtain maximum accuracy on all tasks.  [CrowdApp](https://pedrohserrano.shinyapps.io/crowdapp/) Beta  ## Table of Contents  - [Installation](#installation) - [Examples](#examples) - [Citing this work](#citing-this-work) - [License](#license)  ## Installation  To install the package, please use the `pip` installation as follows:  ```shell pip install crowdED ``` ## Installing from source (Optional)  ``` git clone https://github.com/MaastrichtU-IDS/crowdED.git cd crowdED pip install --editable ./ ``` > Note: currently, crowdED is only compatible with Python 3.6.  ## Examples  ### Create a synthetic dataset of tasks You will need to run `!pip install shortuuid`  ```python import crowded.simulate as cs  #define your parameters total_tasks = 415 p_hard_tasks = 0.4 number_of_valid_answers = 3  #create task dataset df_tasks = cs.Tasks(number_of_valid_answers).create(total_tasks, p_hard_tasks) ``` #### Create a synthetic dataset of workers  ```python import crowded.simulate as cs  #define your parameters total_workers = 40 alpha = 28 beta = 2 #create task dataset df_workers = cs.Workers(alpha, beta).create(total_workers) ``` #### Assign easily and fairly workers to tasks  ```python import crowded.simulate as cs  #workers per task should always be smaller than the number of workers wpt = 5 #create assignment df_tw = cs.AssignTasks(df_tasks, df_workers, wpt).create() ``` #### Compute Bayes probability and predict worker answers ```python import crowded.method as cs  #workers per task should always be smaller than the number of workers wpt = 5 #create assignment df_tw = cs.AssignTasks(df_tasks, df_workers, wpt).create() ``` #### Compute Bayes probability and Predict answers of the workers ```python import crowded.method as cm  #define the parameters x = df_tw['prob_task'] #vector of probabilities of tasks y = df_tw['prob_worker'] #vector of probabilities of workers z = df_tasks['true_answers'].unique()  #vector of valid answers in the experiment #compute probability cp = cm.ComputeProbability(x, y, z) ``` ```python import crowded.method as cm  #define the parameters g = df_tw['true_answers'] #vector of gold standard answers p = cp.predict() #binary vector of 0 and 1 z = df_tasks['true_answers'].unique()  #vector of valid answers in the experiment #compute match worker_answer = cm.WorkerAnswer(g, p, z) #add the answers to the assignment dataset df_tw['worker_answers'] = worker_answer.match() ```  ## Citing this work  If you use CrowdED in a scientific publication, you are highly encouraged (not required) to cite the following paper:  CrowdED: Guideline for Optimal Crowdsourcing Experimental Design. Amrapali Zaveri, Pedro Hernandez Serrano Manisha Desai and Michel Dumontier [https://doi.org/10.1145/3184558.3191543](https://doi.org/10.1145/3184558.3191543).  Bibtex entry:          @inproceedings{Zaveri:2018:CGO:3184558.3191543,         author = {Zaveri, Amrapali and Serrano, Pedro Hernandez and Desai, Manisha and Dumontier, Michel},         title = {CrowdED: Guideline for Optimal Crowdsourcing Experimental Design},         booktitle = {Companion Proceedings of the The Web Conference 2018},         series = {WWW '18},         year = {2018},         isbn = {978-1-4503-5640-4},         location = {Lyon, France},         pages = {1109--1116},         numpages = {8},         url = {https://doi.org/10.1145/3184558.3191543},         doi = {10.1145/3184558.3191543},         acmid = {3191543},         publisher = {International World Wide Web Conferences Steering Committee},         address = {Republic and Canton of Geneva, Switzerland},         keywords = {biomedical, crowdsourcing, data quality, data science, fair, metadata, reproducibility},         }  ## License  This repository is licensed under the terms of [MIT LICENSE](https://github.com/MaastrichtU-IDS/crowdED/blob/master/LICENSE)",https://github.com/MaastrichtU-IDS/crowdED,MaastrichtU-IDS/crowdED
f1f5e076-8334-4a66-8702-ca9bfa5f8fa7,4tu-python-notebook-to-generate-the-cell-sizes-changes-across-yeast-mutants-figure-3-panel-d-from-paper-adaptability-and-evolution-of-the-cell-polarization-machinery-in-budding-yeast,"Python notebook to generate the cell sizes changes across yeast mutants (Figure 3  Panel D from paper: ""Adaptability and evolution of the cell polarization machinery in budding yeast"")","This is the jupyter notebook that generate  the Figure 3 , Panel D from paper: ""Adaptability and evolution of the cell polarization machinery in budding yeast"".",,
f202543a-9769-4694-afca-6fe4785a09a7,holodev,holodev,,https://github.com/joenio/holodev,joenio/holodev
f22d166d-bc77-4295-92e5-aab2657cd6f4,piedmont-covid-19-data-modelling-management,COVID-19 Piedmont,"# Piedmont COVID-19 Data Modelling & Management  [![License: MIT](https://img.shields.io/badge/License-MIT-green.svg)](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/LICENSE) [![Language: Italian](https://img.shields.io/badge/Language-Italian-blue.svg)](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/README-ITA.md)  [![Documentation: Manuale](https://img.shields.io/badge/Docs-Manuale-lightblue.svg)](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/docs/README-ITA.md) [![Language: English](https://img.shields.io/badge/Language-English-red.svg)](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/README.md) [![Documentation: Manual](https://img.shields.io/badge/Docs-Manual-orange.svg)](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/docs/README.md) [![DOI: Zenodo](https://zenodo.org/badge/571272348.svg)](https://zenodo.org/badge/latestdoi/571272348)  This repository contains the [code](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/src), [documentation manual](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/docs) and [data visualisations](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/images/plots) for the design and operation of the Piedmont COVID-19 surveillance data modelling and management pipeline developed in collaboration with the Piedmont Epidemiological Service ([SEPI](https://www.epi.piemonte.it/)).  For privacy purposes all the data in this repository are either [`fake`](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/data/fake-input) (i.e. invented) or [`synthetic`](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/data/synthetic-input) (i.e. simulated) in order to be structurally equivalent to the original individual-level data to accurately showcase the functionalities of the data modelling and management pipeline.   The only reference to the real data can be found in the plots located in the [`images/real-output`](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/images/plots/real-output) folder.   ## How to Access  If you would like to access the real Piedmont COVID-19 surveillance data covering the year 2020 for your research project (i.e. sequences, incidences and empirical time delay distributions visualised [here](https://github.com/UniTo-SEPI/COVID-19_Piedmont/tree/main/images/plots/real-output)), please feel free to contact us by sending us an [email](inphyt@gmail.com).   ## How to Cite  If you use these contents in your work, please cite this repository using the metadata in [`CITATION.bib`](https://github.com/UniTo-SEPI/COVID-19_Piedmont/blob/main/CITATION.bib).  ## References   ### Data   1. CSI Piemonte (2020) [Piedmont Region COVID-19 Data Management Platform](https://www.csipiemonte.it/en/project/piedmont-region-covid-19-platform). *CSI Piemonte* 2. CSI Piemonte (2020) [GESCOVID19: COVID-19 Data Management Platform in Piedmont](https://github.com/regione-piemonte/gescovid19). *GitHub* 3. Leproni (2021) [The Piedmont Region COVID-19 Platform](https://www.masteradabi.it/images/CSI_Piattaforma_COVID_20210308_V2.pdf). *CSI Piemonte* 4. Moroni and Monticone (2022) [Italian COVID-19 Integrated Surveillance Dataset](https://doi.org/10.5281/zenodo.5748141). *Zenodo*  ### Software   1. Monticone and Moroni (2022) [ICD_GEMs.jl: A Julia Package to Translate Between ICD-9 and ICD-10 Codes](https://doi.org/10.5281/zenodo.6564434). *Zenodo* 2. Monticone and Moroni (2022) [UnrollingAverages.jl: A Julia Package to Deconvolve Time Series Data.](https://doi.org/10.5281/zenodo.5725301). *Zenodo*  ### Papers   * Del Manso et al. (2020) [COVID-19 integrated surveillance in Italy: outputs and related activities](https://doi.org/10.19191/EP20.5-6.S2.105). *Epidemiologia & Prevenzione* * Milani et al. (2021). [Characteristics of patients affecting the duration of positivity at SARS-CoV-2: a cohort analysis of the first wave of epidemic in Italy](https://epiprev.it/5814). *Epidemiologia & Prevenzione*  * Starnini et al. (2021) [Impact of data accuracy on the evaluation of COVID-19 mitigation policies](https://www.doi.org/10.1017/dap.2021.25). *Data & Policy*, 3, E28.  * Zhang et al. (2021) [Data science approaches to confronting the COVID-19 pandemic: a narrative review](https://doi.org/10.1098/rsta.2021.0127). *Philosophical Transactions of the Royal Society A* * Vasiliauskaite et al. (2021) [On some fundamental challenges in monitoring epidemics](https://doi.org/10.1098/rsta.2021.0117). *Philosophical Transactions of the Royal Society A* * Badker et al. (2021) [Challenges in reported COVID-19 data: best practices and recommendations for future epidemics](http://dx.doi.org/10.1136/bmjgh-2021-005542). *BMJ Global Health* * Shadbolt et al. (2022) [The Challenges of Data in Future Pandemics](https://doi.org/10.1016/j.epidem.2022.100612). *Epidemics*",https://github.com/UniTo-SEPI/COVID-19_Piedmont,UniTo-SEPI/COVID-19_Piedmont
f2754d1c-fe91-4209-9c12-8144550f9a17,kernel-tuner,Kernel Tuner,"* Allows developers to easily unit test and auto-tune GPU code * Generic auto-tuning of user-defined parameters for CUDA, OpenCL, and C kernels * Supports more than 20 different search optimization methods to speedup tuning * Successfully used in 10+ different eScience projects, across various disciplines  Kernel Tuner simplifies the development of efficient GPU programs, or _kernels_. It does so by making kernels written in C/C++, OpenCL, or CUDA accessible from Python, while taking care of the required synchronization between data kept in host memory and data kept in device memory.  This has a number of advantages. First, it simplifies _auto-tuning_ of the kernel parameters. In fact, Kernel Tuner comes standard with a variety of strategies for efficiently searching the parameter space, leading to  greatly improved performance of tuned kernels. Second, it allows for unit testing of GPU code from within Python.   Kernel Tuner does not add any additional dependencies to the kernel code, and does not require extensive code changes. Furthermore, it is noteworthy that kernels tuned by Kernel Tuner do not require any changes after tuning to make them production ready--tuned kernels can be used as-is from any host programming language.",https://github.com/KernelTuner/kernel_tuner,KernelTuner/kernel_tuner
f2fc7f23-a66a-4297-bd2e-e4331b7efc76,ape-automated-pipeline-explorer,APE,"| Badges | | |:----:|----| | **Fairness** |  [![fair-software.eu](https://img.shields.io/badge/fair--software.eu-%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F%20%20%E2%97%8F-green)](https://fair-software.eu) [![OpenSSF Best Practices](https://bestpractices.coreinfrastructure.org/projects/5749/badge)](https://www.bestpractices.dev/projects/5749) | | **Packages and Releases** |  [![Latest release](https://img.shields.io/github/release/sanctuuary/APE.svg)](https://github.com/sanctuuary/APE/releases/latest) [![Maven Central](https://badgen.net/maven/v/maven-central/io.github.sanctuuary/APE)](https://mvnrepository.com/artifact/io.github.sanctuuary/APE) | **Build Status** | [![Build](https://github.com/sanctuuary/APE/actions/workflows/mvnbuild.yml/badge.svg)](https://github.com/sanctuuary/APE/actions/workflows/mvnbuild.yml) [![CodeQL Analysis](https://github.com/sanctuuary/APE/actions/workflows/codeql.yml/badge.svg)](https://github.com/sanctuuary/APE/actions/workflows/codeql.yml) | | **Documentation** | [![Documentation Status](https://readthedocs.org/projects/ape-framework/badge/?version=latest)](https://ape-framework.readthedocs.io/en/latest/?badge=latest) | | **DOI** | [![DOI](https://zenodo.org/badge/227861653.svg)](https://zenodo.org/badge/latestdoi/227861653) | | **License** |  [![GitHub license](https://img.shields.io/github/license/sanctuuary/APE)](https://github.com/sanctuuary/APE/blob/master/LICENSE) |   APE is a command line tool and Java API for the automated exploration of possible computational pipelines (scientific workflows) from large collections of computational tools.   APE relies on a semantic domain model that includes tool and type taxonomies as controlled vocabularies for the description of computational tools, and functional tool annotations (inputs, outputs, operations performed) using terms from these taxonomies. APE then computes possible workflows based on this domain model and a specification of the available workflow inputs, the intended workflow outputs and possibly additional constraints.   Internally, APE uses a component-based program synthesis approach. It translates the domain knowledge and workflow specification into logical formulas that are then fed to a SAT solver to compute satisfying instances. These solutions are then translated into the actual candidate workflows. For a detailed description, we refer to [[1]](#1).  This [video](https://www.youtube.com/watch?v=CzecqRJXmoM) explains APE in 5 minutes.",https://github.com/sanctuuary/ape,sanctuuary/ape
f32b5070-fbbe-4050-913c-c110d396b8b3,mcfly,mcfly,"* Provides starting point for researchers to use deep learning  * Creates deep learning models for classification and regression on time series data * Derives features automatically from raw data * Helps with finding a suitable model architecture and hyperparameters * Has a tutorial in Python to get you started!   Deep learning is a powerful tool to help with automated classification or regression tasks. However, designing a deep learning network that works well for your data is not trivial: it requires the user to choose the number of layers in the network, the number of nodes in each layer, the type of each layer, and so forth. With so many degrees of freedom, finding the network that is right for your data is an arduous task. Moreover, each network still needs to be calibrated or _trained_ before it can be usefully applied to automated classification or regression tasks.   mcfly simplifies this process by making explicit the required steps while offering useful default values at each step. mcfly then proceeds by trying out many different network configurations, training each one to the data provided by the user. It subsequently lists the performance of each network, along with a visualization that helps the user judge each network's tendency to overfit or underfit the data.",https://github.com/NLeSC/mcfly,NLeSC/mcfly
f3ebaccc-cab2-44b4-b2ff-4dbd69bd165c,2022-vlach-001,Epidemic Network,"![Interface screenshot](netlogo_implementation/documentation/Epidemic_Network_Model_v1%20interface.png)  See full list of documentation resources in [`documentation`](documentation/tableOfContents.md).    ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |seed|integer|random generator seed number used as `random-seed seed`.| |initial-households|integer|initial count of households.| |population-coefficient|integer|household size variance coefficient (mean of exponential distribution).| |initial-clusters|integer|initial count of clusters/settlements.| |cluster-radius-min, cluster-radius-max|integer|minimum/maximum cluster radius used to sample a random cluster radius from an uniform distribution, as `in-radius (min-cluster-radius + (random max-cluster-radius))`.| |search-radius|integer|maximum search distance to establish links.| |max-connections|integer|maximum number of connections (links) per distance range.| |initial-infected|integer|number of initial cases or people agents initialised with the ""incubation"" stage and a stochastic state of stage 1 counter.| |max-daily-contacts|integer|maximum number of connections to be considered contacts each day.| |duration-latent-mean, duration-latent-stddev|float|Mean and standard deviation of duration of the latent stage of the disease, used in `random-normal duration-latent-mean duration-latent-stddev`.| |duration-prodromal-mean, duration-prodromal-stddev|float|Mean and standard deviation of duration of the prodromal stage of the disease, used in `random-normal duration-prodromal-mean duration-prodromal-stddev`.| |duration-contagious-mean, duration-contagious-stddev|float|Mean and standard deviation of duration of the contagious stage of the disease, used in `random-normal duration-contagious-mean duration-contagious-stddev`.| |mortality-rate|float|Average mortality rate applied to all individuals (`people`) in the population.| |transmission-prob-range1-mean, transmission-prob-range1-stddev|float|Mean and standard deviation of transmission probability towards people within range 1 (search-radius * 0.125), used in `random-normal transmission-prob-range1-mean transmission-prob-range1-stddev`.| |transmission-prob-range2-mean, transmission-prob-range2-stddev|float|Mean and standard deviation of transmission probability towards people within range 2 (search-radius * 0.25), used in `random-normal transmission-prob-range2-mean transmission-prob-range2-stddev`.| |transmission-prob-range3-mean, transmission-prob-range3-stddev|float|Mean and standard deviation of transmission probability towards people within range 3 (search-radius * 0.5), used in `random-normal transmission-prob-range3-mean transmission-prob-range3-stddev`.| |transmission-prob-range4-mean, transmission-prob-range4-stddev|float|Mean and standard deviation of transmission probability towards people within range 4 (search-radius), used in `random-normal transmission-prob-range4-mean transmission-prob-range4-stddev`.| |range1-search-radius-share, range2-search-radius-share, range3-search-radius-share, range4-search-radius-share|float|the proportion of `search-radius` effective for each transmission range.| |range1-initial-connections-share, range2-initial-connections-share, range3-initial-connections-share, range4-initial-connections-share|float|the proportion of `max-connections` from which a random sample is taken as initial contacts of the infected people for each distance range.| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |pos-spread|integer|Count of positive disease transmission encounter of an individual (passed to other person)| |neg-spread|integer|Count of negative disease transmission encounter of an individual (not passed to other person)| |count-spread|integer|Cumulative number of daily contacts| |count-daily-contact|integer|Sum of all daily contacts for the actual day (reset each day)| |count-daily-transmission|integer|Sum of all daily disease transmission encounter for the actual day (reset each day)| |count-daily-transmission-pers|integer|Number of people with the potential to performing a disease transmission encounter for the actual day (reset each day)| |nw-betcen|float|Network metrics':' betweenness centrality| |nw-eigcen|float|Network metrics':' eigenvector centrality| |nw-clocen|float|Network metrics':' closeness centrality| |nw-clucoe|float|Network metrics':' clustering coefficient| |nw-mod|float|Network metrics':' modularity| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2022-Vlach-001,
f3fcedb9-20d2-4b91-a485-ae7a60230149,4tu-software-needed-in-data-availability-statement-about-resonant-waves-play-an-important-role-in-the-increasing-heat-waves-in-northern-hemisphere-mid-latitudes-under-global-warming,Software needed in Data Availability Statement about Resonant waves play an important role in the increasing heat waves in Northern Hemisphere mid-latitudes under global warming,The procedures and the data files needed to recreate the figures in the manuscript&nbsp;,,
f4019268-81c8-492d-94c8-0c08a97d5780,4tu-optimal-stretching,optimal-stretching,"MATLAB code to reproduce results presented in the paper ""Optimal Graph Stretching for Distributed Averaging"".     The source code is available as a git repository. Cached computation results are available as additional files.     Git repository  Relevant code is stored in the StretchSim directory.     Runs and visualises optimal graph stretching. The settings in StretchSim.m correspond exactly to the code that was executed to create the results in the paper. The code is fully deterministic and gives the exact same results as in the paper. By default, cached results are used.     See ARTIFACT-EVALUATION.md in the root folder for detailed end-user instructions.     Files  Cached computation results are optionally available, with filenames ending in .cache.zip. These allow inspecting the exact outputs without the need to re-run the computationally intensive experiments. Detailed usage instructions are available in the file ARTIFACT-EVALUATION.md inside the git repository.",https://data.4tu.nl/v3/datasets/02926494-4c9e-4558-924d-f2a26fbe2ca1.git,
f41fb7d7-087c-4f12-8288-343ad04968c1,xenon-grpc,Xenon gRPC server,"* Provides an easy-to-use interface for distributed computing developers who want to use a different language than Java * Xenon gRPC wraps the Xenon library in a server * Retains the Xenon library API as much as possible * Allows to the Xenon Java library to be called from C++, Python, Go, Ruby, C#, Node.js, Objective-C and PHP (any language supported by gRPC) using generated clients.",https://github.com/NLeSC/xenon-grpc,NLeSC/xenon-grpc
f46c6c0a-9937-4eb7-8f09-82adc48f356b,robist,ROBIST,,https://github.com/JustinStarreveld/ROBIST,JustinStarreveld/ROBIST
f47d85b3-1f45-4098-a5c9-1461db17d76e,knime-klifs,KLIFS KNIME nodes,* For cheminformations who are working with kinase proteins in KNIME workflows. * KNIME nodes to interact with KLIFS website,https://github.com/3D-e-Chem/knime-klifs,3D-e-Chem/knime-klifs
f4d791c1-0d1e-43fb-9c27-ff62c4848d47,4tu-djehuty,djehuty,"This dataset contains the source code of the data repository system developed by 4TU.ResearchData and Nikhef, named Djehuty.  The software is released on a monthly schedule and this is the release of June 2025 (version 25.6).",https://data.4tu.nl/v3/datasets/097bc286-9488-4307-8bd1-e92642de811d.git,
f4f8f624-11bc-4d56-8511-4e76804ea14a,4tu-code-to-test-the-implementation-of-nonlinear-bi-level-parent-child-mpc, Code to test the implementation of nonlinear Bi-level Parent-Child MPC,"Implementation, simulation, and testing the performance of Bi-level Parent-Child MPC while controlling a nonlinear system. More details can be found in the referenced paper, which will be added after publication.",https://data.4tu.nl/v3/datasets/1eea98c1-85c4-480d-bc6d-a9e00a94e59a.git,
f5060715-45f9-4fae-9dd9-c3776eb7a7fb,mahiru,Mahiru,"- Single infrastructure for data download, compute-to-data, data-to-compute, federated learning etc. - Decentralised and federated, with policies set by owners of data and software and processing requested by users. - In the proof-of-concept prototype stage, and currently not developed.",https://github.com/SecConNet/mahiru,SecConNet/mahiru
f55989bd-e605-47ce-a615-feab93322cd8,4tu-matlab-scripts-underlying-the-msc-thesis-storm-surge-modelling-due-to-tropical-cyclone-activity,Matlab Scripts underlying the MSc Thesis: Storm Surge Modelling due to Tropical Cyclone Activity,"Tools developed during MSc thesis by L.Westrik (2020) - Storm Surge Modelling due to Tropical Cyclone Activity (https://repository.tudelft.nl/islandora/object/uuid:60c2afa0-354e-4d7f-ba16-53975f5885a4?collection=education). Contains scripts to for easy data handling, generation of Delft3D files and Neural networks. Read appendix D, in thesis report for additional information on the tools. Uses synthetic storm data from: Bloemendaal (2020). Generation of a global synthetic tropical cyclone hazard dataset using STORM.",,
f559f56f-2a7a-46c2-9a95-6282316520d6,fieldcompare,fieldcompare,,https://gitlab.com/dglaeser/fieldcompare.git,
f5808ac6-4ba0-4371-82bf-d0d33fc80aa4,4tu-airswmmv1-0,AirSWMMv1.0,"Stormwater management model (SWMM) software has recently become a modelling tool for the simulation of intermittent water supply systems. However, SWMM is not capable of accurately simulating the air behaviour in the pipe-filling phase, missing, therefore, a relevant factor during pipe pressurization. This work proposes the integration of a conventional accumulator model in the existing SWMM hydraulic model to overcome this gap. SWMM source code was modified to calculate the air piezometric head inside the pipe based on the system boundary conditions, and the air piezometric head was incorporated in the SWMM flow rate pressure component. Experimental data were collected during the rapid filling of a pipe system for three possible configurations that are likely to occur in intermittent water supply pipe systems: no air release, small air release, and large air release. Results show that the improved SWMM better describes the effect of the air behaviour using the extended transport (EXTRAN) surcharge method when compared to the original SWMM. Results also show that the SLOT method with a predefined slot width is not suitable for this purpose; thus, further research is needed to assess if an adjusted slot width could provide better results",https://data.4tu.nl/v3/datasets/a27fb47a-4bdd-419e-b93e-d4320c9b924a.git,
f5bc61b5-9936-4ec6-9826-b1029df31749,setup-apptainer,GitHub action to set up apptainer,"* For developer writing a software development workflow in GitHub Actions and need to run or build Apptainer images * Can be used to test code against Apptainer containers * Tell workflow what you want not how you want it * No need to learn how to install Apptainer yourself, just copy/paste configuration",https://github.com/eWaterCycle/setup-apptainer,eWaterCycle/setup-apptainer
f5dfe24f-497d-4eec-aacf-3272fc5fa61e,hhl-prototype,hhl-prototype,,https://github.com/quantumapplicationlab/hhl-prototype,quantumapplicationlab/hhl-prototype
f6217010-878b-4cb6-9796-f2bf25f295cb,3d-e-chem-knime-plants,KNIME PLANTS,"* For cheminformaticians who want to combine protein-ligand docking with other KNIME workflow steps * Makes it possible to generate a PLANTS config file, run the PLANTS executable and ingest the PLANTS output from a KNIME workflow * The KNIME nodes come bundled with a PLANTS executable, so it does not have to be installed by each user separately * Used in workflows to perform an all-versus-all docking of all ligand-protein complexes of the Kinase and GPRC family in the protein data bank",https://github.com/3D-e-Chem/knime-plants,3D-e-Chem/knime-plants
f6808da7-b81d-4c57-a7ec-ef739f5ef382,overture-maestro,Overture Maestro,"# What is Maestro?  Maestro is a micro service that connects to a genomic metadata server SONG and listens to changes there to build an Elasticsearch index.  Maestro was created to enable genomic researchers to enhance their Overture metadata storage systemSONG by building search indexes, Elasticsearch by default, that makes searching Analyses and Studies easy and powerful.  On the other end, Indexes created my Maestro can be consumed Arranger, a data portal generator.",https://github.com/overture-stack/maestro,overture-stack/maestro
f6a16946-2114-4271-952e-cd3fd0df5df1,mibitrans,mibitrans,A collection of analytical and semi-semianalytical solutions for hydrogeological transport phenomena,https://github.com/MiBiPreT/anatrans,MiBiPreT/anatrans
f6fef000-99cf-42eb-b006-66234d049813,4tu-a-demand-management-and-frequency-settings-model-for-complying-to-the-covid-19-capacity-limits-in-public-transport-operations,A demand management and frequency settings model for complying to the COVID-19 capacity limits in public transport operations,"Description:   This repository contains the software code and the documentation of a model that sets the frequencies of public transport lines and derives the Pareto optimal demand management strategies considering the pandemic-imposed vehicle capacities due to COVID-19.     Referencing:  In case you use this code for scientific purposes, you can cite the paper ""A demand management and frequency settings model for complying to the COVID-19 capacity limits in public transport operations"" once it is publicly available.     License:  MIT License     Dependencies:  Note that the script is written in Python. Running or modifying this script requires an installed version of Python 3.6. In addition, the mathematical model is solved with the use of Gurobi 9.0.3. You would need a Gurobi license to obtain an optimal solution.     To implement the model, you would need to create text files with your dataset following the provided documentation.",,
f70dd306-52a6-493b-b975-bf428826db85,4tu-bounded-private-cycle-detection,bounded-private-cycle-detection,"Python code to reproduce results presented in the paper ""Privacy-Preserving Peer-to-Peer Cycle Detection"".     Runs an implementation of the distributed cycle detection protocol and measures its runtime and communication performance. Performance is measured on randomly generated BarabÃ¡siâ€“Albert graphs for a range of graph densities and searched-for cycle lengths. For each graph, the protocol is repeated in its entirety for each node. Average results for each set of input parameters are written to a CSV file, which can be plotted in figures using the included code.     Usage  See README.md inside the git repository for detailed usage instructions.     Code  The source code is available as a git repository. The relevant code is stored in the src directory.",https://data.4tu.nl/v3/datasets/d522e990-4a6d-4d3e-8665-0f319bf116ab.git,
f7138a3b-f6bd-4106-bdd4-b7e201ddce68,via-appia-revisited,Revisited Via Appia,"* Enable 3D environments with efficient point cloud data (large files, i.e. 27 GB). * Map images and playable videos into the scene with precision with mouse and keyboard shortcuts. * Create storylines with a slideshow or free navigation. * Edit camera paths and transition times. * Applicable to other disciplines like medical images or education.   Travel in time and space along the most important ancient Roman road.  Six monuments and more than a hundred viewpoints show how the Via Appia Antica was seen by artists and photographers over time.  The Via Appia Online Viewer (2022) is a follow to the project [â€œA 3D spatial data infrastructure forÂ *Mapping the Via Appia*â€](https://www.sciencedirect.com/science/article/abs/pii/S221205481630008X?via%3Dihub#!), which is an interactive journey through time along the Roman Appian Way, one of the world's oldest roads. The project called REVISITED Via Appia will be presented at the ***het Valkhof Museum â€¯in Nijmegen (The Netherlands, [link to the exposition memo](https://www.museumhetvalkhof.nl/zien-en-doen/tentoonstellingen/revisited-via-appia/))*** with a gamified online version accessible to everyone.   The exhibition connects the results of years of academic and artistic research and offers a refreshing perspective on the 'Regina viarum' (queen of the roads), to follow in the footsteps of the millions of people who have walked there since Roman times and see how the road has changed over time.",https://github.com/Via-Appia/via-appia-online-viewer,Via-Appia/via-appia-online-viewer
f7c7314d-1438-4fe2-b143-783ca40cd96f,4tu-data-underlying-the-publication-thesis-chapter-scale-learning-in-scale-equivariant-convolutional-networks,Data underlying the publication/thesis chapter: Scale Learning in Scale-Equivariant Convolutional Networks,"VISAPP article and thesis chapter. This article addresses the mismatch between the scales for which scale equivariance methods are designed, and those occurring in real datasets.",https://data.4tu.nl/v3/datasets/e8f834fd-751b-4c50-830d-cbf2381c1266.git,
f7e464a1-236e-4624-90ad-57e7a54ef174,3d-e-chem-knime-python-wrapper,KNIME Python wrapper,"* Provides an abstract classes for KNIME node developers to quickly develop KNIME nodes which call Python code * KNIME provides Python nodes which have an text editor as configuration, this software provides a way to provide your own configuration dialog * Can check if certain Python libraries have been installed  * Will make fields on the configuration dialog available inside the Python code * Can make warning message generated in Python code visible in KNIME platform",https://github.com/3D-e-Chem/knime-python-wrapper,3D-e-Chem/knime-python-wrapper
f800baeb-290c-4d86-a1dc-4ea0246ee776,test-bitbucket,test-bitbucket,,https://bitbucket.org/tpronk-test-workspace/my-repo,
f82199df-d801-42e4-a029-ad7dbbefcc90,4tu-etv-scheduling,etv_scheduling,"This repository contains the code developed for scheduling a fleet of electric towing vehicles at an airport. It accounts for the uncertainty in flight arrival/departure times by both anticipating to it (using delay probability densities) and reacting to it when it occurs. It is being made public both to act as supplementary data for publications and the PhD thesis of Simon van Oosterom, and in order for other researchers to use this repository in their own work.",https://data.4tu.nl/v3/datasets/d1f342d6-578e-4ef1-bbc9-811648057aa9.git,
f8d9510d-75ea-4eb6-9213-296db2ca8d19,4tu-supporting-software-for-the-paper-conformance-in-the-railway-industry-single-input-change-testing-a-eulynx-controller,Supporting software for the paper: Conformance in the Railway Industry: Single-Input-Change Testing a EULYNX Controller,"This repository contains (i) the source code of a Java project called ""jEULYNX""; (ii) the extended source code and binaries of a software simulator of a EULYNX Point subsystem; and (iii) PDFs with instructions on how to install the software and how to reproduce the findings from our paper. The jEULYNX software is the implementation of our SIC test suite generation approach, and runs on any platform that supports a JVM version 18 or higher. The extended software simulator is our case study, and runs on MS Windows 10/11 platforms.",,
f958cca0-0f41-4973-8185-fb077bf6ea36,planeo-dashboard,PlanEO Dashboard,"Climate change threatens human health and our planetâ€™s health. Diarrheal disease is the number two killer of children across the globe. Unchecked, climate change and its impacts on our planet will likely reverse hard-won gains in battling diarrheal diseases. Policy makers need to understand how diarrheal disease burden will change and how we can better protect children in a climate-uncertain future. We propose using artificial intelligence to mine existing literature to describe how climate might change diarrheal diseases in the future. We then will use this knowledge to project the risk of diarrheal disease into the future, preparing communities and informing policy makers.  [Access to the online dashboard](https://mine-dd.github.io/dashboard)  ![](https://research-software-directory.org/image/rpc/get_image?uid=339a8695a0d881ac716874d2b2ef406ee40a6592)",https://github.com/MINE-DD/dashboard,MINE-DD/dashboard
fa3cc533-1d81-4e0c-aa0f-2939241e5c7d,skycoordsjl,SkyCoords.jl,,https://github.com/JuliaAstro/SkyCoords.jl,JuliaAstro/SkyCoords.jl
fa73f30f-f9b8-4ff0-8060-014994f41f70,4tu-proactive-motion-planning-codes-for-emergency-collision-avoidance-in-highway-scenarios,Proactive Motion Planning Codes for Emergency Collision Avoidance in Highway Scenarios,"This repository includes local motion planners for emergency collision avoidance in automated driving systems. These planners incorporate stochastic prediction models for other road users (e.g. vehicles or static obstacles) and a dynamic prediction model for the ego vehicle. Further, the planners are formulated as model predictive control optimization problems and are designed to find a reference trajectory for the ego vehicle to avoid collision with the road users/obstacles and road boundaries while taking into account the uncertainty in predicting the behavior of other road users.",,
faaf7039-54b7-48f3-acda-86fb1c35b525,3d-e-chem-knime-python-node-archetype,KNIME Python node archetype,"* Similar to KNIME node archetype, but gives utilities to call Python library in a node * For KNIME node developers who like to have  quick start and automated builds * Generates a project skeleton that contains all you need to develop your own KNIME nodes * Automates tests and deploying the node to an update site * Includes dependency management and test coverage * Used to create all KNIME Python nodes in the 3D-e-Chem project",https://github.com/3D-e-Chem/knime-python-node-archetype,3D-e-Chem/knime-python-node-archetype
fabd7a30-df5d-49ee-95e1-fe0f153075d0,pattyanalytics,PattyAnalytics,"* Place an object (dense but small point cloud) on a drive map (sparse point cloud with large extent) * Rotate, scale, and translate the object to absolute dimenstion (meters, lat/lon) * Read and write LAS files  This library builds on Python bindings from the [python-pcl package](https://github.com/NLeSC/PattyAnalytics), which in turn uses the state of the art [Point Cloud Library](http://pointclouds.org/). This allows us to scale up to large point clouds, while keeping good performance via optional GPU acceleration.",https://github.com/NLeSC/PattyAnalytics,NLeSC/PattyAnalytics
fad9a725-f151-4188-8cb9-06d149c2e8e8,4tu-global-optimization,global-optimization,Source code for the implementation and simulation of nonmyopic multistep lookahead strategies for the gradient-free optimisation of black-box functions.,https://data.4tu.nl/v3/datasets/5b628284-f4c5-4756-bceb-baccad8aae20.git,
fb10cee7-908b-4d40-8a6f-e3878d5c9ae4,pyglotaran,pyglotaran,,https://github.com/glotaran/pyglotaran,glotaran/pyglotaran
fb18d994-b14e-46e9-90a6-920e59a97004,wam2layers,WAM2layers,"WAM2layers is an atmospheric moisture-tracking model that can be used to determine where precipitation originally evaporated (backtracking), or where evaporated moisture eventually ends up (forward tracking).",https://github.com/WAM2layers/WAM2layers,WAM2layers/WAM2layers
fb6cf907-3ca6-406a-8483-a9664712b422,4tu-np-heating,np_heating,Code for modeling the heat transfer and calculating the temperature evolution of a layered plasmonic nanoparticle and its surroundings after pulsed or continuous-wave illumination. More details can be found in the included file README.md.,https://data.4tu.nl/v3/datasets/df163720-38b8-4d25-a1bc-953ce96744df.git,
fbdf8b39-562d-4347-8637-9de697d4d968,stemmusscope,STEMMUS_SCOPE,"Integrated code of SCOPE and STEMMUS.  SCOPE is a radiative transfer and energy balance model, and STEMMUS model is a two-phase mass and heat transfer model. For more information about the coupling between these two models, please check [this reference](https://gmd.copernicus.org/articles/14/1379/2021/). Before running the model, you need to prepare input data and a configuration file. This can be done using the python package [PyStemmusScope](https://pystemmusscope.readthedocs.io).    ![image](https://raw.githubusercontent.com/EcoExtreML/STEMMUS_SCOPE/main/docs/assets/imgs/coupling_scheme.png) (by Zeng & Su, 2021)  ## Running STEMMUS_SCOPE About how to run `STEMMUS_SCOPE` on Snellius, check [./docs/STEMMUS_SCOPE_on_Snellius.md](https://github.com/EcoExtreML/STEMMUS_SCOPE/blob/main/docs/STEMMUS_SCOPE_on_Snellius.md).  If you want to run `STEMMUS_SCOPE` on CRIB, check [./docs/STEMMUS_SCOPE_on_CRIB.md](https://github.com/EcoExtreML/STEMMUS_SCOPE/blob/main/docs/STEMMUS_SCOPE_on_CRIB.md).  If you want to run `STEMMUS_SCOPE` on your own machine, check [./docs/STEMMUS_SCOPE_on_local_device.md](https://github.com/EcoExtreML/STEMMUS_SCOPE/blob/main/docs/STEMMUS_SCOPE_on_local_device.md).  `STEMMUS_SCOPE` scope also has a Basic Model Interface (BMI) mode implemented. The full BMI is implemented in Python in [PyStemmusScope](https://github.com/EcoExtreML/STEMMUS_SCOPE_Processing/). For more information, check [./docs/STEMMUS_SCOPE_BMI.md](https://github.com/EcoExtreML/STEMMUS_SCOPE/blob/main/docs/STEMMUS_SCOPE_BMI.md).  ## Contributing  If you want to contribute to the development of `STEMMUS_SCOPE`, have a look at the [contribution guidelines](https://github.com/EcoExtreML/STEMMUS_SCOPE/blob/main/CONTRIBUTING.md).  ## How to cite us [![RSD](https://img.shields.io/badge/rsd-ecoextreml-00a3e3.svg)](https://research-software-directory.org/projects/ecoextreml) <!-- [![DOI](https://zenodo.org/badge/DOI/<replace-with-created-DOI>.svg)](https://doi.org/<replace-with-created-DOI>) -->",https://github.com/EcoExtreML/STEMMUS_SCOPE,EcoExtreML/STEMMUS_SCOPE
fc36bc93-cc49-4326-9815-c8e0fb71aae7,4tu-petcluster,petcluster,Petcluster Python module combines complex network and key performance indicators to assess petrochemical clusters.,https://data.4tu.nl/v3/datasets/5973542b-5338-4c1c-870d-618c8eb46df1.git,
fd1c37b2-1a96-4e15-94d2-cbec7e65026a,lightning,Lightning,"* Data processing using GPUs on distributed platforms * Program a cluster of GPUs just like programming a single GPU * Multi-GPU execution of existing CUDA kernels with minimal effort * Focus on scalability and efficiency  Lightning is a framwork for fast data processing using GPUs on distributed platforms. The framework enables distributed multi-GPU execution of compute kernels written in CUDA in a way that resembles programming CUDA using a single GPU. This enables scaling existing CUDA kernels to much larger problems size, far beyond the memory capacity and compute capabilities of a single GPU.",https://github.com/lightning-project/lightning,lightning-project/lightning
fd33ad28-1abc-40c7-b3ba-a91a4e833ac0,haddock3-analysis-components,Components for Haddock3 analysis,Superseded by https://research-software-directory.org/software/haddock3-ui  React components used in the html report generated by the haddock3-analyse command and haddock3 web application.  Example of React component library which can be used from a simple html page or a React application.,https://github.com/i-VRESSE/haddock3-analysis-components,i-VRESSE/haddock3-analysis-components
fdaa80a9-c316-48aa-bd0a-463f21972369,dgroomacoustics,DG_RoomAcoustics,,https://github.com/Building-acoustics-TU-Eindhoven/edg-acoustics,Building-acoustics-TU-Eindhoven/edg-acoustics
fdaaa747-c495-47fe-942f-a26dfcfdccca,4tu-xcrisp,xcrisp,"This repository consists of code to reproduce the results of the paper ""X-CRISP: Domain-Adaptable and Interpretable CRISPR Repair Outcome Prediction"".     The code to reproduce the paper is shared at: https://github.com/joanagoncalveslab/xcrisp     No experimental data was collected as part of this project, this project only utilised previously publicly available datasets. Details on accessing this data is fully described in Methods Section; ""Data and preprocessing"" of the published article.",,
fdd4b5c4-7d52-4968-b682-a193bde2e11d,2025-verhagen-001,Livestock demography,"This module was originally part of the ROMFARMS model (Joyce 2019) in order to simulate different scenarios of animal husbandry practice in the Dutch Roman frontier zone. The original module focused on understanding the economic viability of stockbreeding of cattle, sheep and horses for the Roman army. In particular slaughter rate proved to be an important parameter for this.  The standard input values used in the submodule are taken from the ROMFARMS source code, and are described and substantiated in Joyce (2019). However, users are free to adapt these values to their own research questions. The original module was developed for a runtime of 100 ticks (years), during which the herd size growth will be relatively modest using these parameters. However, after approx. 400 ticks the herd population will start to grow exponentially.          ## Inputs  |**Name**|**Type**|**Description**| |-|-|-| |species-type|string|animal species, OPTIONAL; for multiple species different rules can be specified if desired| |alphaA|floating point|age of senescence of animal| |alpha|floating point|standard deviation age of senescence| |NMR|floating point|mortality rate of neonatal animals| |BMR|floating point|background natural annual mortality rate of animals| |FR|floating point|fertility rate of female animals| |YAA|floating point|maximum age of young animal| |AAA|floating point|minimum age of adult animal| |YMR|floating point|mortality rate of young animal for slaughter| |IMR|floating point|mortality rate of immature animal for slaughter| |AMR|floating point|mortality rate of adult animal for slaughter| |animals|turtle-set|animals found in livestock herd| |sex (agent attribute)|string|sex of animal (F/M)| |age (agent attribute)|integer|age of animal| |fertility (agent attribute)|floating point|fertility of animal| |lactating (agent attribute)|string|lactation state of animal (Y/N)| |survivorship (agent attribute)|floating point|| ||||  ## Outputs  |**Name**|**Type**|**Description**| |-|-|-| |pop-list|integer|number of animals| |d-pop|integer|number of animals at end of year| |d1-pop|integer|number of animals at start of year| |livestock|turtle-set|current turtle set of animals| |pop-growth|floating point|herd population growth as the percentage increase or decrease from the previous year| ||||",https://github.com/Archaeology-ABM/NASSA-modules/tree/main/2025-Verhagen-001,
fe145042-3a94-43e6-be78-f4f74a373387,climate-rapid-evaluation-framework,Climate Rapid Evaluation Framework,"As climate models become increasingly complex, there is a need for systematic and comprehensive evaluation and benchmarking through comparison with best-available observational data to assess model fidelity across a wide range of variables.  **Vision for the Rapid Evaluation Framework:** A community owned evaluation framework, built upon, and compatible with, existing community evaluation packages that incorporates an application programming interface (API) for executing metrics generated from community evaluation packages, across the globe.   **Goal:** The CMIP7 Assessment Fast Track Rapid Evaluation Framework (CMIP7 AFT REF) will be a complete end to end system providing a systematic and rapid performance assessment of the expected models participating in the CMIP7 Assessment Fast Track, supporting the seventh IPCC Assessment Report (AR7) cycle.  **Outcome:** The REF provides the ability to fully integrate evaluation tools into the CMIP publication workflow, and their diagnostic outputs published alongside the model output on the [Earth System Grid Federation (ESGF)](https://esgf.llnl.gov/) through an easily accessible website.   **Wider community use beyond CMIP:** The REF is designed to be a starting point for the community to develop and build upon, with applications across the WCRP Modelling Multiverse and beyond.  See the [CMIP7 Assessment Fast Track Rapid Evaluation Framework announcement](https://wcrp-cmip.org/cmip-phases/cmip7/rapid-evaluation-framework/) for more context.",https://github.com/Climate-REF/climate-ref,Climate-REF/climate-ref
fe1a73a9-ab56-4482-be83-a6cc6d26a689,4tu-agent-based-model-software-written-for-the-master-thesis-scaling-up-staying-secure,"Agent-based model software written for the master thesis Scaling Up, Staying Secure",Code pertaining to the thesis found at:   link. Also see documentation at: linkÂ            Â This library was created in the process of doing a Master's Thesis in Â Computer Science at the TU Delft in the specialization of Cyber Â Security. It was created by Mike van der Boon. You can reach out to him by Â emailing thesis_library@mvdboon.nl.,,
fe854e7e-3673-4bad-9de6-4a3f4f0c7ac1,4tu-emma,EMMA,A Python-based interface that creates an ecotope-map based on output data of a hydrodynamic model.,https://data.4tu.nl/v3/datasets/dc54bf02-e459-4cd9-8be8-43a6feceee59.git,
ff1ae9c6-d44d-4c48-864e-b2f03c8c0c41,4tu-software-code-in-r-for-a-cluster-analysis-to-cluster-and-characterize-autonomous-last-mile-concepts-in-a-standardized-and-holistic-manner,Software code in R for a cluster analysis to cluster and characterize autonomous last mile concepts in a standardized and holistic manner,"This file is the softwarecode in R for a clusteranalysis to cluster and characterize autonomous last mile concepts in a standardized and holistic manner.  With the developed clusters it is possible to better categorize and compare individual autonomous last mile concepts. Furthermore, the developed taxonomy of autonomous last mile concepts and the cluster analysis allow an evaluation of the interrelations between characteristics of autonomous last mile concepts, so that the design of new concepts as well as the adaptation or selection of a concept for a specific use case is supported.",,
